,ner,sentence,pre_sentence,post_sentence,used,section_name,preds_bert,preds_cnn
566,pooling operations,"The most basic version of this approach would not involve combining learned pooling operations , but simply learning pooling operations in the form of the values in âpooling filtersâ.",These pooling layers remain distinct from convolution layers since pooling is performed separately within each channel ; this channel isolation also means that even the option that introduces the largest number of parameters still introduces far fewer parameters than a convolution layer would introduce .,"One step further brings us to what we refer to as tree pooling , in which we learn pooling filters and also learn to responsively combine those learned filters .",0,Tree pooling,1,0
558,explicit attention mechanism,"Based on this , we argue that MRN is an implicit attention model without explicit attention mechanism .","Since MRN does not depend on a few attention parameters ( e.g . ) , our visualization method shows a higher resolution than others .",,0,Qualitative Analysis,0,1
563,fully - convolutional baselines,the two fully - convolutional baselines .,w.r.t .,"Interestingly , the gap significantly widens when considering mAP at the higher accuracy regime of 0.7 IOU : + 41.6 and + 18.4 respectively .",1,Evaluation for visual object tracking,1,0
365,scale expansion algorithm,The detail of scale expansion algorithm is summarized in Algorithm [ reference ] .,"Thanks to the “ progressive ” expansion procedure , these boundary conflicts will not affect the final detections and the performances .","In the pseudocode , are the intermediate results .",0,Progressive Scale Expansion Algorithm,1,1
931,multi - scale and sliding pooling,"We start from a baseline setting of our FeatMap - Net ( “ FullyConvNet Baseline ” in the result table ) , for which multi - scale and sliding pooling is removed .",We present the results of adding different components of FeatMap - Net in Table [ reference ] .,"This baseline setting is the conventional fully convolution network for segmentation , which can be considered as our implementation of the FCN method in .",0,Component Evaluation,1,1
654,FastEval14k,FastEval14k consists of 14000 images with labels from 6000 classes ( subset of 18291 classes from JFT - 300 M ) .,FastEval14k ’ .,"Unlike labels in JFT - 300 M , the images in FastEval14k are densely annotated and there are around 37 labels per image on average .",0,Monitoring Training Progress,1,1
786,coding schemes,The assignment of codes is more balanced for larger coding schemes .,"” for the first component , while the subcode “ 1 ” is only used by 5 % of the words .","In any coding scheme , even the most unpopular codeword is used by about 1000 words .",1,Analysis of Code Efficiency,1,0
938,bidirectional LSTM model,These vectors are concatenated ( into a vector in R 500 ) and fed into either a convolutional model or a bidirectional LSTM model .,"Each model consumes the words in the sentence , which are embedded in R 200 , as well as the distances of each word in the sentence from both the entity - word - span and the number - word - spans ( as described above ) , which are each embedded in R 100 .","The convolutional model uses 600 total filters , with 200 filters for kernels of width 2 , 3 , and 5 , respectively , a ReLU nonlinearity , and maxpooling .",0,C. Information Extraction Details,1,1
716,HFA based methods,"The major advancements of the proposed approach over the HFA based methods are described in the following aspects : Firstly , the proposed approach revises the decomposition of in the HFA based methods to the multiplication of hidden components and , which is more intuitive and concise to model the unrelated components with less extra hyper - parameters .","Specifically , given a feature , the HFA based methods decompose the as , where is the mean feature regarding identity - related component , is the additional noise and are the transformation matrices for identity - related component and age - related component respectively .","Secondly , we explicitly project the identity features on a hypersphere to match the cosine similarity measurement for effectively combining the improvement strategies based on the Softmax loss and the margin of decision boundaries .",1,Discussion,1,0
475,RASP,"The required POS tags were generated with RASP [ reference ] , using the CLAWS2 tagset .",[ reference ] .,,0,Pattern Extraction,1,1
444,CHFusion,subsubsection : Context - Aware Hierarchical Fusion ( CHFusion ),"Wherein our method learns the weights automatically using a neural network ( Equation 1 , 2 and 3 ) .",The results of this experiment are shown in table : chfusion .,1,Context - Aware Hierarchical Fusion ( CHFusion ),1,0
839,Ranking model,"KEYWORDS Ranking model , weak supervision , deep neural network , deep learning , ad - hoc retrieval",Our findings also suggest that supervised neural ranking models can greatly benefit from pre - training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models .,compat=1.11 ternary compat,0,Neural Ranking Models with Weak Supervision,1,0
567,VIS + BOW,VIS + BOW performs multinomial logistic regression based on image features and a BOW vector obtained by summing all the word vectors of the question .,GUESS simply selects the modal answer from the training set for each of 4 question types .,"VIS + LSTM has one LSTM to encode the image and question , while 2 - VIS + BLSTM has two image feature inputs , at the start and the end .",0,Results on DAQURA,1,1
210,Evolution,Evolution has discovered efficient feedforward pathways for recognizing certain objects in the blink of an eye .,"Once trained , the CNN never changes its weights or filters during evaluation .","However , an expert ornithologist , asked to classify a bird belonging to one of two very similar species , may have to think for more than a few milliseconds before answering [ 16 , 17 ] , implying that several feedforward evaluations are performed , where each evaluation tries to elicit different information from the image .",1,Title,0,0
469,Quantitative Analysis,subsubsection : Quantitative Analysis,It manifests that our different - resolution information is properly made use of in this framework .,"To further understand accuracy gain in each branch , we quantitatively analyze the predicted label maps based on connected components .",1,Quantitative Analysis,1,0
162,question and answer representations,The Neural Answer Selection Model ( Figure [ reference ] ) is a supervised model that learns the question and answer representations and predicts their relatedness .,This is a classification task where we treat each training data point as a triple while predicting for the unlabelled question - answer pair .,It employs two different LSTMs to embed raw question inputs and answer inputs .,0,Neural Answer Selection Model,1,1
499,classification function,"IIC is a generic clustering algorithm that directly trains a randomly initialised neural network into a classification function , end - to - end and without any labels .","In this paper , we introduce Invariant Information Clustering ( IIC ) , a method that addresses this issue in a more principled manner .","It involves a simple objective function , which is the mutual information between the function ’s classifications for paired data samples .",1,Introduction,0,0
294,uniform weights,"Also , regardless of the embedding method , learning weights helps models to get better performance compared to the fixed weightings , with either IDF or uniform weights .",This supports the hypothesis that with the embedding vector representation the neural networks learn an embedding that is based on the interactions of query and documents that tends to be tuned better to the corresponding ranking task .,"Although weight learning can significantly affect the performance , it has less impact than learning embeddings .",1,Results and Discussion,1,0
812,masked self - attentional layers,"We present graph attention networks ( GATs ) , novel neural network architectures that operate on graph - structured data , leveraging masked self - attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations .",,"By stacking layers in which nodes are able to attend over their neighborhoods ' features , we enable ( implicitly ) specifying different weights to different nodes in a neighborhood , without requiring any kind of costly matrix operation ( such as inversion ) or depending on knowing the graph structure upfront .",0,ABSTRACT,1,1
136,Google NLP pipeline,The text has been run through a Google NLP pipeline .,"A news article is usually associated with a few ( e.g. , 3–5 ) bullet points and each of them highlights one aspect of its content .","It it tokenized , lowercased , and named entity recognition and coreference resolution have been run .",0,The Reading Comprehension Task,1,1
867,joint update,We can write the joint update for all as Restrict the update to define a contraction mapping in the Euclidean metric .,"Let , and .","This means that there is some such that for any , or in other words , We can immediately see that this implies that for each by letting be the elementary vector that is all zero except for a 1 in position and letting be the all zeros vector .",0,Contraction Map Example,1,1
892,left eye model,"Moreover , for the right eye cluster , the right eye model improves the accuracy more significantly than the left eye model .","Taking the left eye model as an example , it additionally reduces the errors of landmarks of right eye , mouth , and chin , which is due to the correlations among different facial parts .",It can be concluded that each shape prediction layer emphasizes on the corresponding cluster respectively .,1,Analysis of Shape Prediction Layers,1,0
737,fusion sub - components,"Below , we go into the details of our point cloud and fusion sub - components .","[ reference ] D ) , in which we use a dense spatial anchor mechanism to improve the 3D box predictions and two scoring functions to select the best predictions .",,0,PointFusion,1,1
247,generic face alignment algorithm,"[ reference ] that even as a generic face alignment algorithm , 3DDFA still demonstrates competitive performance on the common set and state - of - the - art performance on the challenging set .",It can be seen in Tabel .,,1,Medium Pose Face Alignment,1,0
315,matrix of convolutional filters,The result is reshaped to generate a matrix of convolutional filters .,"The hypernetwork is a fully connected layer ( denotes filter length and the number of filters per relation , i.e. output channels of the convolution ) that is applied to the relation embedding .","Whilst the overall dimensionality of the filter set is , the rank is restricted to to encourage parameter sharing between relations .",0,Scoring Function and Model Architecture,1,1
132,downsamplers,"Existing literatures have considered two types of downsamplers , including direct downsampler and bicubic downsampler .",,"In this paper , we consider the bicubic downsampler since when k is delta kernel and the noise level is zero , Eqn .",1,Downsampler .,1,0
297,deeper model,The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart .,"There exists a solution by construction to the deeper model : the added layers are identity mapping , and the other layers are copied from the learned shallower model .",But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution ( or unable to do so in feasible time ) .,1,Introduction,1,0
684,LSTM Q + I,"[ reference ] , Q + I vs Question , and LSTM Q + I vs LSTM Q.","No , as shown in Table .",Our results demonstrate clearly the positive impact of using multiple attention layers .,1,Results and analysis,1,0
986,label maps,"We demonstrate that this approach is effective at synthesizing photos from label maps , reconstructing objects from edge maps , and colorizing images , among other tasks .",This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations .,"Indeed , since the release of the pix2pix software associated with this paper , a large number of internet users ( many of them artists ) have posted their own experiments with our system , further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking .",0,Image - to - Image Translation with Conditional Adversarial Networks,1,1
982,mixture 10 Gaussians,"In contrast , the VAE exhibit systematic differences from the mixture 10 Gaussians indicating that the VAE emphasizes matching the modes of the distribution as discussed above ( Figure [ reference ] d ) .",The adversarial autoencoder successfully matched the aggregated posterior with the prior distribution ( Figure [ reference ] b ) .,"An important difference between VAEs and adversarial autoencoders is that in VAEs , in order to back - propagate through the KL divergence by Monte - Carlo sampling , we need to have access to the exact functional form of the prior distribution .",0,Relationship to Variational Autoencoders,0,1
246,CUDA implementations,"While it is possible to have a 10 speed up for char - CNN by using more recent CUDA implementations of convolutions , fastText takes less than a minute to train on these datasets .",Table [ reference ] shows that methods using convolutions are several orders of magnitude slower than fastText .,The GRNNs method of tang2015document takes around 12 hours per epoch on CPU with a single thread .,0,Training time .,1,1
881,vanilla Transformers,"Specifically , Transformer - XL significantly outperforms a contemporary method using vanilla Transformers , suggesting the advantage of Transformer - XL is generalizable to modeling short sequences .","Although Transformer - XL is mainly designed to better capture longer - term dependency , it dramatically improves the single - model SoTA from 23.7 to 21.8 .",We also report the results on word - level Penn Treebank in Table [ reference ] .,0,Main Results,1,0
963,probability distribution strategies,We also experiment with majority voting and averaging the probability distribution strategies for ensemble models using the same set of models as our weighted averaging ensemble method ( as described above ) .,"Formally , we replace Equations [ reference ] and [ reference ] in the paper with the following equations respectively : Our final ensemble model , DR - BiLSTM ( Ensemble ) is the combination of the following 6 models : tanh - Projection , DR - BiLSTM ( with 1 round of dependent reading ) , DR - BiLSTM ( with 3 rounds of dependent reading ) , and 3 DR - BiLSTMs with different initialization seeds .",Figure [ reference ] shows the behavior of the majority voting strategy with different number of models .,0,Ensemble Strategy Study,1,1
687,biasing process,"Then , in the biasing process , for each layer in the conditional PixelRNN , one simply maps the conditioning map into a map that is added to the input - to - state map of the corresponding layer ; this is performed using a unmasked convolution .","In the upsampling process , one uses a convolutional network with deconvolutional layers to construct an enlarged feature map of size , where is the number of features in the output map of the upsampling network .",The larger image is then generated as usual .,0,Multi - Scale PixelRNN,1,1
443,class embedding,"It uses a simpler variant of skip - conditioning : instead of first splitting into chunks , we concatenate the entire with the class embedding , and pass the resulting vector to each residual block through skip connections .",The BigGAN - deep model ( Figure [ reference ] ) differs from BigGAN in several aspects .,"BigGAN - deep is based on residual blocks with bottlenecks he2016resnets , which incorporate two additional convolutions : the first reduces the number of channels by a factor of before the more expensive convolutions ; the second produces the required number of output channels .",1,Architectural details,0,1
181,greedy search process,The greedy search process applies rules in a manually defined order .,"op2 "" Korea "" ) , then determine if they are aligned by checking if name is the tail concept of country .","The results are mutually exclusive which means once a graph fragment is aligned by one rule , it can not be realigned .",0,JAMR Aligner .,1,0
116,deep network architectures,This process is similar to that of the original deep network architectures such as VGG and ResNet .,"That is , the dimension slowly increases in input - side layers and sharply increases in output - side layers .",The visual illustrations of additive and multiplicative PyramidNets are shown in Figure [ reference ] .,0,Feature Map Dimension Configuration,1,0
148,DSS,"Deep Sliding Shapes ( DSS ) generates 3D regions using a proposal network and then processes them using a 3D convolutional network , which is prohibitively slow .",Comparison with other methods We compare our model with three approaches from the current state of the art .,Our model outperforms DSS by 3 % mAP while being 15 times faster .,1,Evaluation on SUN - RGBD,1,0
54,dynamic sentence representation,"Given the caption representation from the language model , , the operator outputs a dynamic sentence representation at each step through a weighted sum using alignment probabilities : The corresponding alignment probability for the word in the caption is obtained using the caption representation and the current hidden state of the generative model : where , , and are the learned model parameters of the alignment model .","[ reference ] ) , with and initialized to learned biases : The function is used to compute the alignment between the input caption and intermediate image generative steps bahdanau_mt .",The function of Eq .,0,Image Model : the Conditional DRAW Network,0,1
873,capsules,"It is inspired by the notion of capsules developed in : capsules are new types of neurons which encapsulate more information in a local pool operation ( e.g. , a convolution operation in a CNN ) by computing a small vector of highly informative outputs rather than just taking a scalar output .","In particular , we propose a new model , referred to as Graph Capsule Convolution Neural Networks ( GCAPS - CNN ) .",Our graph capsule idea is quite general and can be employed in any version of GCNN model either design for solving graph semi - supervised problem or doing sequence learning on graphs via Graph Convolution Recurrent Neural Network models ( GCRNNs ) .,1,Introduction,0,0
102,evo - toy,"evo - toy shows the evaluation of the accuracy with the iterations in both of these cases , averaged across 100 runs .",fig :,It is clear that pairing the data - augmented pairs in one batch accelerates the convergence of this model .,0,Data - augmented batches : toy model,1,1
285,first order approximation,"Motivated as a first order approximation of the graph laplacian methods , propose the following layer - wise propagation rule : Here where is the real valued adjacency matrix for an undirected graph .",,Adding the identity matrix corresponds to adding self loops to the graph .,0,The special case of Kipf and Welling ( 2016 ),0,1
256,LSTM system,"For the LSTM system , the conversation - side i - vector is appended to each frame of input .",A 100 - dimensional i - vector is generated for each conversation side .,"For convolutional networks , this approach is inappropriate because we do not expect to see spatially contiguous patterns in the input .",0,Speaker Adaptive Modeling,1,1
399,person representation,"At last , the features of global full body and local body parts are concatenated to be a 256 - dimension feature as the final person representation .",The Dropout is adopted after each FC layer to prevent overfitting .,,0,Feature Extraction and Fusion,1,0
744,CNN - based depth estimation,CNN - based depth estimation .,"Given training data like the one produced by , then we believe that our method has the capacity to learn finer facial details , too .",Our work has been inspired by the work of who showed that a CNN can be directly trained to regress from pixels to depth values using as input a single image .,0,Closely related work,1,0
519,3D face regressor model,"[ f , t , Π , α s , α exp ] to regress for the 3D face regressor model .","Putting them together , we have in total 62 parameters α =",,0,3D morphable model,1,0
620,pose prediction,"To achieve this , we refine our previous pose prediction by learning correction offsets ( i.e. feedback ) denoted by .","Ultimately , our goal is to predict the location of the joint corresponding to each visible human body part .","Furthermore , we only learn correction offsets for joints that are visible .",0,Multi - Task Loss,1,1
704,element - wise means,"With element - wise means , this is trivial ; each computes the mean of only its active inputs .","As with dropout , signals may need appropriate rescaling .","In experiments , we train with dropout and a mixture model of local and global sampling for drop - path .",1,Regularization via Drop - path,1,0
854,reading comprehension models,"This is reminiscent of the ( soft ) attention mechanism used in reading comprehension models ( e.g. , Cheng2016 , wang2017gated ) .","Pooling over lemma - occurrences effectively connects different text passages ( even across texts ) that are otherwise disconnected , mitigating the problems arising from long - distance dependencies .","However , our setup is more general as it allows for the connection of multiple passages ( via pooling ) at once and is able to deal with multiple inputs which is necessary to make use of additional input texts such as relevant background knowledge .",0,Refined Word Embeddings ( ),1,0
750,MAN - AGER,"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .",We thus call it a leakage of information from D.,"As such , the guiding signals from D are not only available to G at the end in terms of the scalar reward signals , but also available in terms of a goal embedding vector during the generation process to guide G how to get improved .",0,Introduction,1,1
747,Siamese nets,"1 - shot 5 - shot 1 - shot 5 - shot MANN , no conv [ reference ] 82.8 % 94.9 % -- MAML , no conv ( ours ) 89.7 ± 1.1 % 97.5 ± 0.6 % -- Siamese nets [ reference ] 97.3 % 98.4 % 88.2 % 97.0 % matching nets [ reference ] 98.1 % 98.9 % 93.8 % 98.5 % neural statistician [ reference ] 98.1 % 99.5 % 93.2 % 98.1 % memory mod .",Accuracy Omniglot [ reference ],[ reference ] 98.4 % 99.6 % 95.0 % 98.6 % MAML ( ours ),1,Classification,0,1
314,concatenation of previous layer encodings,"As is also typical , we can have multiple such layers ( l ) that feed into each other through the concatenation of previous layer encodings .",a BiLSTM :,"The last layer l has both forward ( f l c , 1 , ... , f l c , n ) and backward (",1,Sentence - based Character Model,1,0
644,spectral normalization method,"Moreover , the RBF - B kernel managed to stabilize the MMD - GAN training for various configurations of the spectral normalization method .","In Appendix [ reference ] , we showed the proposed generalized power iteration ( Section [ reference ] ) imposes a stronger Lipschitz constraint than the method in , and benefited MMD - GAN training using the repulsive loss .","In Appendix [ reference ] , we showed the gradient penalty can also be used with the repulsive loss .",0,Quantitative Analysis,1,0
980,AI techniques,We illustrate the promise of ALE by developing and benchmarking domain - independent agents designed using well - established AI techniques for both reinforcement learning and planning .,"Most importantly , it provides a rigorous testbed for evaluating and comparing approaches to these problems .","In doing so , we also propose an evaluation methodology made possible by ALE , reporting empirical results on over 55 different games .",0,The Arcade Learning Environment : An Evaluation Platform for General Agents,1,1
468,quaternion convolution,"This section defines the internal quaternion representation ( Section [ reference ] ) , the quaternion convolution ( Section [ reference ] ) , a proper parameter initialization ( Section [ reference ] ) , and the connectionist temporal classification ( Section [ reference ] ) .",,,0,Quaternion convolutional neural networks,0,1
357,DeepLab,"It is more efficient and accurate than modern segmentation frameworks , such as FCN and DeepLab .",ICNet still performs satisfyingly regarding common thing and stuff understanding .,"Compared to our baseline model , it achieves 5.4 times speedup .",0,COCO - Stuff,1,1
851,classification layers,This feature vector is then passed through classification layers to compute the loss .,"For each sliding window , we drop out the values in all channels whose spatial locations are covered by the window and generate a new feature vector for the region proposal .","Based on the loss of all the windows , we select the one with the highest loss .",1,Adversarial Spatial Dropout for Occlusion,1,0
133,deeper architecture,"When using CaffeNet as the backbone , we directly replace the original FC7 layer with the Eigenlayer , in case that one might argue that the performance gain is brought by deeper architecture .","We mainly use two networks pre - trained on ImageNet as backbones , , CaffeNet and ResNet - 50 .","When using ResNet - 50 as the backbone , we have to insert the Eigenlayer before the last FC layer because ResNet has no hidden FC layer and the influence of adding a layer into a 50 - layer architecture can be neglected .",0,Datasets and Settings,1,1
248,fc 7 features,Nearest neighbors are computed using fc 7 features .,"We approach this difficult task in the style of SIFT flow : we retrieve near neighbors using a coarse similarity measure , and then compute dense correspondences on which we impose an MRF smoothness prior which finally allows all images to be warped into alignment .","Since we are specifically testing the quality of alignment , we use the same nearest neighbors for convnet or conventional features , and we compute both types of features at the same locations , the grid of convnet rf centers in the response to a single image .",1,Intraclass alignment,1,0
376,Non - approximate joint training,( iii ) Non - approximate joint training .,This solver is included in our released Python code .,"As discussed above , the bounding boxes predicted by RPN are also functions of the input .",0,Sharing Features for RPN and Fast R - CNN,1,0
785,Rainbow,"Additionally , unlike Rainbow , Reactor does not use Noisy Networks fortunato2017noisy , which was reported to have contributed to the performance gains .","( see Figure [ reference ] , right ) .","When evaluating under the no - op starts regime ( Table [ reference ] ) , Reactor out performs all methods except for Rainbow .",0,Comparing to prior work,1,0
424,GK,", Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( MLK ) .","We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP )",Baselines Settings :,1,Experiment and Results,1,0
798,log - likelihood training,"When beam - search is used , the ranking of the compared approaches is the same , but the margin between the proposed methods and log - likelihood training becomes smaller .","Surprisingly , the best performing method is REINFORCE with critic , with an additional BLEU point advantage over the actor - critic .",The final performances of the actor - critic and the REINFORCE - critic with greedy search are also and BLEU points respectively better than what ranzato2015sequence report for their MIXER approach .,0,IWSLT 2014 with a convolutional encoder,1,1
94,low - rank language models,"Empirically , our high - rank language model outperforms conventional low - rank language models on several benchmarks , as shown in Section [ reference ] .","For example , semantic meanings might not be those bases since a few hundred meanings may not be enough to cover everyday meanings , not to mention niche meanings in specialized domains .",We also provide evidences in Section [ reference ] to support our hypothesis that learning a high - rank language model is important .,0,Hypothesis : Natural Language is High - Rank,1,1
233,people detector output,ROI are either based on a ground truth ( GT ROI ) or on the people detector output ( det ROI ) .,This corresponds to unary only performance .,Results on WAF .,0,Multi Person Pose Estimation,1,0
236,pose estimation,"Unlike our proposed pose estimation , they regress poses by using iterative methods which involve computationally costly face rendering .","Some recently addressed faces in particular , though their methods are designed to estimate 2D landmarks along with 3D face shapes .",We regress 6DoF directly from image intensities without such rendering steps .,1,Related work,0,0
53,log - linear extractive summarization model,"Since these corpora are too small to train large neural networks on , namas trained their models on the Gigaword corpus , but combined it with an additional log - linear extractive summarization model with handcrafted features , that is trained on the DUC 2003 corpus .","The DUC corpus comes in two parts : the 2003 corpus consisting of 624 document , summary pairs and the 2004 corpus consisting of 500 pairs .","They call the original neural attention model the ABS model , and the combined model ABS + .",0,DUC Corpus,0,1
823,3D human pose model,Information captured by the 3D human pose model is embedded in the CNN architecture as an additional layer that lifts 2D landmark coordinates into 3D while imposing that they lie on the space of physically plausible poses .,"We propose a novel CNN architecture that learns to combine the image appearance based predictions provided by convolutional - pose - machine style 2D landmark detectors , with the geometric 3D skeletal information encoded in a novel pretrained model of 3D human pose .","The advantage of integrating the output proposed by the 2D landmark location predictors – based purely on image appearance – with the 3D pose predicted by a probabilistic model , is that the 2D landmark location estimates are improved by guaranteeing that they satisfy the anatomical 3D constraints encapsulated in the human 3D pose model .",1,Introduction,0,0
945,analytically - differentiable rendering layer,"With the projection parameter , 3D shape , and texture , a novel analytically - differentiable rendering layer is designed to reconstruct the original input face .","Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture , respectively .",The entire network is end - to - end trainable with only weak supervision .,0,Nonlinear 3D Face Morphable Model,1,1
872,RNN - based VQA approach,"Secondly , in this paper we have shown that it is possible to extend the state - of - the - art RNN - based VQA approach so as to incorporate the large volumes of information required to answer general , open - ended , questions about images .","Indeed , at the time of submitting this paper , our image captioning model outperforms the state - of - the - art on several captioning datasets .","The knowledge bases which are currently available do not contain much of the information which would be beneficial to this process , but nonetheless can still be used to significantly improve performance on questions requiring external knowledge ( such as ’",0,Conclusions,1,1
372,Class - Activation Mapping,Change in Class - Activation Mapping :,"As summarized in Table 4 , we observe an average 3.4 % improvement across five different networks , implying better localization accuracy .","To qualitatively study the improvement in localization due to PC , we obtain samples from the CUB - 200 - 2011 dataset and visualize the localization regions returned from Grad - CAM for both the baseline and PC - trained VGG - 16 model .",1,Improvement in Localization Ability,1,0
770,convnet features,"In this paper , we provide evidence that convnet features perform at least as well as conventional ones , even in the regime of point - to - point correspondence , and we show considerable performance improvement in certain settings , including category - level keypoint prediction .","Or do large receptive fields mean that correspondence is effectively pooled away , making this a task better suited for hand - engineered features ?",,1,Introduction,0,1
186,CTC - LSTM models,These features were also used in the original work on CTC - LSTM models for speech recognition .,The input features for these experiments were 123 dimensional FBANK features ( 40 + energy + + ) .,The CTC layer itself was trained on the 61 label set .,0,CTC setup,1,0
573,convolutional image filters,"The spatial term uses the FoE model from , while the data term replaces traditional derivative filters with a set of learned convolutional image filters .",They formulate a classical flow problem with a data term and a spatial term .,"With limited training data and a small set of filters , it did not fully show the full promise of learning flow .",0,Related Work,0,1
301,fully - connected linear layer,"Finally , a fully - connected linear layer projects to the output of the network , i.e. , the Q - values .",All these layers are separated by Rectifier Linear Units ( ReLu ) .,The optimization employed to train the network is RMSProp ( with momentum parameter ) .,1,Network Architecture,1,0
123,linearized and anonymized AMR graph,"Following konstas2017neural , the input sequence is a linearized and anonymized AMR graph .",,Linearization is used to convert the graph into a sequence : The depth - first traversal of the graph defines the indexing between nodes and tokens in the sequence .,1,Sequential AMRs,0,0
971,Inter - Algorithm Normalization,subsubsection : Inter - Algorithm Normalization,"For example , humans often play games without seeking to maximize score ; humans also benefit from prior knowledge that is difficult to incorporate into domain - independent agents .",A third alternative is to normalize using the scores achieved by the algorithms themselves .,1,Inter - Algorithm Normalization,0,0
559,Autoregressive NMT without RNNs,paragraph : Autoregressive NMT without RNNs,Choosing to factorize the machine translation output distribution autoregressively enables straightforward maximum likelihood training with a cross - entropy loss applied at each decoding step : This loss provides direct supervision for each conditional probability prediction .,"Since the entire target translation is known at training time , the calculation of later conditional probabilities ( and their corresponding losses ) does not depend on the output words chosen during earlier decoding steps .",0,Autoregressive NMT without RNNs,0,1
474,stochastic approximations,"Thus , the gradients and are stochastic approximations to the true gradients .","If the true gradients are and , then we can define and with random variables and .","Consequently , we analyze convergence of GANs by two time - scale stochastic approximations algorithms .",1,Two Time - Scale Update Rule for GANs,1,0
882,RoI,"For the RoI pooling layer , its inputs are a predicted box and the convolutional feature map , both being functions of .",( [ reference ] ) lies on the spatial transform of a predicted box that determines RoI pooling .,"In Fast R - CNN , the box proposals are pre - computed and fixed , and the backpropagation of RoI pooling layer in only involves .",0,End - to - End Training,1,1
168,bimodal,"Similarly to bimodal fusion ( sec : bimodal ) , after trimodal fusion we pass the fused features through to incorporate contextual information in them , which yields where , is scalar for , , , and is the context - aware trimodal feature vector .","So , we define the fused features as where , is scalar for and .",,0,Trimodal fusion,1,1
167,self - supervised way,"Instead , it is trained to encode 3D model views in a self - supervised way , overcoming the need of a large pose - annotated dataset .","Finally , the AAE does not require any real pose - annotated training data .",A schematic overview of the approach is shown in Fig [ reference ] .,0,Introduction,1,0
369,multi - layer bidirectional Transformer,"For example , BERT is based on a multi - layer bidirectional Transformer , and is trained on plain text for masked word prediction and next sentence prediction tasks .",These are neural network language models trained on text data using unsupervised objectives .,"To apply a pre - trained model to specific NLU tasks , we often need to fine - tune , for each task , the model with additional task - specific layers using task - specific training data .",0,Introduction,1,0
177,speech systems,"Deep Speech also handles challenging noisy environments better than widely used , state - of - the - art commercial speech systems .","Our system , called Deep Speech , outperforms previously published results on the widely studied Switchboard Hub5’00 , achieving 16.0 % error on the full test set .",,1,Deep Speech : Scaling up end - to - end speech recognition,1,0
26,dilated models,"Also , the dilated models outperform their regular counterparts , Vanilla ( did n’t converge , omitted ) , LSTM and GRU , without increasing the model complexity .","To the best of our knowledge , the dilated GRU with 1.27 BPC achieves the best result among models of similar sizes without layer normalizations .",,1,Language modeling,0,1
5,MMD - rep - b,"[ reference ] shows that : When PICO was used , the hinge , MMD - rbf and MMD - rep methods were sensitive to the choices of while MMD - rep - b was robust .",Fig .,"For hinge and MMD - rbf , higher may result in better FID scores and less diverged cases over 16 learning rate combinations .",1,Experiments,1,0
657,VQA models,"Visual features from CNN already have implicit attention and selectivity over the image region , thus the resulting class activation maps are similar to the maps generated by the attention mechanisms of the VQA models in .",The example in lower part of Figure [ reference ] shows the heatmaps generated by two different questions and answers .,,0,Understanding the Visual QA model,1,0
405,1 - hidden - layer mlp generative model,the overall architecture for a Mult - vae pr / Mult - dae with 1 - hidden - layer mlp generative model would be [ I → 600 → 200,"As a concrete example , recall I is the total number of items ,",→ 600 → I ] .,0,Experimental setup,1,1
319,Inverse of Block Matrices,appendix : Inverse of Block Matrices,bibliography : References,,1,Inverse of Block Matrices,0,1
601,deep model,"Unexpectedly , such degradation is not caused by overfitting , and adding more layers to a suitably deep model leads to higher training error , as reported in [ reference ][ reference ] and thoroughly verified by our experiments .","When deeper networks are able to start converging , a degradation problem has been exposed : with the network depth increasing , accuracy gets saturated ( which might be unsurprising ) and then degrades rapidly .",Fig .,0,Introduction,1,1
702,domain discriminator,"For example , we can set the architecture of the domain discriminator to be the layer - by - layer concatenation of two replicas of the label predictor followed by a two layer non - linear perceptron aimed to learn the XOR - function .",We assume that the family of domain classifiers is rich enough to contain the symmetric difference hypothesis set of : It is not an unrealistic assumption as we have a freedom to pick whichever we want .,"Given the assumption holds , one can easily show that training the is closely related to the estimation of .",0,Relation to - distance,1,1
238,SDE,"The SDE dynamics are governed by a "" noise scale "" g ≈ N / B for the learning rate , N the training dataset size , and B the batch size .",The authors argued that the SGD algorithm can be derived through Euler - Maruyama discretization of a Stochastic Differential Equation ( SDE ) .,They conclude that a higher noise scale prevents SGD from settling into sharper minima .,0,Related Work,0,1
648,GMMs,"With this in mind , it is easy to generalize GMMs in a multi - layered fashion .","⇡ iN x|bi , AiATi .","Instead of sampling one transformation from a set , we can sample a path of transformations in a network of k layers , see Figure 1 ( c ) .",0,Title,1,0
347,sequence - based LSTM,"As noted in , the simple BOW model performs roughly as well if not better than the sequence - based LSTM for the VQA task .","Other question representations , such as an LSTM , can also be used , however , BOW has fewer parameters yet has shown good performance .","Specifically , we compute where represents the BOW weights for word vectors , and is the bias term .",1,Word Guided Spatial Attention in One - Hop Model,0,1

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('release_data/dev.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = []\n",
    "for json_str in json_list:\n",
    "    result.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = {}\n",
    "for paper in result:\n",
    "    # Each word in the paper will be a single entry in the following sentences array.\n",
    "    # The entry consists of the start and end index of the sentences.\n",
    "    # Later, also the section start and end index, as well as the section name\n",
    "    # and section index will be added to the object.\n",
    "    sentences = []  # a list of objects.\n",
    "    for sentence in paper['sentences']:\n",
    "        for i in range(sentence[0], sentence[1]):\n",
    "            # For each word in the sentence, add an entry that contains the start and end index for the sentence of the word.\n",
    "            sentences.append({'start': sentence[0], 'end': sentence[1]})\n",
    "\n",
    "    # Populate the sentences list with section information.\n",
    "    for index, section in enumerate(paper['sections']):\n",
    "        # Get the first sentence of the section.\n",
    "        sentence = sentences[section[0]]\n",
    "        # The section name is the first sentence of the section.\n",
    "        section_name = paper['words'][sentence['start']:sentence['end']]\n",
    "        \n",
    "        # Example for the first sentence on a section:\n",
    "        # [\"section\", \":\", \"Abstract\"]\n",
    "        # If the first sentence starts with [\"section\", \":\"], we are only interested in the words after that prefix.\n",
    "        if len(section_name) >= 2 and section_name[1] == \":\":\n",
    "            section_name_length = len(section_name)\n",
    "            section_name = section_name[2:]\n",
    "        else:\n",
    "            section_name_length = 0\n",
    "            if index == 0:\n",
    "                # First section will always be labled as 'Title'\n",
    "                section_name = ['Title']\n",
    "            else:\n",
    "                section_name = []\n",
    "        \n",
    "        # Add section info\n",
    "        for i in range(section[0], section[1]):\n",
    "            sentences[i]['section'] = {'name': section_name, 'index': index, 'start': section[0] + section_name_length, 'end': section[1]}\n",
    "\n",
    "    # Iterate through all ranges of named entities and retrieve the corresponding sentence and section info for that word\n",
    "    # by querying the index from the sentences array.\n",
    "    words = paper['words']\n",
    "    for entity in paper['ner']:\n",
    "        # info is an object with\n",
    "        info = sentences[entity[0]]\n",
    "        begin_of_section = info['start'] == info['section']['start']\n",
    "        end_of_section = info['end'] == info['section']['end']\n",
    "\n",
    "        # The sentence will be a list of words\n",
    "        sentence = words[info['start']:info['end']]\n",
    "\n",
    "        # Add the previous sentence, but only if the current sentence is not the start of the section.\n",
    "        if not begin_of_section:\n",
    "            pre_info = sentences[info['start'] - 1]\n",
    "            pre_sentence = words[pre_info['start']:pre_info['end']]\n",
    "        else:\n",
    "            pre_sentence = []\n",
    "\n",
    "        # Add the next sentence, but only if the current sentence is not the end of the section.\n",
    "        if not end_of_section:\n",
    "            post_info = sentences[info['end'] + 1]\n",
    "            post_sentence = words[post_info['start']:post_info['end']]\n",
    "        else:\n",
    "            post_sentence = []\n",
    "\n",
    "        entity_name = words[entity[0]:entity[1]]\n",
    "        # Create an entry for the extracted entity\n",
    "        doc_id = paper['doc_id']\n",
    "        entry = {\n",
    "                    'doc_id': doc_id,                                     # Document id\n",
    "                    'relation': entity[2],                                # Classification (method, dataset, metric, ...)\n",
    "                    'ner': \" \".join(entity_name),                         # Name of the entity\n",
    "                    'sentence':  \" \".join(sentence),                      # Sentence = array of words\n",
    "                    'pre_sentence': \" \".join(pre_sentence),               # Previous sentence\n",
    "                    'post_sentence': \" \".join(post_sentence),             # Next sentence\n",
    "                    'section_name' : \" \".join(info['section']['name']),   # Name of the section\n",
    "                    'section_index' : info['section']['index']            # Index of the section\n",
    "                }\n",
    "        if doc_id not in content:\n",
    "            content[doc_id] = []\n",
    "        content[doc_id].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Attention Boosted Sequential Inference Model',\n",
       "  'sentence': 'document : Attention Boosted Sequential Inference Model',\n",
       "  'pre_sentence': 'bibliography : References',\n",
       "  'post_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Attention mechanism',\n",
       "  'sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language processing',\n",
       "  'sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention boosted natural language inference model',\n",
       "  'sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'pre_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'post_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'pre_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'post_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'word attention',\n",
       "  'sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'pre_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'post_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'adaptive direction - oriented attention mechanisms',\n",
       "  'sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'pre_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'post_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer of natural language inference models',\n",
       "  'sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'pre_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'post_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'pre_sentence': 'Attention mechanism has been proven effective on natural language processing .',\n",
       "  'post_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'inference model aESIM',\n",
       "  'sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'pre_sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'post_sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'local subsentential inference',\n",
       "  'sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'pre_sentence': 'This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction - oriented attention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM .',\n",
       "  'post_sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'pre_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'post_sentence': 'acmlicensed [ DAPA’19 ] DAPA2019WSDMWorkshoponDeepmatchinginPracticalApplicationsFebruary15th , 2019Melbourne , Australia 2019',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'MultiNLI',\n",
       "  'sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'pre_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'post_sentence': 'acmlicensed [ DAPA’19 ] DAPA2019WSDMWorkshoponDeepmatchinginPracticalApplicationsFebruary15th , 2019Melbourne , Australia 2019',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quora',\n",
       "  'sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'pre_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'post_sentence': 'acmlicensed [ DAPA’19 ] DAPA2019WSDMWorkshoponDeepmatchinginPracticalApplicationsFebruary15th , 2019Melbourne , Australia 2019',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'pre_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'post_sentence': 'acmlicensed [ DAPA’19 ] DAPA2019WSDMWorkshoponDeepmatchinginPracticalApplicationsFebruary15th , 2019Melbourne , Australia 2019',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'The empirical studies on the SNLI , MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model .',\n",
       "  'pre_sentence': 'This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .',\n",
       "  'post_sentence': 'acmlicensed [ DAPA’19 ] DAPA2019WSDMWorkshoponDeepmatchinginPracticalApplicationsFebruary15th , 2019Melbourne , Australia 2019',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 0},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'Natural language inference',\n",
       "  'sentence': 'Natural language inference ( NLI ) is an important and significant task in natural language processing ( NLP ) .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLI )',\n",
       "  'sentence': 'Natural language inference ( NLI ) is an important and significant task in natural language processing ( NLP ) .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language processing ( NLP )',\n",
       "  'sentence': 'Natural language inference ( NLI ) is an important and significant task in natural language processing ( NLP ) .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language inference',\n",
       "  'sentence': 'Table [ reference ] shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .',\n",
       "  'pre_sentence': 'It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .',\n",
       "  'post_sentence': 'In the literature , the task of NLI is usually viewed as a relation classification .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLI',\n",
       "  'sentence': 'In the literature , the task of NLI is usually viewed as a relation classification .',\n",
       "  'pre_sentence': 'Table [ reference ] shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .',\n",
       "  'post_sentence': 'It learns the relation between a premise and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'relation classification',\n",
       "  'sentence': 'In the literature , the task of NLI is usually viewed as a relation classification .',\n",
       "  'pre_sentence': 'Table [ reference ] shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .',\n",
       "  'post_sentence': 'It learns the relation between a premise and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLI',\n",
       "  'sentence': 'The existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .',\n",
       "  'pre_sentence': 'It learns the relation between a premise and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .',\n",
       "  'post_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'feature - based models',\n",
       "  'sentence': 'The existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .',\n",
       "  'pre_sentence': 'It learns the relation between a premise and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .',\n",
       "  'post_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'neural network - based models',\n",
       "  'sentence': 'The existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .',\n",
       "  'pre_sentence': 'It learns the relation between a premise and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .',\n",
       "  'post_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Feature - based models',\n",
       "  'sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'pre_sentence': 'The existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .',\n",
       "  'post_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'classifier',\n",
       "  'sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'pre_sentence': 'The existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .',\n",
       "  'post_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'relation classification',\n",
       "  'sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'pre_sentence': 'The existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .',\n",
       "  'post_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'end - to - end neural network - based models',\n",
       "  'sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'pre_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'post_sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLP tasks',\n",
       "  'sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'pre_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'post_sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'machine translation',\n",
       "  'sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'pre_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'post_sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language inference',\n",
       "  'sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'pre_sentence': 'Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as - gram length and the real - valued feature of length difference , then train a classifier to perform relation classification .',\n",
       "  'post_sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'neural network - based models',\n",
       "  'sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'pre_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'post_sentence': 'The architectures of the two types of models are shown in Figure [ reference ] .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLI',\n",
       "  'sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'pre_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'post_sentence': 'The architectures of the two types of models are shown in Figure [ reference ] .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence encoding models',\n",
       "  'sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'pre_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'post_sentence': 'The architectures of the two types of models are shown in Figure [ reference ] .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence interaction - aggregation models',\n",
       "  'sentence': 'On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .',\n",
       "  'pre_sentence': 'Recently , end - to - end neural network - based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .',\n",
       "  'post_sentence': 'The architectures of the two types of models are shown in Figure [ reference ] .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Sentence encoding models',\n",
       "  'sentence': 'Sentence encoding models ( their main architecture is shown in Figure [ reference ] .a ) independently encode a pair of sentences , a premise and a hypothesis using pre - trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .',\n",
       "  'pre_sentence': 'The architectures of the two types of models are shown in Figure [ reference ] .',\n",
       "  'post_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'multi - layer perceptron ( MLP',\n",
       "  'sentence': 'Sentence encoding models ( their main architecture is shown in Figure [ reference ] .a ) independently encode a pair of sentences , a premise and a hypothesis using pre - trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .',\n",
       "  'pre_sentence': 'The architectures of the two types of models are shown in Figure [ reference ] .',\n",
       "  'post_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM ( Long Short - Term Memory networks',\n",
       "  'sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'pre_sentence': 'Sentence encoding models ( their main architecture is shown in Figure [ reference ] .a ) independently encode a pair of sentences , a premise and a hypothesis using pre - trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .',\n",
       "  'post_sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'GRU ( Gated Recurrent Units',\n",
       "  'sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'pre_sentence': 'Sentence encoding models ( their main architecture is shown in Figure [ reference ] .a ) independently encode a pair of sentences , a premise and a hypothesis using pre - trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .',\n",
       "  'post_sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'pre_sentence': 'Sentence encoding models ( their main architecture is shown in Figure [ reference ] .a ) independently encode a pair of sentences , a premise and a hypothesis using pre - trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .',\n",
       "  'post_sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLI training scheme',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence encoding architectures',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM or GRU',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'mean / max pooling',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'self - attention network',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'hierarchical convolutional networks',\n",
       "  'sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'pre_sentence': 'In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .',\n",
       "  'post_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'pre_sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'post_sentence': 'Talman et al . designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'max pooling',\n",
       "  'sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'pre_sentence': 'For example , Conneau et al . proposed a generic NLI training scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , self - attention network and hierarchical convolutional networks .',\n",
       "  'post_sentence': 'Talman et al . designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'hierarchical Bi - LSTM max pooling ( HBMP ) model',\n",
       "  'sentence': 'Talman et al . designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .',\n",
       "  'pre_sentence': 'The experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .',\n",
       "  'post_sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'pre_sentence': 'Talman et al . designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .',\n",
       "  'post_sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'pre_sentence': 'Talman et al . designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .',\n",
       "  'post_sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'pre_sentence': 'Talman et al . designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .',\n",
       "  'post_sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM',\n",
       "  'sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'pre_sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'post_sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention mechanisms',\n",
       "  'sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'pre_sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'post_sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'sentence encoding',\n",
       "  'sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'pre_sentence': 'This model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown better results than the model with a single Bi - LSTM .',\n",
       "  'post_sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM model',\n",
       "  'sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'pre_sentence': 'Besides LSTM , attention mechanisms could also be used to boost the effectiveness of sentence encoding .',\n",
       "  'post_sentence': 'Sentence interaction - aggregation models ( their main architecture is shown in Figure [ reference ] .b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the final decision .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Sentence interaction - aggregation models',\n",
       "  'sentence': 'Sentence interaction - aggregation models ( their main architecture is shown in Figure [ reference ] .b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the final decision .',\n",
       "  'pre_sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'post_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'vector representations',\n",
       "  'sentence': 'Sentence interaction - aggregation models ( their main architecture is shown in Figure [ reference ] .b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the final decision .',\n",
       "  'pre_sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'post_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence encoding models',\n",
       "  'sentence': 'Sentence interaction - aggregation models ( their main architecture is shown in Figure [ reference ] .b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the final decision .',\n",
       "  'pre_sentence': 'The model developed by Ghaeini et al . added self - attention to LSTM model , and achieved better performance .',\n",
       "  'post_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence encoding model',\n",
       "  'sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'pre_sentence': 'Sentence interaction - aggregation models ( their main architecture is shown in Figure [ reference ] .b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the final decision .',\n",
       "  'post_sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence interaction - aggregation models',\n",
       "  'sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'pre_sentence': 'Sentence interaction - aggregation models ( their main architecture is shown in Figure [ reference ] .b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the final decision .',\n",
       "  'post_sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'machine translation task',\n",
       "  'sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'pre_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'post_sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention mechanism',\n",
       "  'sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'pre_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'post_sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language process',\n",
       "  'sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'pre_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'post_sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLP',\n",
       "  'sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'pre_sentence': 'Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .',\n",
       "  'post_sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'pairwise word interaction model',\n",
       "  'sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'pre_sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'post_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'PWIM )',\n",
       "  'sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'pre_sentence': 'Bahdanau et al . translated and aligned text simultaneously in machine translation task , innovatively introducing attention mechanism to natural language process ( NLP ) .',\n",
       "  'post_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'bilateral multi - perspective matching ( BiMPM ) model',\n",
       "  'sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'pre_sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'post_sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'matching strategies',\n",
       "  'sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'pre_sentence': 'He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level fine - grained information .',\n",
       "  'post_sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence interation - aggregation models',\n",
       "  'sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'pre_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'post_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'pre_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'post_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Enhanced Sequential Inference Model',\n",
       "  'sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'pre_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'post_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sequential inference model',\n",
       "  'sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'pre_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'post_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'chain LSTMs',\n",
       "  'sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'pre_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'post_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence encoding models',\n",
       "  'sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'pre_sentence': 'Wang et al . put forward a bilateral multi - perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as different types of attention .',\n",
       "  'post_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'pre_sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'post_sentence': 'Word attention can characterize the different contribution of each word .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'pre_sentence': 'The empirical studies of Lan et al . and Chen et al . concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .',\n",
       "  'post_sentence': 'Word attention can characterize the different contribution of each word .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'Word attention',\n",
       "  'sentence': 'Word attention can characterize the different contribution of each word .',\n",
       "  'pre_sentence': 'Although ESIM has achieved excellent achievements , this model does n’t consider the attention along the words in a sentence in its Bi - LSTM layer .',\n",
       "  'post_sentence': 'Therefore , it will be beneficial to put word attention into the Bi - LTSM layer .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LTSM layer',\n",
       "  'sentence': 'Therefore , it will be beneficial to put word attention into the Bi - LTSM layer .',\n",
       "  'pre_sentence': 'Word attention can characterize the different contribution of each word .',\n",
       "  'post_sentence': 'Moreover , the orientation of the words represents the direction of the information flow , either forward or backward , should not be ignored .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM model',\n",
       "  'sentence': 'In traditional Bi - LSTM model , the forward and the backward vectors learnt by Bi - LSTM are simply jointed .',\n",
       "  'pre_sentence': 'Moreover , the orientation of the words represents the direction of the information flow , either forward or backward , should not be ignored .',\n",
       "  'post_sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'In traditional Bi - LSTM model , the forward and the backward vectors learnt by Bi - LSTM are simply jointed .',\n",
       "  'pre_sentence': 'Moreover , the orientation of the words represents the direction of the information flow , either forward or backward , should not be ignored .',\n",
       "  'post_sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'word encoding',\n",
       "  'sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'pre_sentence': 'In traditional Bi - LSTM model , the forward and the backward vectors learnt by Bi - LSTM are simply jointed .',\n",
       "  'post_sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'pre_sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'post_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention layer',\n",
       "  'sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'pre_sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'post_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'pre_sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'post_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'adaptive orientation embedding layer',\n",
       "  'sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'pre_sentence': 'It ’s necessary to consider whether each orientation ( forward or backward ) has different importance on word encoding , thus adaptively joint the two orientation vectors together with different weights .',\n",
       "  'post_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention boosted Bi - LSTM',\n",
       "  'sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'pre_sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'post_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'pre_sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'post_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'pre_sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'post_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'pre_sentence': 'Therefore , in this study , using ESIM model as the baseline , we add an attention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .',\n",
       "  'post_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'pre_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'post_sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'MultiNLI',\n",
       "  'sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'pre_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'post_sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quora',\n",
       "  'sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'pre_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'post_sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'pre_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'post_sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'pre_sentence': 'We name this attention boosted Bi - LSTM as Bi - aLSTM , and denote the modified ESIM as aESIM .',\n",
       "  'post_sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'pre_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'post_sentence': 'This paper is organized as follows .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLP tasks',\n",
       "  'sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'pre_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'post_sentence': 'This paper is organized as follows .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'text classification',\n",
       "  'sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'pre_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'post_sentence': 'This paper is organized as follows .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'machine translation',\n",
       "  'sentence': 'We believe that the architecture of Bi - aLSTM has potentially to be used in other NLP tasks such as text classification , machine translation and so on .',\n",
       "  'pre_sentence': 'Experimental results on SNLI , MultiNLI and Quora benchmarks have demonstrated better performance of aESIM model than that of the baseline ESIM and the other state - of - the - art models .',\n",
       "  'post_sentence': 'This paper is organized as follows .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'We introduce the general frameworks of ESIM and aESIM in Section 2 .',\n",
       "  'pre_sentence': 'This paper is organized as follows .',\n",
       "  'post_sentence': 'We describe the datasets and the experiment settings , and analyze our experimental results in Section 3 .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'We introduce the general frameworks of ESIM and aESIM in Section 2 .',\n",
       "  'pre_sentence': 'This paper is organized as follows .',\n",
       "  'post_sentence': 'We describe the datasets and the experiment settings , and analyze our experimental results in Section 3 .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Attention Boosted Sequential Inference Model',\n",
       "  'sentence': 'section : Attention Boosted Sequential Inference Model',\n",
       "  'pre_sentence': 'We then draw conclusions in Section 4 .',\n",
       "  'post_sentence': 'Supposed that we have two sentences and , where represents premise and represents hypothesis .',\n",
       "  'section_name': 'Attention Boosted Sequential Inference Model',\n",
       "  'section_index': 2},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'subsection : ESIM model',\n",
       "  'pre_sentence': 'The goal is to predict the label meaning for their relation .',\n",
       "  'post_sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Enhanced Sequential Inference Model',\n",
       "  'sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'input encoding layer',\n",
       "  'sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'local inference modeling layer',\n",
       "  'sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'inference composition layer',\n",
       "  'sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'classification layer',\n",
       "  'sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'input encoding layer',\n",
       "  'sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'pre_sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'post_sentence': 'Secondly , ESIM implements the local inference layer for enhancing the sentence information .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'pre_sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'post_sentence': 'Secondly , ESIM implements the local inference layer for enhancing the sentence information .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'pre_sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'post_sentence': 'Secondly , ESIM implements the local inference layer for enhancing the sentence information .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'word embeddings',\n",
       "  'sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'pre_sentence': 'Enhanced Sequential Inference Model ( ESIM ) is composed of four main components : input encoding layer , local inference modeling layer , inference composition layer and classification layer .',\n",
       "  'post_sentence': 'Secondly , ESIM implements the local inference layer for enhancing the sentence information .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Secondly , ESIM implements the local inference layer for enhancing the sentence information .',\n",
       "  'pre_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'post_sentence': 'First it calculates a similarity matrix based on and .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'local inference layer',\n",
       "  'sentence': 'Secondly , ESIM implements the local inference layer for enhancing the sentence information .',\n",
       "  'pre_sentence': 'In the input encoding layer , ESIM first uses Bi - LSTM layer to encode input sentence pairs ( Equations 1 - 2 ) , which can be initialized using pre - trained word embeddings ( e.g. Glove 840B vectors ) , where is the word embedding vector of the - th word in , is that of word in .',\n",
       "  'post_sentence': 'First it calculates a similarity matrix based on and .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'local inference',\n",
       "  'sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'pre_sentence': 'It further enhances the local inference information collected as below .',\n",
       "  'post_sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'pre_sentence': 'It further enhances the local inference information collected as below .',\n",
       "  'post_sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'inference composition',\n",
       "  'sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'pre_sentence': 'It further enhances the local inference information collected as below .',\n",
       "  'post_sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'summation',\n",
       "  'sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'pre_sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'post_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'pre_sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'post_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'max and average pooling',\n",
       "  'sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'pre_sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'post_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'classifier',\n",
       "  'sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'pre_sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'post_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'fully connected multi - layer perceptron',\n",
       "  'sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'pre_sentence': 'After the enhancement of local inference , another Bi - LSTM layer is used to capture local inference information and their context for inference composition .',\n",
       "  'post_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM1 layer',\n",
       "  'sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'pre_sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'post_sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'encoding layer',\n",
       "  'sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'pre_sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'post_sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM2 layer',\n",
       "  'sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'pre_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'local inference layer',\n",
       "  'sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'pre_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'inference composition layer',\n",
       "  'sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'pre_sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'subsection : aESIM model',\n",
       "  'pre_sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'post_sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention boosted sequential inference model',\n",
       "  'sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM )',\n",
       "  'sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'pre_sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'post_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'encoding layer',\n",
       "  'sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'pre_sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'post_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'local inference modeling layer',\n",
       "  'sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'pre_sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'post_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'decoding layer',\n",
       "  'sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'pre_sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'post_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'classification layer',\n",
       "  'sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'pre_sentence': 'The overall architecture of our newly proposed attention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .',\n",
       "  'post_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layers',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM1',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'LSTM2',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM layers',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'pre_sentence': 'In detail , aESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classification layer .',\n",
       "  'post_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'pre_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'post_sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM layers',\n",
       "  'sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'pre_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'post_sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'pre_sentence': 'The only difference between ESIM and aESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - aLSTM layers in aESIM .',\n",
       "  'post_sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'word embeddings',\n",
       "  'sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'pre_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'post_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM layer',\n",
       "  'sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'pre_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'post_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'local inference modeling layer',\n",
       "  'sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'pre_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'post_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM layer',\n",
       "  'sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'pre_sentence': 'Therefore , as illustrated in Figure [ reference ] , the layers with red - dotted circles in ESIM will be replaced by the Bi - aLSTM layers shown in the right upper corner of the Figure [ reference ] and the details of Bi - aLSTM can be found in Figure [ reference ] .',\n",
       "  'post_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'forward LSTM layer',\n",
       "  'sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'pre_sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'post_sentence': 'As described in introduction section , in the following newly proposed Bi - aLSTM , we add word attention and additive operation on both orientations of traditional Bi - LSTM layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'backward LSTM layer',\n",
       "  'sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'pre_sentence': 'Given the word vector of the - th word in sentence , which can be obtained by pre - trained word embeddings such as Glove 840B vectors in the first Bi - aLSTM layer or obtained from the local inference modeling layer in the second Bi - aLSTM layer .',\n",
       "  'post_sentence': 'As described in introduction section , in the following newly proposed Bi - aLSTM , we add word attention and additive operation on both orientations of traditional Bi - LSTM layer .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'As described in introduction section , in the following newly proposed Bi - aLSTM , we add word attention and additive operation on both orientations of traditional Bi - LSTM layer .',\n",
       "  'pre_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'post_sentence': 'Word attention layer',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'additive operation',\n",
       "  'sentence': 'As described in introduction section , in the following newly proposed Bi - aLSTM , we add word attention and additive operation on both orientations of traditional Bi - LSTM layer .',\n",
       "  'pre_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'post_sentence': 'Word attention layer',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'As described in introduction section , in the following newly proposed Bi - aLSTM , we add word attention and additive operation on both orientations of traditional Bi - LSTM layer .',\n",
       "  'pre_sentence': 'We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information and .',\n",
       "  'post_sentence': 'Word attention layer',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Word attention layer',\n",
       "  'sentence': 'Word attention layer',\n",
       "  'pre_sentence': 'As described in introduction section , in the following newly proposed Bi - aLSTM , we add word attention and additive operation on both orientations of traditional Bi - LSTM layer .',\n",
       "  'post_sentence': 'It ’s obvious that not all words contribute equally to the representation of a sentence .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Attention mechanism',\n",
       "  'sentence': 'Attention mechanism , which is introduced in , is extremely effective to extract vital words from the whole sentence , and is particularly beneficial to generate the sentence vector .',\n",
       "  'pre_sentence': 'It ’s obvious that not all words contribute equally to the representation of a sentence .',\n",
       "  'post_sentence': 'Therefore , we use the following attention mechanism after we get and .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention mechanism',\n",
       "  'sentence': 'Therefore , we use the following attention mechanism after we get and .',\n",
       "  'pre_sentence': 'Attention mechanism , which is introduced in , is extremely effective to extract vital words from the whole sentence , and is particularly beneficial to generate the sentence vector .',\n",
       "  'post_sentence': 'Suppose , we then have where is obtained after one - layer MLP for the input , is the importance of word , is calculated by the SoftMax unit on the context vector of the sentence which is randomly initialized and modified during the training , is the attention enhanced vector through multiplying the weight and original vector , where correspond to the forward vector and the backward vector , respectively .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'one - layer MLP',\n",
       "  'sentence': 'Suppose , we then have where is obtained after one - layer MLP for the input , is the importance of word , is calculated by the SoftMax unit on the context vector of the sentence which is randomly initialized and modified during the training , is the attention enhanced vector through multiplying the weight and original vector , where correspond to the forward vector and the backward vector , respectively .',\n",
       "  'pre_sentence': 'Therefore , we use the following attention mechanism after we get and .',\n",
       "  'post_sentence': 'Adaptive word direction layer In traditional Bi - LSTM model , the forward and the backward vectors of a word are considered to have equal importance on the word representation .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'SoftMax unit',\n",
       "  'sentence': 'Suppose , we then have where is obtained after one - layer MLP for the input , is the importance of word , is calculated by the SoftMax unit on the context vector of the sentence which is randomly initialized and modified during the training , is the attention enhanced vector through multiplying the weight and original vector , where correspond to the forward vector and the backward vector , respectively .',\n",
       "  'pre_sentence': 'Therefore , we use the following attention mechanism after we get and .',\n",
       "  'post_sentence': 'Adaptive word direction layer In traditional Bi - LSTM model , the forward and the backward vectors of a word are considered to have equal importance on the word representation .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Adaptive word direction layer',\n",
       "  'sentence': 'Adaptive word direction layer In traditional Bi - LSTM model , the forward and the backward vectors of a word are considered to have equal importance on the word representation .',\n",
       "  'pre_sentence': 'Suppose , we then have where is obtained after one - layer MLP for the input , is the importance of word , is calculated by the SoftMax unit on the context vector of the sentence which is randomly initialized and modified during the training , is the attention enhanced vector through multiplying the weight and original vector , where correspond to the forward vector and the backward vector , respectively .',\n",
       "  'post_sentence': 'The model simply connects the forward and backward vectors head and tail without weighing their importance .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM model',\n",
       "  'sentence': 'Adaptive word direction layer In traditional Bi - LSTM model , the forward and the backward vectors of a word are considered to have equal importance on the word representation .',\n",
       "  'pre_sentence': 'Suppose , we then have where is obtained after one - layer MLP for the input , is the importance of word , is calculated by the SoftMax unit on the context vector of the sentence which is randomly initialized and modified during the training , is the attention enhanced vector through multiplying the weight and original vector , where correspond to the forward vector and the backward vector , respectively .',\n",
       "  'post_sentence': 'The model simply connects the forward and backward vectors head and tail without weighing their importance .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'word representation',\n",
       "  'sentence': 'Adaptive word direction layer In traditional Bi - LSTM model , the forward and the backward vectors of a word are considered to have equal importance on the word representation .',\n",
       "  'pre_sentence': 'Suppose , we then have where is obtained after one - layer MLP for the input , is the importance of word , is calculated by the SoftMax unit on the context vector of the sentence which is randomly initialized and modified during the training , is the attention enhanced vector through multiplying the weight and original vector , where correspond to the forward vector and the backward vector , respectively .',\n",
       "  'post_sentence': 'The model simply connects the forward and backward vectors head and tail without weighing their importance .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'adaptive direction layer',\n",
       "  'sentence': 'Therefore , we propose a new adaptive direction layer to learn the contribution of different directions for a single word .',\n",
       "  'pre_sentence': 'Thus , different direction vectors of a word make different contribution to the representation , especially the words in a long sentence .',\n",
       "  'post_sentence': 'Formally , given two direction word vectors and , the whole word vector can be expressed as : where , and denote weight matrix and the bias , denotes the nonlinear function , denotes the concentration .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'word and orientation enhanced Bi - LSTM',\n",
       "  'sentence': 'Then we can get the whole sentence vector as below : This word and orientation enhanced Bi - LSTM is called Bi - aLSTM .',\n",
       "  'pre_sentence': 'All the parameters can be learned during training .',\n",
       "  'post_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'Then we can get the whole sentence vector as below : This word and orientation enhanced Bi - LSTM is called Bi - aLSTM .',\n",
       "  'pre_sentence': 'All the parameters can be learned during training .',\n",
       "  'post_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'pre_sentence': 'Then we can get the whole sentence vector as below : This word and orientation enhanced Bi - LSTM is called Bi - aLSTM .',\n",
       "  'post_sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layers',\n",
       "  'sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'pre_sentence': 'Then we can get the whole sentence vector as below : This word and orientation enhanced Bi - LSTM is called Bi - aLSTM .',\n",
       "  'post_sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language inference',\n",
       "  'sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'pre_sentence': 'Then we can get the whole sentence vector as below : This word and orientation enhanced Bi - LSTM is called Bi - aLSTM .',\n",
       "  'post_sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'pre_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'pre_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM models',\n",
       "  'sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'pre_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'sentimental classification task',\n",
       "  'sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'pre_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'deep learning models',\n",
       "  'sentence': 'We selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets .',\n",
       "  'pre_sentence': 'the Stanford Natural Language Inference ( SNLI ) corpus , the Multi - Genre Natural Language Inference ( MultiNLI ) corpus , and Quora duplicate question dataset .',\n",
       "  'post_sentence': 'The Stanford Natural Language Inference ( SNLI',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'generalization ability',\n",
       "  'sentence': 'We selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets .',\n",
       "  'pre_sentence': 'the Stanford Natural Language Inference ( SNLI ) corpus , the Multi - Genre Natural Language Inference ( MultiNLI ) corpus , and Quora duplicate question dataset .',\n",
       "  'post_sentence': 'The Stanford Natural Language Inference ( SNLI',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'SNLI',\n",
       "  'pre_sentence': 'We selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets .',\n",
       "  'post_sentence': 'The Stanford Natural Language Inference ( SNLI',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'MultiNLI',\n",
       "  'sentence': 'MultiNLI',\n",
       "  'pre_sentence': 'Consequently , we remove all pairs with relation ’ - ’ during training , validating and testing processes .',\n",
       "  'post_sentence': 'This corpus is a crowd - sourced collection of 433 K sentence pairs annotated with textual entailment information .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'cross - genre generation evaluation',\n",
       "  'sentence': 'The corpus is modeled on the SNLI corpus , but differs in that covers a range of genres of spoken and written text , and supports a distinctive cross - genre generation evaluation .',\n",
       "  'pre_sentence': 'This corpus is a crowd - sourced collection of 433 K sentence pairs annotated with textual entailment information .',\n",
       "  'post_sentence': 'The Quora dataset contains 400 , 000 question pairs .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quora',\n",
       "  'sentence': 'Quora',\n",
       "  'pre_sentence': 'The corpus is modeled on the SNLI corpus , but differs in that covers a range of genres of spoken and written text , and supports a distinctive cross - genre generation evaluation .',\n",
       "  'post_sentence': 'The Quora dataset contains 400 , 000 question pairs .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'The hyper - parameters of aESIM model are listed as follows .',\n",
       "  'pre_sentence': 'We use the validation set to select models for testing .',\n",
       "  'post_sentence': 'We use the Adam method for optimization .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Adam method',\n",
       "  'sentence': 'We use the Adam method for optimization .',\n",
       "  'pre_sentence': 'The hyper - parameters of aESIM model are listed as follows .',\n",
       "  'post_sentence': 'The first momentum is set to be 0.9 and the second 0.999 .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'optimization',\n",
       "  'sentence': 'We use the Adam method for optimization .',\n",
       "  'pre_sentence': 'The hyper - parameters of aESIM model are listed as follows .',\n",
       "  'post_sentence': 'The first momentum is set to be 0.9 and the second 0.999 .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'learning rate',\n",
       "  'sentence': 'The initial learning rate is set to 0.0005 , and the batch size is 128 .',\n",
       "  'pre_sentence': 'The first momentum is set to be 0.9 and the second 0.999 .',\n",
       "  'post_sentence': 'The dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM',\n",
       "  'sentence': 'The dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .',\n",
       "  'pre_sentence': 'The initial learning rate is set to 0.0005 , and the batch size is 128 .',\n",
       "  'post_sentence': 'We employ non - linearity function replacing rectified linear unit on account of its faster convergence rate .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'word embedding',\n",
       "  'sentence': 'The dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .',\n",
       "  'pre_sentence': 'The initial learning rate is set to 0.0005 , and the batch size is 128 .',\n",
       "  'post_sentence': 'We employ non - linearity function replacing rectified linear unit on account of its faster convergence rate .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'rectified linear unit',\n",
       "  'sentence': 'We employ non - linearity function replacing rectified linear unit on account of its faster convergence rate .',\n",
       "  'pre_sentence': 'The dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .',\n",
       "  'post_sentence': 'Dropout rate is set to 0.2 during training .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'convergence rate',\n",
       "  'sentence': 'We employ non - linearity function replacing rectified linear unit on account of its faster convergence rate .',\n",
       "  'pre_sentence': 'The dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .',\n",
       "  'post_sentence': 'Dropout rate is set to 0.2 during training .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'Dropout rate',\n",
       "  'sentence': 'Dropout rate is set to 0.2 during training .',\n",
       "  'pre_sentence': 'We employ non - linearity function replacing rectified linear unit on account of its faster convergence rate .',\n",
       "  'post_sentence': 'We use pre - trained 300 - D Glove 840B vectors to initialize word embeddings .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'Out - of - vocabulary ( OOV ) words',\n",
       "  'sentence': 'Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .',\n",
       "  'pre_sentence': 'We use pre - trained 300 - D Glove 840B vectors to initialize word embeddings .',\n",
       "  'post_sentence': 'All vectors are updated during training .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Gaussian samples',\n",
       "  'sentence': 'Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .',\n",
       "  'pre_sentence': 'We use pre - trained 300 - D Glove 840B vectors to initialize word embeddings .',\n",
       "  'post_sentence': 'All vectors are updated during training .',\n",
       "  'section_name': 'Setting',\n",
       "  'section_index': 7},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNIL',\n",
       "  'sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'feature engineering method',\n",
       "  'sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'pre_sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'post_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'sentence vector - based models',\n",
       "  'sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'pre_sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'post_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention - based models',\n",
       "  'sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'pre_sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'post_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'pre_sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'post_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'pre_sentence': 'Except for comparing our method aESIM with ESIM , we listed the experimental results of methods with their references in Table [ reference ] on SNIL .',\n",
       "  'post_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'pre_sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'post_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'pre_sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'post_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Keras',\n",
       "  'sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'pre_sentence': 'In Table [ reference ] , the method in the first block is a traditional feature engineering method , those in the second are the sentence vector - based models , those in the third are attention - based models , and ESIM and our aESIM are shown in the fourth block .',\n",
       "  'post_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'CBOW',\n",
       "  'sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'pre_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'post_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM',\n",
       "  'sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'pre_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'post_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'pre_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'post_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'pre_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'post_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quroa corpus',\n",
       "  'sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'pre_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'post_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'CNN',\n",
       "  'sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'pre_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'post_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'RNN models',\n",
       "  'sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'pre_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'post_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'accuracy',\n",
       "  'sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'pre_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'post_sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'ACC',\n",
       "  'sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'pre_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'post_sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'precision score',\n",
       "  'sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'pre_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'post_sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'accuracy',\n",
       "  'sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'pre_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'post_sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'pre_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'post_sentence': 'It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'pre_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'post_sentence': 'It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'accuracy',\n",
       "  'sentence': 'It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .',\n",
       "  'pre_sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'post_sentence': 'It also achieved 88.01 % on Quora .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'MultiNLI',\n",
       "  'sentence': 'It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .',\n",
       "  'pre_sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'post_sentence': 'It also achieved 88.01 % on Quora .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quora',\n",
       "  'sentence': 'It also achieved 88.01 % on Quora .',\n",
       "  'pre_sentence': 'It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .',\n",
       "  'post_sentence': 'Therefore , we concluded that aESIM with further word attention and word orientation operation was superior to ESIM model .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'Therefore , we concluded that aESIM with further word attention and word orientation operation was superior to ESIM model .',\n",
       "  'pre_sentence': 'It also achieved 88.01 % on Quora .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'Therefore , we concluded that aESIM with further word attention and word orientation operation was superior to ESIM model .',\n",
       "  'pre_sentence': 'It also achieved 88.01 % on Quora .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'Attention visualization',\n",
       "  'sentence': 'subsection : Attention visualization',\n",
       "  'pre_sentence': 'Therefore , we concluded that aESIM with further word attention and word orientation operation was superior to ESIM model .',\n",
       "  'post_sentence': 'We selected three types of sentence pairs from a premise and its three hypothesis sentences in the test set of SNLI corpus as shown in Figure [ reference ] , where the premise sentence is ‘ A woman with a green headscarf , blue shirt and a very big grin ’ , and three hypothesis sentences are ‘ the woman has been shot ’ ,',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'We selected three types of sentence pairs from a premise and its three hypothesis sentences in the test set of SNLI corpus as shown in Figure [ reference ] , where the premise sentence is ‘ A woman with a green headscarf , blue shirt and a very big grin ’ , and three hypothesis sentences are ‘ the woman has been shot ’ ,',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': '‘ the woman is very happy ’ and ‘ the woman is young ’ with relation labels',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'Figures 4.a - 4.c are the visualization of the attention layer between sentence pairs after the Bi - LSTM layer in ESIM model and that after Bi - aLSTM layer in aESIM model for contrasting ESIM and aESIM .',\n",
       "  'pre_sentence': 'Each pair of sentences has their key word pairs : grin - shot , grin - happy and grin - young , which determines whether the premise can entail the hypothesis .',\n",
       "  'post_sentence': 'By doing so , we could understand how the models judge the relation between two sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'Figures 4.a - 4.c are the visualization of the attention layer between sentence pairs after the Bi - LSTM layer in ESIM model and that after Bi - aLSTM layer in aESIM model for contrasting ESIM and aESIM .',\n",
       "  'pre_sentence': 'Each pair of sentences has their key word pairs : grin - shot , grin - happy and grin - young , which determines whether the premise can entail the hypothesis .',\n",
       "  'post_sentence': 'By doing so , we could understand how the models judge the relation between two sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - aLSTM layer',\n",
       "  'sentence': 'Figures 4.a - 4.c are the visualization of the attention layer between sentence pairs after the Bi - LSTM layer in ESIM model and that after Bi - aLSTM layer in aESIM model for contrasting ESIM and aESIM .',\n",
       "  'pre_sentence': 'Each pair of sentences has their key word pairs : grin - shot , grin - happy and grin - young , which determines whether the premise can entail the hypothesis .',\n",
       "  'post_sentence': 'By doing so , we could understand how the models judge the relation between two sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'Figures 4.a - 4.c are the visualization of the attention layer between sentence pairs after the Bi - LSTM layer in ESIM model and that after Bi - aLSTM layer in aESIM model for contrasting ESIM and aESIM .',\n",
       "  'pre_sentence': 'Each pair of sentences has their key word pairs : grin - shot , grin - happy and grin - young , which determines whether the premise can entail the hypothesis .',\n",
       "  'post_sentence': 'By doing so , we could understand how the models judge the relation between two sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Figures 4.a - 4.c are the visualization of the attention layer between sentence pairs after the Bi - LSTM layer in ESIM model and that after Bi - aLSTM layer in aESIM model for contrasting ESIM and aESIM .',\n",
       "  'pre_sentence': 'Each pair of sentences has their key word pairs : grin - shot , grin - happy and grin - young , which determines whether the premise can entail the hypothesis .',\n",
       "  'post_sentence': 'By doing so , we could understand how the models judge the relation between two sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'Figures 4.a - 4.c are the visualization of the attention layer between sentence pairs after the Bi - LSTM layer in ESIM model and that after Bi - aLSTM layer in aESIM model for contrasting ESIM and aESIM .',\n",
       "  'pre_sentence': 'Each pair of sentences has their key word pairs : grin - shot , grin - happy and grin - young , which determines whether the premise can entail the hypothesis .',\n",
       "  'post_sentence': 'By doing so , we could understand how the models judge the relation between two sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'We could conclude that our aESIM model had the higher weight than ESIM model on each key word pair , especially in Figure [ reference ]',\n",
       "  'pre_sentence': 'In each Figure , the brighter the color , the higher the weight is .',\n",
       "  'post_sentence': '.b , where the similarity of ‘ happy ’ and ‘ grin ’ in aESIM model is much higher than that in ESIM model .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'We could conclude that our aESIM model had the higher weight than ESIM model on each key word pair , especially in Figure [ reference ]',\n",
       "  'pre_sentence': 'In each Figure , the brighter the color , the higher the weight is .',\n",
       "  'post_sentence': '.b , where the similarity of ‘ happy ’ and ‘ grin ’ in aESIM model is much higher than that in ESIM model .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': '.b , where the similarity of ‘ happy ’ and ‘ grin ’ in aESIM model is much higher than that in ESIM model .',\n",
       "  'pre_sentence': 'We could conclude that our aESIM model had the higher weight than ESIM model on each key word pair , especially in Figure [ reference ]',\n",
       "  'post_sentence': 'Therefore , our aESIM model was able to capture the most important word pair in each pair of sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': '.b , where the similarity of ‘ happy ’ and ‘ grin ’ in aESIM model is much higher than that in ESIM model .',\n",
       "  'pre_sentence': 'We could conclude that our aESIM model had the higher weight than ESIM model on each key word pair , especially in Figure [ reference ]',\n",
       "  'post_sentence': 'Therefore , our aESIM model was able to capture the most important word pair in each pair of sentences .',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'Therefore , our aESIM model was able to capture the most important word pair in each pair of sentences .',\n",
       "  'pre_sentence': '.b , where the similarity of ‘ happy ’ and ‘ grin ’ in aESIM model is much higher than that in ESIM model .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'Attention visualization',\n",
       "  'section_index': 9},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'In this study , we propose an improved version of ESIM named aESIM for NLI .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'It modifies the Bi - LSTM layer to collect more information .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM',\n",
       "  'sentence': 'In this study , we propose an improved version of ESIM named aESIM for NLI .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'It modifies the Bi - LSTM layer to collect more information .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'NLI',\n",
       "  'sentence': 'In this study , we propose an improved version of ESIM named aESIM for NLI .',\n",
       "  'pre_sentence': '',\n",
       "  'post_sentence': 'It modifies the Bi - LSTM layer to collect more information .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'Bi - LSTM layer',\n",
       "  'sentence': 'It modifies the Bi - LSTM layer to collect more information .',\n",
       "  'pre_sentence': 'In this study , we propose an improved version of ESIM named aESIM for NLI .',\n",
       "  'post_sentence': 'We evaluate our aESIM model on three NLI corpora .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'We evaluate our aESIM model on three NLI corpora .',\n",
       "  'pre_sentence': 'It modifies the Bi - LSTM layer to collect more information .',\n",
       "  'post_sentence': 'Experimental results show that aESIM model achieves better performance than ESIM model .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'aESIM model',\n",
       "  'sentence': 'Experimental results show that aESIM model achieves better performance than ESIM model .',\n",
       "  'pre_sentence': 'We evaluate our aESIM model on three NLI corpora .',\n",
       "  'post_sentence': 'In the future , we will evaluate how attention mechanisms can be applied on other tasks and explore a way to use less time and space with guaranteed accuracy .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM model',\n",
       "  'sentence': 'Experimental results show that aESIM model achieves better performance than ESIM model .',\n",
       "  'pre_sentence': 'We evaluate our aESIM model on three NLI corpora .',\n",
       "  'post_sentence': 'In the future , we will evaluate how attention mechanisms can be applied on other tasks and explore a way to use less time and space with guaranteed accuracy .',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'attention mechanisms',\n",
       "  'sentence': 'In the future , we will evaluate how attention mechanisms can be applied on other tasks and explore a way to use less time and space with guaranteed accuracy .',\n",
       "  'pre_sentence': 'Experimental results show that aESIM model achieves better performance than ESIM model .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'Table [ reference ] shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .',\n",
       "  'pre_sentence': 'It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .',\n",
       "  'post_sentence': 'In the literature , the task of NLI is usually viewed as a relation classification .',\n",
       "  'section_name': 'Introduction',\n",
       "  'section_index': 1},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'ESIM',\n",
       "  'sentence': 'Figure [ reference ] shows a high - level view of the ESIM architecture , where the bottom LSTM1 layer of Figure [ reference ] is the input encoding layer ,',\n",
       "  'pre_sentence': 'Instead of summation adopted by Parikh et al . , ESIM proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier : a fully connected multi - layer perceptron .',\n",
       "  'post_sentence': 'the middle part with LSTM2 layer is the local inference layer , the upper part is the inference composition layer .',\n",
       "  'section_name': 'ESIM model',\n",
       "  'section_index': 3},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Task',\n",
       "  'ner': 'natural language processing',\n",
       "  'sentence': 'Besides , this Bi - aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi - aLSTM is capable of improving the performance of Bi - LSTM models on sentimental classification task ( for space limitation , this results will not be shown in the paper ) .',\n",
       "  'pre_sentence': 'Its whole architecture is shown in the Figure [ reference ] , is applied in ESIM model to replace the two Bi - LSTM layers for the task of natural language inference .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'aESIM model',\n",
       "  'section_index': 4},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'the Stanford Natural Language Inference ( SNLI ) corpus , the Multi - Genre Natural Language Inference ( MultiNLI ) corpus , and Quora duplicate question dataset .',\n",
       "  'pre_sentence': 'We evaluated our model on three datasets :',\n",
       "  'post_sentence': 'We selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'MultiNLI',\n",
       "  'sentence': 'the Stanford Natural Language Inference ( SNLI ) corpus , the Multi - Genre Natural Language Inference ( MultiNLI ) corpus , and Quora duplicate question dataset .',\n",
       "  'pre_sentence': 'We evaluated our model on three datasets :',\n",
       "  'post_sentence': 'We selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quora',\n",
       "  'sentence': 'the Stanford Natural Language Inference ( SNLI ) corpus , the Multi - Genre Natural Language Inference ( MultiNLI ) corpus , and Quora duplicate question dataset .',\n",
       "  'pre_sentence': 'We evaluated our model on three datasets :',\n",
       "  'post_sentence': 'We selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'The Stanford Natural Language Inference ( SNLI',\n",
       "  'pre_sentence': 'SNLI',\n",
       "  'post_sentence': ') corpus contains 570 , 152 sentence pairs , including 549 K training pairs , 10 K validation pairs and 10 K testing pairs .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'The corpus is modeled on the SNLI corpus , but differs in that covers a range of genres of spoken and written text , and supports a distinctive cross - genre generation evaluation .',\n",
       "  'pre_sentence': 'This corpus is a crowd - sourced collection of 433 K sentence pairs annotated with textual entailment information .',\n",
       "  'post_sentence': 'The Quora dataset contains 400 , 000 question pairs .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'Quora',\n",
       "  'sentence': 'The Quora dataset contains 400 , 000 question pairs .',\n",
       "  'pre_sentence': 'Quora',\n",
       "  'post_sentence': 'The task of this corpus is to judge whether the two sentences means the same affair .',\n",
       "  'section_name': 'Datasets',\n",
       "  'section_index': 6},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'MultiNLI',\n",
       "  'sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'pre_sentence': 'Where the results of ESIM and aESIM are implemented by ourselves on Keras , the results of the others are taken from their original publications .',\n",
       "  'post_sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'CNN',\n",
       "  'sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'pre_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'post_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Method',\n",
       "  'ner': 'RNN models',\n",
       "  'sentence': 'Finally , we compare several types of CNN and RNN models on Quroa corpus shown in Table [ reference ] , the results of theses CNN and RNN models are taken from .',\n",
       "  'pre_sentence': 'We then compare the baseline models , CBOW , Bi - LSTM with ESIM and our aESIM on MultiNLI corpus shown In Table [ reference ] , where the results of the baselines are taken from .',\n",
       "  'post_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Material',\n",
       "  'ner': 'SNLI',\n",
       "  'sentence': 'According to the results in Tables 2 - 4 , aESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .',\n",
       "  'pre_sentence': 'The accuracy ( ACC ) of each method is measured by the commonly used precision score , and the methods with the best accuracy are marked in bold .',\n",
       "  'post_sentence': 'It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .',\n",
       "  'section_name': 'Experiment results',\n",
       "  'section_index': 8},\n",
       " {'doc_id': '007ff2ca5f297b04636699ce4d01ca6d6f21dc77',\n",
       "  'relation': 'Metric',\n",
       "  'ner': 'accuracy',\n",
       "  'sentence': 'In the future , we will evaluate how attention mechanisms can be applied on other tasks and explore a way to use less time and space with guaranteed accuracy .',\n",
       "  'pre_sentence': 'Experimental results show that aESIM model achieves better performance than ESIM model .',\n",
       "  'post_sentence': '',\n",
       "  'section_name': 'Conclusion',\n",
       "  'section_index': 10}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['007ff2ca5f297b04636699ce4d01ca6d6f21dc77']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. filter for method\n",
    "is_method = df['relation'] == 'Method'\n",
    "df_method = df[is_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_method['ner_list'] = [','.join(map(str, l)) for l in df_method['ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. shuffle\n",
    "df_method = df_method.sample(frac=1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method_unique = df_method.drop_duplicates(subset='ner', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method_sample = df_method_unique.sample(n = 1000, random_state = 101)\n",
    "df_method_sample.to_csv('annotation_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relation</th>\n",
       "      <th>ner</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pre_sentence</th>\n",
       "      <th>post_sentence</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30875</th>\n",
       "      <td>10203151008a20b32ce089f7f9d580005c2426cf</td>\n",
       "      <td>Method</td>\n",
       "      <td>convolutional layer activations</td>\n",
       "      <td>In particular for image retrieval , Babenko et...</td>\n",
       "      <td>Using CNN layer activations as off - the - she...</td>\n",
       "      <td>Generalization to other tasks is attained by C...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97801</th>\n",
       "      <td>4087ebc37a1650dbb5d8205af0850bee74f3784b</td>\n",
       "      <td>Method</td>\n",
       "      <td>weight initialization</td>\n",
       "      <td>A poor weight initialization may take longer t...</td>\n",
       "      <td>Optimal parameter initialization remains a cru...</td>\n",
       "      <td>Here , we propose a method of weight re - init...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54406</th>\n",
       "      <td>220a0b46840a2a1421c62d3d343397ab087a3f17</td>\n",
       "      <td>Method</td>\n",
       "      <td>Spatio - temporal filters</td>\n",
       "      <td>Spatio - temporal filters .</td>\n",
       "      <td>Of course spatial pyramids are widely used in ...</td>\n",
       "      <td>Burt and Adelson lay out the theory of spatio ...</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103733</th>\n",
       "      <td>435259c5f3cffd75ef837a8e638cc8f6244e25c4</td>\n",
       "      <td>Method</td>\n",
       "      <td>sliding - window strategy</td>\n",
       "      <td>A naive approach follows a sliding - window st...</td>\n",
       "      <td>Originally designed for image recognition and ...</td>\n",
       "      <td>As explained before , this technique presents ...</td>\n",
       "      <td>Methods</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17186</th>\n",
       "      <td>0a053f55804eee01f3c8b4138a1d3364d5bc45ac</td>\n",
       "      <td>Method</td>\n",
       "      <td>Neural LP</td>\n",
       "      <td>IRN and Neural LP explore multi - step relatio...</td>\n",
       "      <td>Hence , recent works have proposed approaches ...</td>\n",
       "      <td>Compared to RL - based approaches , it is hard...</td>\n",
       "      <td>Knowledge Base Completion</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43474</th>\n",
       "      <td>19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7</td>\n",
       "      <td>Method</td>\n",
       "      <td>off - policy RL algorithm</td>\n",
       "      <td>The learner applies an off - policy RL algorit...</td>\n",
       "      <td>For each learner update , a minibatch of exper...</td>\n",
       "      <td>The gradients are communicated to the paramete...</td>\n",
       "      <td>Distributed Architecture</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86913</th>\n",
       "      <td>3729a9a140aa13b3b26210d333fd19659fc21471</td>\n",
       "      <td>Method</td>\n",
       "      <td>random strategy</td>\n",
       "      <td>We see that the scores of the semantic tasks d...</td>\n",
       "      <td>Table [ reference ] shows the results of train...</td>\n",
       "      <td>In our preliminary experiments , we have found...</td>\n",
       "      <td>Order of training</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64621</th>\n",
       "      <td>289e91654f6da968d625481ef21f52892052d4fc</td>\n",
       "      <td>Method</td>\n",
       "      <td>char - based models</td>\n",
       "      <td>We observed the following from the Table [ ref...</td>\n",
       "      <td>Table [ reference ] gives the performance of o...</td>\n",
       "      <td>That may be because in Chinese the words can o...</td>\n",
       "      <td>Performance Comparison</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23173</th>\n",
       "      <td>0ca2bd0e40a8f0a57665535ae1c31561370ad183</td>\n",
       "      <td>Method</td>\n",
       "      <td>recurrent generalization of stochastic depth</td>\n",
       "      <td>The COPY operation used in our model can be re...</td>\n",
       "      <td>It is however different to our model in the se...</td>\n",
       "      <td>This results in occasional copy operations of ...</td>\n",
       "      <td>RELATED WORK</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22776</th>\n",
       "      <td>0c9ae806059196007938f24d0327a4237ed6adf5</td>\n",
       "      <td>Method</td>\n",
       "      <td>classification function</td>\n",
       "      <td>IIC is a generic clustering algorithm that dir...</td>\n",
       "      <td>In this paper , we introduce Invariant Informa...</td>\n",
       "      <td>It involves a simple objective function , whic...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          doc_id relation  \\\n",
       "30875   10203151008a20b32ce089f7f9d580005c2426cf   Method   \n",
       "97801   4087ebc37a1650dbb5d8205af0850bee74f3784b   Method   \n",
       "54406   220a0b46840a2a1421c62d3d343397ab087a3f17   Method   \n",
       "103733  435259c5f3cffd75ef837a8e638cc8f6244e25c4   Method   \n",
       "17186   0a053f55804eee01f3c8b4138a1d3364d5bc45ac   Method   \n",
       "...                                          ...      ...   \n",
       "43474   19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7   Method   \n",
       "86913   3729a9a140aa13b3b26210d333fd19659fc21471   Method   \n",
       "64621   289e91654f6da968d625481ef21f52892052d4fc   Method   \n",
       "23173   0ca2bd0e40a8f0a57665535ae1c31561370ad183   Method   \n",
       "22776   0c9ae806059196007938f24d0327a4237ed6adf5   Method   \n",
       "\n",
       "                                                 ner  \\\n",
       "30875                convolutional layer activations   \n",
       "97801                          weight initialization   \n",
       "54406                      Spatio - temporal filters   \n",
       "103733                     sliding - window strategy   \n",
       "17186                                      Neural LP   \n",
       "...                                              ...   \n",
       "43474                      off - policy RL algorithm   \n",
       "86913                                random strategy   \n",
       "64621                            char - based models   \n",
       "23173   recurrent generalization of stochastic depth   \n",
       "22776                        classification function   \n",
       "\n",
       "                                                 sentence  \\\n",
       "30875   In particular for image retrieval , Babenko et...   \n",
       "97801   A poor weight initialization may take longer t...   \n",
       "54406                         Spatio - temporal filters .   \n",
       "103733  A naive approach follows a sliding - window st...   \n",
       "17186   IRN and Neural LP explore multi - step relatio...   \n",
       "...                                                   ...   \n",
       "43474   The learner applies an off - policy RL algorit...   \n",
       "86913   We see that the scores of the semantic tasks d...   \n",
       "64621   We observed the following from the Table [ ref...   \n",
       "23173   The COPY operation used in our model can be re...   \n",
       "22776   IIC is a generic clustering algorithm that dir...   \n",
       "\n",
       "                                             pre_sentence  \\\n",
       "30875   Using CNN layer activations as off - the - she...   \n",
       "97801   Optimal parameter initialization remains a cru...   \n",
       "54406   Of course spatial pyramids are widely used in ...   \n",
       "103733  Originally designed for image recognition and ...   \n",
       "17186   Hence , recent works have proposed approaches ...   \n",
       "...                                                   ...   \n",
       "43474   For each learner update , a minibatch of exper...   \n",
       "86913   Table [ reference ] shows the results of train...   \n",
       "64621   Table [ reference ] gives the performance of o...   \n",
       "23173   It is however different to our model in the se...   \n",
       "22776   In this paper , we introduce Invariant Informa...   \n",
       "\n",
       "                                            post_sentence  \\\n",
       "30875   Generalization to other tasks is attained by C...   \n",
       "97801   Here , we propose a method of weight re - init...   \n",
       "54406   Burt and Adelson lay out the theory of spatio ...   \n",
       "103733  As explained before , this technique presents ...   \n",
       "17186   Compared to RL - based approaches , it is hard...   \n",
       "...                                                   ...   \n",
       "43474   The gradients are communicated to the paramete...   \n",
       "86913   In our preliminary experiments , we have found...   \n",
       "64621   That may be because in Chinese the words can o...   \n",
       "23173   This results in occasional copy operations of ...   \n",
       "22776   It involves a simple objective function , whic...   \n",
       "\n",
       "                     section_name  section_index  \n",
       "30875                Introduction              1  \n",
       "97801                    Abstract              1  \n",
       "54406                Related Work              2  \n",
       "103733                    Methods              4  \n",
       "17186   Knowledge Base Completion             15  \n",
       "...                           ...            ...  \n",
       "43474    Distributed Architecture              7  \n",
       "86913           Order of training             33  \n",
       "64621      Performance Comparison             24  \n",
       "23173                RELATED WORK              3  \n",
       "22776                Introduction              1  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_method_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_method_unique)\n",
    "len(df_method_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

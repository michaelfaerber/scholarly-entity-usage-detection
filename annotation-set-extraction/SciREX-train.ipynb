{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('release_data/train.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = []\n",
    "for json_str in json_list:\n",
    "    result.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Convolutional Neural Network Architectures', 'sentence': 'document : Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'pre_sentence': 'bibliography : References', 'post_sentence': 'Semantic matching is of central importance to many natural language tasks .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Matching Natural Language Sentences', 'sentence': 'document : Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'pre_sentence': 'bibliography : References', 'post_sentence': 'Semantic matching is of central importance to many natural language tasks .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Semantic matching', 'sentence': 'Semantic matching is of central importance to many natural language tasks .', 'pre_sentence': '', 'post_sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'natural language tasks', 'sentence': 'Semantic matching is of central importance to many natural language tasks .', 'pre_sentence': '', 'post_sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'matching algorithm', 'sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'pre_sentence': 'Semantic matching is of central importance to many natural language tasks .', 'post_sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional neural network models', 'sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'pre_sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'post_sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional strategy', 'sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'pre_sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'post_sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'vision', 'sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'pre_sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'post_sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'speech', 'sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'pre_sentence': 'A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .', 'post_sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'layer - by - layer composition', 'sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'pre_sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'post_sentence': 'Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling', 'sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'pre_sentence': 'As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .', 'post_sentence': 'Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .', 'pre_sentence': 'The proposed models not only nicely represent the hierarchical structures of sentences with their layer - by - layer composition and pooling , but also capture the rich matching patterns at different levels .', 'post_sentence': 'The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .', 'pre_sentence': 'Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .', 'post_sentence': '', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .', 'pre_sentence': 'Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .', 'post_sentence': '', 'section_name': 'Convolutional Neural Network Architectures for Matching Natural Language Sentences', 'section_index': 0, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Matching', 'sentence': 'Matching two potentially heterogenous language objects is central to many natural language applications .', 'pre_sentence': '', 'post_sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'natural language applications', 'sentence': 'Matching two potentially heterogenous language objects is central to many natural language applications .', 'pre_sentence': '', 'post_sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'paraphrase identification', 'sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'pre_sentence': 'Matching two potentially heterogenous language objects is central to many natural language applications .', 'post_sentence': 'Examples include top - re - ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'relevance', 'sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'pre_sentence': 'Matching two potentially heterogenous language objects is central to many natural language applications .', 'post_sentence': 'Examples include top - re - ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'information retrieval', 'sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'pre_sentence': 'Matching two potentially heterogenous language objects is central to many natural language applications .', 'post_sentence': 'Examples include top - re - ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'top - re - ranking', 'sentence': 'Examples include top - re - ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .', 'pre_sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'post_sentence': 'Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'machine translation', 'sentence': 'Examples include top - re - ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .', 'pre_sentence': 'It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between “ linguistic objects ” of different nature at different levels of abstractions .', 'post_sentence': 'Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sentence - matching algorithm', 'sentence': 'A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .', 'pre_sentence': 'Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .', 'post_sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'deep neural network models', 'sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'pre_sentence': 'A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .', 'post_sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional strategy', 'sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'pre_sentence': 'A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .', 'post_sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'image', 'sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'pre_sentence': 'A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .', 'post_sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'speech', 'sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'pre_sentence': 'A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .', 'post_sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'natural language', 'sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'pre_sentence': 'A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .', 'post_sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'hierarchical composition', 'sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'pre_sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'post_sentence': 'Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'comprehensive fusion of matching patterns', 'sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'pre_sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'post_sentence': 'Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architecture', 'sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'pre_sentence': 'Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .', 'post_sentence': 'Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .', 'pre_sentence': 'To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .', 'post_sentence': 'This is part of our continuing effort in understanding natural language objects and the matching between them .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'natural language objects', 'sentence': 'This is part of our continuing effort in understanding natural language objects and the matching between them .', 'pre_sentence': 'Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .', 'post_sentence': 'Our main contributions can be summarized as follows .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching', 'sentence': 'This is part of our continuing effort in understanding natural language objects and the matching between them .', 'pre_sentence': 'Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .', 'post_sentence': 'Our main contributions can be summarized as follows .', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'deep convolutional network architectures', 'sentence': 'First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .', 'pre_sentence': 'Our main contributions can be summarized as follows .', 'post_sentence': '', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'hierarchical sentence modeling', 'sentence': 'First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .', 'pre_sentence': 'Our main contributions can be summarized as follows .', 'post_sentence': '', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'layer - by - layer composition', 'sentence': 'First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .', 'pre_sentence': 'Our main contributions can be summarized as follows .', 'post_sentence': '', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling', 'sentence': 'First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .', 'pre_sentence': 'Our main contributions can be summarized as follows .', 'post_sentence': '', 'section_name': 'Introduction', 'section_index': 1, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution network', 'sentence': 'We start by introducing a convolution network in Section [ reference ] as the basic architecture for sentence modeling , and how it is related to existing sentence models .', 'pre_sentence': '', 'post_sentence': 'Based on that , in Section [ reference ] , we propose two architectures for sentence matching , with a detailed discussion of their relation .', 'section_name': 'Roadmap', 'section_index': 2, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence modeling', 'sentence': 'We start by introducing a convolution network in Section [ reference ] as the basic architecture for sentence modeling , and how it is related to existing sentence models .', 'pre_sentence': '', 'post_sentence': 'Based on that , in Section [ reference ] , we propose two architectures for sentence matching , with a detailed discussion of their relation .', 'section_name': 'Roadmap', 'section_index': 2, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sentence models', 'sentence': 'We start by introducing a convolution network in Section [ reference ] as the basic architecture for sentence modeling , and how it is related to existing sentence models .', 'pre_sentence': '', 'post_sentence': 'Based on that , in Section [ reference ] , we propose two architectures for sentence matching , with a detailed discussion of their relation .', 'section_name': 'Roadmap', 'section_index': 2, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence matching', 'sentence': 'Based on that , in Section [ reference ] , we propose two architectures for sentence matching , with a detailed discussion of their relation .', 'pre_sentence': 'We start by introducing a convolution network in Section [ reference ] as the basic architecture for sentence modeling , and how it is related to existing sentence models .', 'post_sentence': 'In Section [ reference ] , we briefly discuss the learning of the proposed architectures .', 'section_name': 'Roadmap', 'section_index': 2, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Convolutional Sentence Model', 'sentence': 'section : Convolutional Sentence Model', 'pre_sentence': 'Then in Section [ reference ] , we report our empirical study , followed by a brief discussion of related work in Section [ reference ] .', 'post_sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architecture', 'sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'pre_sentence': '', 'post_sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'modeling sentences', 'sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'pre_sentence': '', 'post_sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'unsupervised methods', 'sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'pre_sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'post_sentence': 'As in most convolutional models , we use convolution units with a local “ receptive field ” and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution', 'sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'pre_sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'post_sentence': 'As in most convolutional models , we use convolution units with a local “ receptive field ” and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling', 'sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'pre_sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'post_sentence': 'As in most convolutional models , we use convolution units with a local “ receptive field ” and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'fixed length vectorial representation', 'sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'pre_sentence': 'We start with proposing a new convolutional architecture for modeling sentences .', 'post_sentence': 'As in most convolutional models , we use convolution units with a local “ receptive field ” and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional models', 'sentence': 'As in most convolutional models , we use convolution units with a local “ receptive field ” and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .', 'pre_sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'post_sentence': '', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution units', 'sentence': 'As in most convolutional models , we use convolution units with a local “ receptive field ” and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .', 'pre_sentence': 'As illustrated in Figure [ reference ] , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .', 'post_sentence': '', 'section_name': 'Convolutional Sentence Model', 'section_index': 3, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution in Layer - 1', 'sentence': 'As shown in Figure [ reference ] , the convolution in Layer - 1 operates on sliding windows of words ( width ) , and the convolutions in deeper layers are defined in a similar way .', 'pre_sentence': '', 'post_sentence': 'Generally , with sentence input , the convolution unit for feature map of type -', 'section_name': 'Convolution', 'section_index': 4, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sentence input', 'sentence': 'Generally , with sentence input , the convolution unit for feature map of type -', 'pre_sentence': 'As shown in Figure [ reference ] , the convolution in Layer - 1 operates on sliding windows of words ( width ) , and the convolutions in deeper layers are defined in a similar way .', 'post_sentence': '( among of them ) on Layer - is and its matrix form is , where gives the output of feature map of type - for location in Layer - ; is the parameters for on Layer - , with matrix form ; is the activation function ( e.g. , Sigmoid or Relu ) denotes the segment of Layer - for the convolution at location , while concatenates the vectors for ( width of sliding window ) words from sentence input .', 'section_name': 'Convolution', 'section_index': 4, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution unit', 'sentence': 'Generally , with sentence input , the convolution unit for feature map of type -', 'pre_sentence': 'As shown in Figure [ reference ] , the convolution in Layer - 1 operates on sliding windows of words ( width ) , and the convolutions in deeper layers are defined in a similar way .', 'post_sentence': '( among of them ) on Layer - is and its matrix form is , where gives the output of feature map of type - for location in Layer - ; is the parameters for on Layer - , with matrix form ; is the activation function ( e.g. , Sigmoid or Relu ) denotes the segment of Layer - for the convolution at location , while concatenates the vectors for ( width of sliding window ) words from sentence input .', 'section_name': 'Convolution', 'section_index': 4, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'feature map', 'sentence': 'Generally , with sentence input , the convolution unit for feature map of type -', 'pre_sentence': 'As shown in Figure [ reference ] , the convolution in Layer - 1 operates on sliding windows of words ( width ) , and the convolutions in deeper layers are defined in a similar way .', 'post_sentence': '( among of them ) on Layer - is and its matrix form is , where gives the output of feature map of type - for location in Layer - ; is the parameters for on Layer - , with matrix form ; is the activation function ( e.g. , Sigmoid or Relu ) denotes the segment of Layer - for the convolution at location , while concatenates the vectors for ( width of sliding window ) words from sentence input .', 'section_name': 'Convolution', 'section_index': 4, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Sigmoid', 'sentence': '( among of them ) on Layer - is and its matrix form is , where gives the output of feature map of type - for location in Layer - ; is the parameters for on Layer - , with matrix form ; is the activation function ( e.g. , Sigmoid or Relu ) denotes the segment of Layer - for the convolution at location , while concatenates the vectors for ( width of sliding window ) words from sentence input .', 'pre_sentence': 'Generally , with sentence input , the convolution unit for feature map of type -', 'post_sentence': '', 'section_name': 'Convolution', 'section_index': 4, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Relu', 'sentence': '( among of them ) on Layer - is and its matrix form is , where gives the output of feature map of type - for location in Layer - ; is the parameters for on Layer - , with matrix form ; is the activation function ( e.g. , Sigmoid or Relu ) denotes the segment of Layer - for the convolution at location , while concatenates the vectors for ( width of sliding window ) words from sentence input .', 'pre_sentence': 'Generally , with sentence input , the convolution unit for feature map of type -', 'post_sentence': '', 'section_name': 'Convolution', 'section_index': 4, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Max - Pooling', 'sentence': 'paragraph : Max - Pooling', 'pre_sentence': '( among of them ) on Layer - is and its matrix form is , where gives the output of feature map of type - for location in Layer - ; is the parameters for on Layer - , with matrix form ; is the activation function ( e.g. , Sigmoid or Relu ) denotes the segment of Layer - for the convolution at location , while concatenates the vectors for ( width of sliding window ) words from sentence input .', 'post_sentence': 'We take a max - pooling in every two - unit window for every , after each convolution The effects of pooling are two - fold : 1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section [ reference ] for some analysis ) .', 'section_name': 'Max - Pooling', 'section_index': 5, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'max - pooling', 'sentence': 'We take a max - pooling in every two - unit window for every , after each convolution The effects of pooling are two - fold : 1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section [ reference ] for some analysis ) .', 'pre_sentence': '', 'post_sentence': '', 'section_name': 'Max - Pooling', 'section_index': 5, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling', 'sentence': 'We take a max - pooling in every two - unit window for every , after each convolution The effects of pooling are two - fold : 1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section [ reference ] for some analysis ) .', 'pre_sentence': '', 'post_sentence': '', 'section_name': 'Max - Pooling', 'section_index': 5, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence representation', 'sentence': 'We take a max - pooling in every two - unit window for every , after each convolution The effects of pooling are two - fold : 1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section [ reference ] for some analysis ) .', 'pre_sentence': '', 'post_sentence': '', 'section_name': 'Max - Pooling', 'section_index': 5, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution and pooling strategy', 'sentence': 'The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .', 'pre_sentence': '', 'post_sentence': 'More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional unit', 'sentence': 'To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit a gate which sets the output vectors to all - zeros if the input is all zeros .', 'pre_sentence': 'More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .', 'post_sentence': 'For any given sentence input , the output of type - filter for location in the layer is given by where if all the elements in vector equals 0 , otherwise .', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'type - filter', 'sentence': 'For any given sentence input , the output of type - filter for location in the layer is given by where if all the elements in vector equals 0 , otherwise .', 'pre_sentence': 'To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit a gate which sets the output vectors to all - zeros if the input is all zeros .', 'post_sentence': 'This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'max - pooling', 'sentence': 'This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .', 'pre_sentence': 'For any given sentence input , the output of type - filter for location in the layer is given by where if all the elements in vector equals 0 , otherwise .', 'post_sentence': 'Actually it creates a natural hierarchy of all - zero padding ( as illustrated in Figure [ reference ] ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'neural net', 'sentence': 'Actually it creates a natural hierarchy of all - zero padding ( as illustrated in Figure [ reference ] ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .', 'pre_sentence': 'This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .', 'post_sentence': '', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'prediction', 'sentence': 'Actually it creates a natural hierarchy of all - zero padding ( as illustrated in Figure [ reference ] ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .', 'pre_sentence': 'This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .', 'post_sentence': '', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'backward propagation', 'sentence': 'Actually it creates a natural hierarchy of all - zero padding ( as illustrated in Figure [ reference ] ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .', 'pre_sentence': 'This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .', 'post_sentence': '', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'learning', 'sentence': 'Actually it creates a natural hierarchy of all - zero padding ( as illustrated in Figure [ reference ] ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .', 'pre_sentence': 'This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .', 'post_sentence': '', 'section_name': 'Length Variability', 'section_index': 6, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Convolutional Architecture', 'sentence': 'subsection : Some Analysis on the Convolutional Architecture', 'pre_sentence': 'Actually it creates a natural hierarchy of all - zero padding ( as illustrated in Figure [ reference ] ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .', 'post_sentence': 'The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional unit', 'sentence': 'The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .', 'pre_sentence': '', 'post_sentence': 'Figure [ reference ] gives an example on what could happen on the first two layers with input sentence “', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'max - pooling', 'sentence': 'The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .', 'pre_sentence': '', 'post_sentence': 'Figure [ reference ] gives an example on what could happen on the first two layers with input sentence “', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'local selection mechanism', 'sentence': 'The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .', 'pre_sentence': '', 'post_sentence': 'Figure [ reference ] gives an example on what could happen on the first two layers with input sentence “', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'recursive autoencoder', 'sentence': 'The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .', 'pre_sentence': '', 'post_sentence': 'Figure [ reference ] gives an example on what could happen on the first two layers with input sentence “', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution units', 'sentence': 'Just for illustration purpose , we present a dramatic choice of parameters ( by turning off some elements in ) to make the convolution units focus on different segments within a 3 - word window .', 'pre_sentence': 'The cat sat on the mat ” .', 'post_sentence': 'For example , some feature maps ( group 2 ) give compositions for “', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'feature maps', 'sentence': 'Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in Figure [ reference ] ) .', 'pre_sentence': 'the cat ” and “ cat sat ” , each being a vector .', 'post_sentence': 'The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between “ on the ” and “ the mat ” for feature maps group 2 from the rightmost two sliding windows .', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution layer', 'sentence': 'Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in Figure [ reference ] ) .', 'pre_sentence': 'the cat ” and “ cat sat ” , each being a vector .', 'post_sentence': 'The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between “ on the ” and “ the mat ” for feature maps group 2 from the rightmost two sliding windows .', 'section_name': 'Some Analysis on the Convolutional Architecture', 'section_index': 7, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Recursive Models', 'sentence': 'paragraph : Relation to Recursive Models', 'pre_sentence': 'The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between “ on the ” and “ the mat ” for feature maps group 2 from the rightmost two sliding windows .', 'post_sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional model', 'sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'pre_sentence': '', 'post_sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Recurrent Neural Network', 'sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'pre_sentence': '', 'post_sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'RNN', 'sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'pre_sentence': '', 'post_sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Recursive Auto - Encoder', 'sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'pre_sentence': '', 'post_sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'RAE', 'sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'pre_sentence': '', 'post_sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'RAE', 'sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'pre_sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'post_sentence': 'Instead , it takes multiple choices of composition via a large feature map ( encoded in for different ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'gating function', 'sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'pre_sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'post_sentence': 'Instead , it takes multiple choices of composition via a large feature map ( encoded in for different ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'external parser', 'sentence': 'First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .', 'pre_sentence': 'Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .', 'post_sentence': 'Instead , it takes multiple choices of composition via a large feature map ( encoded in for different ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'RAE', 'sentence': 'With any window width , the type of composition would be much richer than that of RAE .', 'pre_sentence': 'Instead , it takes multiple choices of composition via a large feature map ( encoded in for different ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .', 'post_sentence': 'Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional model', 'sentence': 'Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .', 'pre_sentence': 'With any window width , the type of composition would be much richer than that of RAE .', 'post_sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'supervised training', 'sentence': 'Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .', 'pre_sentence': 'With any window width , the type of composition would be much richer than that of RAE .', 'post_sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'supervised learning - to - match framework', 'sentence': 'Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .', 'pre_sentence': 'With any window width , the type of composition would be much richer than that of RAE .', 'post_sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'recursive models', 'sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'pre_sentence': 'Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .', 'post_sentence': 'For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a “ global ” synthesis on the learned sentence representation .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architecture', 'sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'pre_sentence': 'Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .', 'post_sentence': 'For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a “ global ” synthesis on the learned sentence representation .', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching', 'sentence': 'For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a “ global ” synthesis on the learned sentence representation .', 'pre_sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'post_sentence': '', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'network afterwards', 'sentence': 'For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a “ global ” synthesis on the learned sentence representation .', 'pre_sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'post_sentence': '', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sentence representation', 'sentence': 'For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a “ global ” synthesis on the learned sentence representation .', 'pre_sentence': 'However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .', 'post_sentence': '', 'section_name': 'Relation to Recursive Models', 'section_index': 8, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Shallow ” Convolutional Models', 'sentence': 'paragraph : Relation to “ Shallow ” Convolutional Models', 'pre_sentence': 'For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a “ global ” synthesis on the learned sentence representation .', 'post_sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional sentence model', 'sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'pre_sentence': '', 'post_sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architecture', 'sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'pre_sentence': '', 'post_sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SENNA', 'sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'pre_sentence': '', 'post_sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution layer', 'sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'pre_sentence': '', 'post_sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'max - pooling', 'sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'pre_sentence': '', 'post_sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'local convolutions', 'sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'pre_sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'post_sentence': 'Since the sentence - level sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'global pooling', 'sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'pre_sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'post_sentence': 'Since the sentence - level sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'soft ” local template matching', 'sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'pre_sentence': 'The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .', 'post_sentence': 'Since the sentence - level sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'global pooling', 'sentence': 'Since the sentence - level sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .', 'pre_sentence': 'This type of models , with local convolutions and a global pooling , essentially do a “ soft ” local template matching and is able to detect local features useful for a certain task .', 'post_sentence': 'It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional model', 'sentence': 'It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .', 'pre_sentence': 'Since the sentence - level sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .', 'post_sentence': '', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SENNA - type architecture', 'sentence': 'It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .', 'pre_sentence': 'Since the sentence - level sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .', 'post_sentence': '', 'section_name': 'Relation to “ Shallow ” Convolutional Models', 'section_index': 9, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Convolutional Matching Models', 'sentence': 'section : Convolutional Matching Models', 'pre_sentence': 'It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .', 'post_sentence': 'Based on the discussion in Section [ reference ] , we propose two related convolutional architectures , namely Arc - I and Arc - II ) , for matching two sentences .', 'section_name': 'Convolutional Matching Models', 'section_index': 10, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architectures', 'sentence': 'Based on the discussion in Section [ reference ] , we propose two related convolutional architectures , namely Arc - I and Arc - II ) , for matching two sentences .', 'pre_sentence': '', 'post_sentence': '', 'section_name': 'Convolutional Matching Models', 'section_index': 10, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Based on the discussion in Section [ reference ] , we propose two related convolutional architectures , namely Arc - I and Arc - II ) , for matching two sentences .', 'pre_sentence': '', 'post_sentence': '', 'section_name': 'Convolutional Matching Models', 'section_index': 10, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Based on the discussion in Section [ reference ] , we propose two related convolutional architectures , namely Arc - I and Arc - II ) , for matching two sentences .', 'pre_sentence': '', 'post_sentence': '', 'section_name': 'Convolutional Matching Models', 'section_index': 10, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Architecture - I', 'sentence': 'subsection : Architecture - I ( Arc - I )', 'pre_sentence': 'Based on the discussion in Section [ reference ] , we propose two related convolutional architectures , namely Arc - I and Arc - II ) , for matching two sentences .', 'post_sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'subsection : Architecture - I ( Arc - I )', 'pre_sentence': 'Based on the discussion in Section [ reference ] , we propose two related convolutional architectures , namely Arc - I and Arc - II ) , for matching two sentences .', 'post_sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Architecture - I', 'sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'pre_sentence': '', 'post_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'pre_sentence': '', 'post_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'multi - layer perceptron', 'sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'pre_sentence': '', 'post_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'pre_sentence': '', 'post_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Siamese architecture', 'sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'pre_sentence': 'Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .', 'post_sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'pre_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'post_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional sentence model', 'sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'pre_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'post_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Siamese architecture', 'sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'pre_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'post_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'pre_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'post_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution model', 'sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'pre_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'post_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching task', 'sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'pre_sentence': 'It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .', 'post_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'forward phase', 'sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'pre_sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'post_sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'prediction', 'sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'pre_sentence': 'Although Arc - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .', 'post_sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'backward phase ( learning', 'sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'pre_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'post_sentence': '', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional model', 'sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'pre_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'post_sentence': '', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching', 'sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'pre_sentence': 'In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .', 'post_sentence': '', 'section_name': 'Architecture - I ( Arc - I )', 'section_index': 11, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Architecture - II', 'sentence': 'subsection : Architecture - II ( Arc - II )', 'pre_sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'post_sentence': 'In view of the drawback of Architecture - I , we propose Architecture - II ( Arc - II ) that is built directly on the interaction space between two sentences .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'subsection : Architecture - II ( Arc - II )', 'pre_sentence': 'This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .', 'post_sentence': 'In view of the drawback of Architecture - I , we propose Architecture - II ( Arc - II ) that is built directly on the interaction space between two sentences .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Architecture - I', 'sentence': 'In view of the drawback of Architecture - I , we propose Architecture - II ( Arc - II ) that is built directly on the interaction space between two sentences .', 'pre_sentence': '', 'post_sentence': 'It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Architecture - II', 'sentence': 'In view of the drawback of Architecture - I , we propose Architecture - II ( Arc - II ) that is built directly on the interaction space between two sentences .', 'pre_sentence': '', 'post_sentence': 'It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'In view of the drawback of Architecture - I , we propose Architecture - II ( Arc - II ) that is built directly on the interaction space between two sentences .', 'pre_sentence': '', 'post_sentence': 'It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Layer - 1', 'sentence': 'Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through “ one - dimensional ” ( 1D ) convolutions .', 'pre_sentence': 'It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .', 'post_sentence': 'For segment on and segment on , we have the feature map where simply concatenates the vectors for sentence segments for and : Clearly the 1D convolution preserves the location information about both segments .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'one - dimensional ” ( 1D ) convolutions', 'sentence': 'Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through “ one - dimensional ” ( 1D ) convolutions .', 'pre_sentence': 'It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .', 'post_sentence': 'For segment on and segment on , we have the feature map where simply concatenates the vectors for sentence segments for and : Clearly the 1D convolution preserves the location information about both segments .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '1D convolution', 'sentence': 'For segment on and segment on , we have the feature map where simply concatenates the vectors for sentence segments for and : Clearly the 1D convolution preserves the location information about both segments .', 'pre_sentence': 'Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through “ one - dimensional ” ( 1D ) convolutions .', 'post_sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '2D max - pooling', 'sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'pre_sentence': 'For segment on and segment on , we have the feature map where simply concatenates the vectors for sentence segments for and : Clearly the 1D convolution preserves the location information about both segments .', 'post_sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '2D convolution', 'sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'pre_sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'post_sentence': '', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '2D convolution', 'sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'pre_sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'post_sentence': '', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '2D max - pooling', 'sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'pre_sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'post_sentence': '', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architecture', 'sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'pre_sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'post_sentence': '', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'image input', 'sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'pre_sentence': 'After that in Layer - 2 , it performs a 2D max - pooling in non - overlapping windows ( illustrated in Figure [ reference ] )', 'post_sentence': '', 'section_name': 'Architecture - II ( Arc - II )', 'section_index': 12, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '2D - Convolution', 'sentence': 'paragraph : The 2D - Convolution', 'pre_sentence': 'In Layer - 3 , we perform a 2D convolution on windows of output from Layer - 2 : This could go on for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .', 'post_sentence': 'After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation which encodes the information from both sentences .', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'low level representation', 'sentence': 'After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation which encodes the information from both sentences .', 'pre_sentence': '', 'post_sentence': 'The general two - dimensional convolution is formulated as where concatenates the corresponding vectors from its 2D receptive field in Layer - .', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'high level representation', 'sentence': 'After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation which encodes the information from both sentences .', 'pre_sentence': '', 'post_sentence': 'The general two - dimensional convolution is formulated as where concatenates the corresponding vectors from its 2D receptive field in Layer - .', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'two - dimensional convolution', 'sentence': 'The general two - dimensional convolution is formulated as where concatenates the corresponding vectors from its 2D receptive field in Layer - .', 'pre_sentence': 'After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation which encodes the information from both sentences .', 'post_sentence': 'This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': '1D case', 'sentence': 'This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .', 'pre_sentence': 'The general two - dimensional convolution is formulated as where concatenates the corresponding vectors from its 2D receptive field in Layer - .', 'post_sentence': 'This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions : 1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling strategy', 'sentence': 'This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions : 1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .', 'pre_sentence': 'This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .', 'post_sentence': '', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'dynamic pooling', 'sentence': 'This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions : 1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .', 'pre_sentence': 'This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .', 'post_sentence': '', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'similarity learning context', 'sentence': 'This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions : 1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .', 'pre_sentence': 'This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .', 'post_sentence': '', 'section_name': 'The 2D - Convolution', 'section_index': 13, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'subsection : Some Analysis on Arc - II', 'pre_sentence': 'This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions : 1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .', 'post_sentence': '', 'section_name': 'Some Analysis on Arc - II', 'section_index': 14, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Order Preservation', 'sentence': 'paragraph : Order Preservation', 'pre_sentence': 'subsection : Some Analysis on Arc - II', 'post_sentence': 'Both the convolution and pooling operation in Architecture - II have this order preserving property .', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution and pooling operation', 'sentence': 'Both the convolution and pooling operation in Architecture - II have this order preserving property .', 'pre_sentence': '', 'post_sentence': 'Generally , contains information about the words in before those in , although they may be generated with slightly different segments in , due to the 2D pooling ( illustrated in Figure [ reference ] ) .', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Architecture - II', 'sentence': 'Both the convolution and pooling operation in Architecture - II have this order preserving property .', 'pre_sentence': '', 'post_sentence': 'Generally , contains information about the words in before those in , although they may be generated with slightly different segments in , due to the 2D pooling ( illustrated in Figure [ reference ] ) .', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '2D pooling', 'sentence': 'Generally , contains information about the words in before those in , although they may be generated with slightly different segments in , due to the 2D pooling ( illustrated in Figure [ reference ] ) .', 'pre_sentence': 'Both the convolution and pooling operation in Architecture - II have this order preserving property .', 'post_sentence': 'The orders is however retained in a “ conditional ” sense .', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Our experiments show that when Arc - II is trained on the triples where randomly shuffles the words in , it consistently gains some ability of finding the correct in the usual contrastive negative sampling setting , which however does not happen with Arc - I .', 'pre_sentence': 'The orders is however retained in a “ conditional ” sense .', 'post_sentence': '', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'contrastive negative sampling setting', 'sentence': 'Our experiments show that when Arc - II is trained on the triples where randomly shuffles the words in , it consistently gains some ability of finding the correct in the usual contrastive negative sampling setting , which however does not happen with Arc - I .', 'pre_sentence': 'The orders is however retained in a “ conditional ” sense .', 'post_sentence': '', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Our experiments show that when Arc - II is trained on the triples where randomly shuffles the words in , it consistently gains some ability of finding the correct in the usual contrastive negative sampling setting , which however does not happen with Arc - I .', 'pre_sentence': 'The orders is however retained in a “ conditional ” sense .', 'post_sentence': '', 'section_name': 'Order Preservation', 'section_index': 15, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'It is not hard to show that Arc - II actually subsumes Arc - I as a special case .', 'pre_sentence': '', 'post_sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'It is not hard to show that Arc - II actually subsumes Arc - I as a special case .', 'pre_sentence': '', 'post_sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'pre_sentence': 'It is not hard to show that Arc - II actually subsumes Arc - I as a special case .', 'post_sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'pre_sentence': 'It is not hard to show that Arc - II actually subsumes Arc - I as a special case .', 'post_sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'pre_sentence': 'It is not hard to show that Arc - II actually subsumes Arc - I as a special case .', 'post_sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'pre_sentence': 'It is not hard to show that Arc - II actually subsumes Arc - I as a special case .', 'post_sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution layer', 'sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'pre_sentence': 'Indeed , in Arc - II if we choose ( by turning off some parameters in ) to keep the representations of the two sentences separated until the final MLP , Arc - II can actually act fully like Arc - I , as illustrated in Figure [ reference ] .', 'post_sentence': 'As a result , the output for each filter , denoted ( is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in Arc - I .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution layer', 'sentence': 'As a result , the output for each filter , denoted ( is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in Arc - I .', 'pre_sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'post_sentence': 'Clearly the 2D pooling that follows will reduce to 1D pooling , with this separateness preserved .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'As a result , the output for each filter , denoted ( is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in Arc - I .', 'pre_sentence': 'More specifically , if we let the feature maps in the first convolution layer to be either devoted to or devoted to ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .', 'post_sentence': 'Clearly the 2D pooling that follows will reduce to 1D pooling , with this separateness preserved .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': '2D pooling', 'sentence': 'Clearly the 2D pooling that follows will reduce to 1D pooling , with this separateness preserved .', 'pre_sentence': 'As a result , the output for each filter , denoted ( is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in Arc - I .', 'post_sentence': 'If we further limit the parameters in the second convolution units ( more specifically ) to those for and , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of Arc - I .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '1D pooling', 'sentence': 'Clearly the 2D pooling that follows will reduce to 1D pooling , with this separateness preserved .', 'pre_sentence': 'As a result , the output for each filter , denoted ( is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in Arc - I .', 'post_sentence': 'If we further limit the parameters in the second convolution units ( more specifically ) to those for and , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of Arc - I .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution units', 'sentence': 'If we further limit the parameters in the second convolution units ( more specifically ) to those for and , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of Arc - I .', 'pre_sentence': 'Clearly the 2D pooling that follows will reduce to 1D pooling , with this separateness preserved .', 'post_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'If we further limit the parameters in the second convolution units ( more specifically ) to those for and , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of Arc - I .', 'pre_sentence': 'Clearly the 2D pooling that follows will reduce to 1D pooling , with this separateness preserved .', 'post_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'pre_sentence': 'If we further limit the parameters in the second convolution units ( more specifically ) to those for and , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of Arc - I .', 'post_sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'internal abstraction', 'sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'pre_sentence': 'If we further limit the parameters in the second convolution units ( more specifically ) to those for and , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of Arc - I .', 'post_sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'pre_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'post_sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'diverging processes', 'sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'pre_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'post_sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'successive composition', 'sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'pre_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'post_sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'fusion of matching patterns', 'sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'pre_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'post_sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching linguistic objects', 'sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'pre_sentence': 'As suggested by the order - preserving property and the generality of Arc - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .', 'post_sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'pre_sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'post_sentence': '', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'This intuition is verified by the superior performance of Arc - II in experiments ( Section [ reference ] ) on different matching tasks .', 'pre_sentence': 'As a result , Arc - II can naturally blend two seemingly diverging processes : 1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .', 'post_sentence': '', 'section_name': 'Model Generality', 'section_index': 16, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'discriminative training strategy', 'sentence': 'We employ a discriminative training strategy with a large margin objective .', 'pre_sentence': '', 'post_sentence': 'Suppose that we are given the following triples from the oracle , with matched with better than with .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'ranking - based loss', 'sentence': 'We have the following ranking - based loss as objective : where is predicted matching score for , and includes the parameters for convolution layers and those for the MLP .', 'pre_sentence': 'Suppose that we are given the following triples from the oracle , with matched with better than with .', 'post_sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'predicted matching score', 'sentence': 'We have the following ranking - based loss as objective : where is predicted matching score for , and includes the parameters for convolution layers and those for the MLP .', 'pre_sentence': 'Suppose that we are given the following triples from the oracle , with matched with better than with .', 'post_sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution layers', 'sentence': 'We have the following ranking - based loss as objective : where is predicted matching score for , and includes the parameters for convolution layers and those for the MLP .', 'pre_sentence': 'Suppose that we are given the following triples from the oracle , with matched with better than with .', 'post_sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'We have the following ranking - based loss as objective : where is predicted matching score for , and includes the parameters for convolution layers and those for the MLP .', 'pre_sentence': 'Suppose that we are given the following triples from the oracle , with matched with better than with .', 'post_sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'optimization', 'sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'pre_sentence': 'We have the following ranking - based loss as objective : where is predicted matching score for , and includes the parameters for convolution layers and those for the MLP .', 'post_sentence': 'The gating function ( see Section [ reference ] ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'back - propagation', 'sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'pre_sentence': 'We have the following ranking - based loss as objective : where is predicted matching score for , and includes the parameters for convolution layers and those for the MLP .', 'post_sentence': 'The gating function ( see Section [ reference ] ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'gating function', 'sentence': 'The gating function ( see Section [ reference ] ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .', 'pre_sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'post_sentence': 'In other words , We use stochastic gradient descent for the optimization of models .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution units', 'sentence': 'The gating function ( see Section [ reference ] ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .', 'pre_sentence': 'The optimization is relatively straightforward for both architectures with the standard back - propagation .', 'post_sentence': 'In other words , We use stochastic gradient descent for the optimization of models .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'stochastic gradient descent', 'sentence': 'In other words , We use stochastic gradient descent for the optimization of models .', 'pre_sentence': 'The gating function ( see Section [ reference ] ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .', 'post_sentence': 'All the proposed models perform better with mini - batch ( in sizes ) which can be easily parallelized on single machine with multi - cores .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'optimization of models', 'sentence': 'In other words , We use stochastic gradient descent for the optimization of models .', 'pre_sentence': 'The gating function ( see Section [ reference ] ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .', 'post_sentence': 'All the proposed models perform better with mini - batch ( in sizes ) which can be easily parallelized on single machine with multi - cores .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'regularization', 'sentence': 'For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500 K instances ) .', 'pre_sentence': 'All the proposed models perform better with mini - batch ( in sizes ) which can be easily parallelized on single machine with multi - cores .', 'post_sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'early stopping', 'sentence': 'For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500 K instances ) .', 'pre_sentence': 'All the proposed models perform better with mini - batch ( in sizes ) which can be easily parallelized on single machine with multi - cores .', 'post_sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'early stopping', 'sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'pre_sentence': 'For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500 K instances ) .', 'post_sentence': 'We use 50 - dimensional word embedding trained with the Word2Vec :', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'dropout', 'sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'pre_sentence': 'For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500 K instances ) .', 'post_sentence': 'We use 50 - dimensional word embedding trained with the Word2Vec :', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'overfitting problem', 'sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'pre_sentence': 'For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500 K instances ) .', 'post_sentence': 'We use 50 - dimensional word embedding trained with the Word2Vec :', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '50 - dimensional word embedding', 'sentence': 'We use 50 - dimensional word embedding trained with the Word2Vec :', 'pre_sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'post_sentence': 'the embedding for English words ( Section [ reference ] & [ reference ] ) is learnt on Wikipedia ( 1B words ) , while that for Chinese words ( Section [ reference ] ) is learnt on Weibo data ( 300 M words ) .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Word2Vec', 'sentence': 'We use 50 - dimensional word embedding trained with the Word2Vec :', 'pre_sentence': 'For small datasets ( less than 10k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .', 'post_sentence': 'the embedding for English words ( Section [ reference ] & [ reference ] ) is learnt on Wikipedia ( 1B words ) , while that for Chinese words ( Section [ reference ] ) is learnt on Weibo data ( 300 M words ) .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'word embedding', 'sentence': 'Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .', 'pre_sentence': 'the embedding for English words ( Section [ reference ] & [ reference ] ) is learnt on Wikipedia ( 1B words ) , while that for Chinese words ( Section [ reference ] ) is learnt on Weibo data ( 300 M words ) .', 'post_sentence': 'We vary the maximum length of words for different tasks to cope with its longest sentence .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'training', 'sentence': 'Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .', 'pre_sentence': 'the embedding for English words ( Section [ reference ] & [ reference ] ) is learnt on Wikipedia ( 1B words ) , while that for Chinese words ( Section [ reference ] ) is learnt on Weibo data ( 300 M words ) .', 'post_sentence': 'We vary the maximum length of words for different tasks to cope with its longest sentence .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'ReLu', 'sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'pre_sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'post_sentence': '', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'activation function', 'sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'pre_sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'post_sentence': '', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolution and MLP', 'sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'pre_sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'post_sentence': '', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'pre_sentence': '', 'post_sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'competitor models', 'sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'pre_sentence': '', 'post_sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Sentence Completion', 'sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'pre_sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'post_sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Tweet - Response Matching', 'sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'pre_sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'post_sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching of language objects', 'sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'pre_sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'post_sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'paraphrase identification', 'sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'pre_sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'post_sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching homogeneous objects', 'sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'pre_sentence': 'We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .', 'post_sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching', 'sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'pre_sentence': 'Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .', 'post_sentence': '', 'section_name': 'Experiments', 'section_index': 18, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Competitor Methods', 'sentence': 'subsection : Competitor Methods', 'pre_sentence': 'Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .', 'post_sentence': 'WordEmbed :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'matching score', 'sentence': 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DeepMatch : We take the matching model in and train it on our datasets with 3 hidden layers and 1 , 000 hidden nodes in the first hidden layer ;', 'pre_sentence': 'We first represent each short - text as the sum of the embedding of the words it contains .', 'post_sentence': 'uRAE + MLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DeepMatch : We take the matching model in and train it on our datasets with 3 hidden layers and 1 , 000 hidden nodes in the first hidden layer ;', 'pre_sentence': 'We first represent each short - text as the sum of the embedding of the words it contains .', 'post_sentence': 'uRAE + MLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'DeepMatch', 'sentence': 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DeepMatch : We take the matching model in and train it on our datasets with 3 hidden layers and 1 , 000 hidden nodes in the first hidden layer ;', 'pre_sentence': 'We first represent each short - text as the sum of the embedding of the words it contains .', 'post_sentence': 'uRAE + MLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'matching model', 'sentence': 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DeepMatch : We take the matching model in and train it on our datasets with 3 hidden layers and 1 , 000 hidden nodes in the first hidden layer ;', 'pre_sentence': 'We first represent each short - text as the sum of the embedding of the words it contains .', 'post_sentence': 'uRAE + MLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'uRAE + MLP', 'sentence': 'uRAE + MLP :', 'pre_sentence': 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DeepMatch : We take the matching model in and train it on our datasets with 3 hidden layers and 1 , 000 hidden nodes in the first hidden layer ;', 'post_sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Unfolding Recursive Autoencoder', 'sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'pre_sentence': 'uRAE + MLP :', 'post_sentence': 'SenMLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': '100 - dimensional vector representation', 'sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'pre_sentence': 'uRAE + MLP :', 'post_sentence': 'SenMLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'pre_sentence': 'uRAE + MLP :', 'post_sentence': 'SenMLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SENNA - type sentence model', 'sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'pre_sentence': 'uRAE + MLP :', 'post_sentence': 'SenMLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence representation', 'sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'pre_sentence': 'uRAE + MLP :', 'post_sentence': 'SenMLP :', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SenMLP', 'sentence': 'SenMLP :', 'pre_sentence': 'We use the Unfolding Recursive Autoencoder to get a 100 - dimensional vector representation of each sentence , and put an MLP on the top as in WordEmbed ; SENNA + MLP / sim : We use the SENNA - type sentence model for sentence representation ;', 'post_sentence': 'We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .', 'pre_sentence': 'SenMLP :', 'post_sentence': 'All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'score of coherence', 'sentence': 'We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .', 'pre_sentence': 'SenMLP :', 'post_sentence': 'All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'MLP', 'sentence': 'All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .', 'pre_sentence': 'We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .', 'post_sentence': '', 'section_name': 'Competitor Methods', 'section_index': 19, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Sentence Completion', 'sentence': 'subsection : Experiment I : Sentence Completion', 'pre_sentence': 'All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .', 'post_sentence': 'This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'artificial task', 'sentence': 'This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .', 'pre_sentence': '', 'post_sentence': 'Basically , we take a sentence from Reuters with two', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'matching models', 'sentence': 'This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .', 'pre_sentence': '', 'post_sentence': 'Basically , we take a sentence from Reuters with two', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching', 'sentence': 'The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .', 'pre_sentence': 'The task is then to recover the original second clause for any given first clause .', 'post_sentence': 'We deliberately make the task harder by using negative second clauses similar to the original ones , both in training and testing .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sentence models', 'sentence': 'The two proposed models get nearly half of the cases right , with large margin over other sentence models and models without explicit sequence modeling .', 'pre_sentence': 'All models are trained on 3 million triples ( from 600 K positive pairs ) , and tested on 50 K positive pairs , each accompanied by four negatives , with results shown in Table [ reference ] .', 'post_sentence': 'Arc - II outperforms', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'explicit sequence modeling', 'sentence': 'The two proposed models get nearly half of the cases right , with large margin over other sentence models and models without explicit sequence modeling .', 'pre_sentence': 'All models are trained on 3 million triples ( from 600 K positive pairs ) , and tested on 50 K positive pairs , each accompanied by four negatives , with results shown in Table [ reference ] .', 'post_sentence': 'Arc - II outperforms', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Arc - II outperforms', 'pre_sentence': 'The two proposed models get nearly half of the cases right , with large margin over other sentence models and models without explicit sequence modeling .', 'post_sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'pre_sentence': 'Arc - II outperforms', 'post_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'joint modeling of matching', 'sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'pre_sentence': 'Arc - II outperforms', 'post_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence meaning', 'sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'pre_sentence': 'Arc - II outperforms', 'post_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional model', 'sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'pre_sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'post_sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SENNA + MLP', 'sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'pre_sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'post_sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional architectures', 'sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'pre_sentence': 'Arc - I significantly , showing the power of joint modeling of matching and sentence meaning .', 'post_sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'representation model', 'sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'pre_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'post_sentence': '', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'word - embedding', 'sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'pre_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'post_sentence': '', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'parsing', 'sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'pre_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'post_sentence': '', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sentence representation', 'sentence': 'It is a bit surprising that uRAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation .', 'pre_sentence': 'As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .', 'post_sentence': '', 'section_name': 'Experiment I : Sentence Completion', 'section_index': 20, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'matching model', 'sentence': 'We hold out 300 K original ( tweet , response ) pairs and test the matching model on their ability to pick the original response from four random negatives , with results reported in Table [ reference ] .', 'pre_sentence': ': It is hard to find a job , better start polishing your resume .', 'post_sentence': 'This task is slightly easier than Experiment I , with more training instances and purely random negatives .', 'section_name': 'Experiment II : Matching A Response to A Tweet', 'section_index': 21, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Again Arc - II beats other models with large margins , while two convolutional sentence models', 'pre_sentence': 'It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime⇔ rest ) .', 'post_sentence': 'Arc - I and SENNA + MLP come next .', 'section_name': 'Experiment II : Matching A Response to A Tweet', 'section_index': 21, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional sentence models', 'sentence': 'Again Arc - II beats other models with large margins , while two convolutional sentence models', 'pre_sentence': 'It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime⇔ rest ) .', 'post_sentence': 'Arc - I and SENNA + MLP come next .', 'section_name': 'Experiment II : Matching A Response to A Tweet', 'section_index': 21, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Arc - I and SENNA + MLP come next .', 'pre_sentence': 'Again Arc - II beats other models with large margins , while two convolutional sentence models', 'post_sentence': '', 'section_name': 'Experiment II : Matching A Response to A Tweet', 'section_index': 21, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SENNA + MLP', 'sentence': 'Arc - I and SENNA + MLP come next .', 'pre_sentence': 'Again Arc - II beats other models with large margins , while two convolutional sentence models', 'post_sentence': '', 'section_name': 'Experiment II : Matching A Response to A Tweet', 'section_index': 21, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Paraphrase Identification', 'sentence': 'subsection : Experiment III : Paraphrase Identification', 'pre_sentence': 'Arc - I and SENNA + MLP come next .', 'post_sentence': 'Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Paraphrase identification', 'sentence': 'Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .', 'pre_sentence': '', 'post_sentence': 'This experiment is included to test our methods on matching homogenous objects .', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'natural language understanding', 'sentence': 'Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .', 'pre_sentence': '', 'post_sentence': 'This experiment is included to test our methods on matching homogenous objects .', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching homogenous objects', 'sentence': 'This experiment is included to test our methods on matching homogenous objects .', 'pre_sentence': 'Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .', 'post_sentence': 'Here we use the benchmark MSRP dataset , which contains 4 , 076 instances for training and 1 , 725 for test .', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'synonymy', 'sentence': 'As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires instances to work favorably .', 'pre_sentence': 'We use all the training instances and report the test performance from early stopping .', 'post_sentence': 'Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8% / 83.6 % ) , achieved with unfolding - RAE and other features designed for this task .', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'generic matching models', 'sentence': 'Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8% / 83.6 % ) , achieved with unfolding - RAE and other features designed for this task .', 'pre_sentence': 'As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires instances to work favorably .', 'post_sentence': '', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'accuracy', 'sentence': 'Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8% / 83.6 % ) , achieved with unfolding - RAE and other features designed for this task .', 'pre_sentence': 'As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires instances to work favorably .', 'post_sentence': '', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'F1 score', 'sentence': 'Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8% / 83.6 % ) , achieved with unfolding - RAE and other features designed for this task .', 'pre_sentence': 'As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires instances to work favorably .', 'post_sentence': '', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'unfolding - RAE', 'sentence': 'Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8% / 83.6 % ) , achieved with unfolding - RAE and other features designed for this task .', 'pre_sentence': 'As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires instances to work favorably .', 'post_sentence': '', 'section_name': 'Experiment III : Paraphrase Identification', 'section_index': 22, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Arc - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .', 'pre_sentence': '', 'post_sentence': 'Its superiority over Arc - I , however , is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns , as in Experiment - I.', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Its superiority over Arc - I , however , is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns , as in Experiment - I.', 'pre_sentence': 'Arc - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .', 'post_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching', 'sentence': 'Its superiority over Arc - I , however , is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns , as in Experiment - I.', 'pre_sentence': 'Arc - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .', 'post_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'learning process', 'sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'pre_sentence': 'Its superiority over Arc - I , however , is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns , as in Experiment - I.', 'post_sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'curriculum learning', 'sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'pre_sentence': 'Its superiority over Arc - I , however , is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns , as in Experiment - I.', 'post_sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'convolutional models', 'sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'pre_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'post_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'pre_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'post_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'II', 'sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'pre_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'post_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'SENNA + MLP', 'sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'pre_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'post_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'bag - of - words models', 'sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'pre_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'post_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching sentences', 'sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'pre_sentence': 'This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .', 'post_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - I', 'sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'pre_sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'post_sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'pre_sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'post_sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'accuracy', 'sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'pre_sentence': 'As another important observation , convolutional models ( Arc - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .', 'post_sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'auxiliary task', 'sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'pre_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'post_sentence': 'We noticed that simple sum of embedding learned via Word2Vec yields reasonably good results on all three tasks .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'pre_sentence': 'Quite interestingly , as shown by our other experiments , Arc - I and Arc - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .', 'post_sentence': 'We noticed that simple sum of embedding learned via Word2Vec yields reasonably good results on all three tasks .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sum of embedding', 'sentence': 'We noticed that simple sum of embedding learned via Word2Vec yields reasonably good results on all three tasks .', 'pre_sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'post_sentence': 'We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Word2Vec', 'sentence': 'We noticed that simple sum of embedding learned via Word2Vec yields reasonably good results on all three tasks .', 'pre_sentence': 'It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .', 'post_sentence': 'We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Word2Vec embedding', 'sentence': 'We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .', 'pre_sentence': 'We noticed that simple sum of embedding learned via Word2Vec yields reasonably good results on all three tasks .', 'post_sentence': 'This is in contrast with other bag - of - words models like DeepMatch .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'vector summation', 'sentence': 'We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .', 'pre_sentence': 'We noticed that simple sum of embedding learned via Word2Vec yields reasonably good results on all three tasks .', 'post_sentence': 'This is in contrast with other bag - of - words models like DeepMatch .', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'bag - of - words models', 'sentence': 'This is in contrast with other bag - of - words models like DeepMatch .', 'pre_sentence': 'We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .', 'post_sentence': '', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'DeepMatch', 'sentence': 'This is in contrast with other bag - of - words models like DeepMatch .', 'pre_sentence': 'We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .', 'post_sentence': '', 'section_name': 'Discussions', 'section_index': 23, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'Matching structured objects', 'sentence': 'Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .', 'pre_sentence': '', 'post_sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'language objects', 'sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'pre_sentence': 'Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .', 'post_sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'vectorial representations', 'sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'pre_sentence': 'Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .', 'post_sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Metric', 'ner': 'matching score', 'sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'pre_sentence': 'Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .', 'post_sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'inner product', 'sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'pre_sentence': 'Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .', 'post_sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'deep architecture', 'sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'pre_sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'post_sentence': 'Our models are related to the long thread of work on sentence representation .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'bag - of - words representation of text', 'sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'pre_sentence': 'When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .', 'post_sentence': 'Our models are related to the long thread of work on sentence representation .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence representation', 'sentence': 'Our models are related to the long thread of work on sentence representation .', 'pre_sentence': 'Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .', 'post_sentence': 'Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'sum of word - embedding', 'sentence': 'Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .', 'pre_sentence': 'Our models are related to the long thread of work on sentence representation .', 'post_sentence': 'There is very little work on convolutional modeling of language .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'classification', 'sentence': 'Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .', 'pre_sentence': 'Our models are related to the long thread of work on sentence representation .', 'post_sentence': 'There is very little work on convolutional modeling of language .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'convolutional modeling of language', 'sentence': 'There is very little work on convolutional modeling of language .', 'pre_sentence': 'Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .', 'post_sentence': 'In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'sentence representation', 'sentence': 'In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .', 'pre_sentence': 'There is very little work on convolutional modeling of language .', 'post_sentence': 'This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'dynamic convolutional neural network', 'sentence': 'In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .', 'pre_sentence': 'There is very little work on convolutional modeling of language .', 'post_sentence': 'This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'pooling strategy', 'sentence': 'This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .', 'pre_sentence': 'In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .', 'post_sentence': '', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'classification problems', 'sentence': 'This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .', 'pre_sentence': 'In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .', 'post_sentence': '', 'section_name': 'Related Work', 'section_index': 24, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'deep convolutional architectures', 'sentence': 'We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .', 'pre_sentence': '', 'post_sentence': 'Empirical study shows our models can outperform competitors on a variety of matching tasks .', 'section_name': 'Conclusion', 'section_index': 25, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching natural language sentences', 'sentence': 'We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .', 'pre_sentence': '', 'post_sentence': 'Empirical study shows our models can outperform competitors on a variety of matching tasks .', 'section_name': 'Conclusion', 'section_index': 25, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'hierarchical modeling', 'sentence': 'We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .', 'pre_sentence': '', 'post_sentence': 'Empirical study shows our models can outperform competitors on a variety of matching tasks .', 'section_name': 'Conclusion', 'section_index': 25, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Task', 'ner': 'matching tasks', 'sentence': 'Empirical study shows our models can outperform competitors on a variety of matching tasks .', 'pre_sentence': 'We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .', 'post_sentence': '', 'section_name': 'Conclusion', 'section_index': 25, 'salient': '0'}\n",
      "{'doc_id': '07f3f736d90125cb2b04e7408782af411c67dd5a', 'relation': 'Method', 'ner': 'Arc - II', 'sentence': 'Arc - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while Arc - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .', 'pre_sentence': 'We use 3 - word window throughout all experiments , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .', 'post_sentence': 'We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .', 'section_name': 'Training', 'section_index': 17, 'salient': '0'}\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "for paper in result:\n",
    "    # Each word in the paper will be a single entry in the following sentences array.\n",
    "    # The entry consists of the start and end index of the sentences.\n",
    "    # Later, also the section start and end index, as well as the section name\n",
    "    # and section index will be added to the object.\n",
    "    sentences = []  # a list of objects.\n",
    "    for sentence in paper['sentences']:\n",
    "        for i in range(sentence[0], sentence[1]):\n",
    "            # For each word in the sentence, add an entry that contains the start and end index for the sentence of the word.\n",
    "            sentences.append({'start': sentence[0], 'end': sentence[1]})\n",
    "\n",
    "    # Populate the sentences list with section information.\n",
    "    for index, section in enumerate(paper['sections']):\n",
    "        # Get the first sentence of the section.\n",
    "        sentence = sentences[section[0]]\n",
    "        # The section name is the first sentence of the section.\n",
    "        section_name = paper['words'][sentence['start']:sentence['end']]\n",
    "        \n",
    "        # Example for the first sentence on a section:\n",
    "        # [\"section\", \":\", \"Abstract\"]\n",
    "        # If the first sentence starts with [\"section\", \":\"], we are only interested in the words after that prefix.\n",
    "        if len(section_name) >= 2 and section_name[1] == \":\":\n",
    "            section_name_length = len(section_name)\n",
    "            section_name = section_name[2:]\n",
    "        else:\n",
    "            section_name_length = 0\n",
    "            if index == 0:\n",
    "                # First section will always be labled as 'Title'\n",
    "                section_name = ['Title']\n",
    "            else:\n",
    "                section_name = []\n",
    "        \n",
    "        # Add section info\n",
    "        for i in range(section[0], section[1]):\n",
    "            sentences[i]['section'] = {'name': section_name, 'index': index, 'start': section[0] + section_name_length, 'end': section[1]}\n",
    "\n",
    "    # Iterate through all ranges of named entities and retrieve the corresponding sentence and section info for that word\n",
    "    # by querying the index from the sentences array.\n",
    "    words = paper['words']\n",
    "    salient_entities = [x.lower() for x in paper['coref'].keys()]\n",
    "    for entity in paper['ner']:\n",
    "        # info is an object with\n",
    "        info = sentences[entity[0]]\n",
    "        begin_of_section = info['start'] == info['section']['start']\n",
    "        end_of_section = info['end'] == info['section']['end']\n",
    "\n",
    "        # The sentence will be a list of words\n",
    "        sentence = words[info['start']:info['end']]\n",
    "\n",
    "        # Add the previous sentence, but only if the current sentence is not the start of the section.\n",
    "        if not begin_of_section:\n",
    "            pre_info = sentences[info['start'] - 1]\n",
    "            pre_sentence = words[pre_info['start']:pre_info['end']]\n",
    "        else:\n",
    "            pre_sentence = []\n",
    "\n",
    "        # Add the next sentence, but only if the current sentence is not the end of the section.\n",
    "        if not end_of_section:\n",
    "            post_info = sentences[info['end'] + 1]\n",
    "            post_sentence = words[post_info['start']:post_info['end']]\n",
    "        else:\n",
    "            post_sentence = []\n",
    "\n",
    "        entity_name = words[entity[0]:entity[1]]\n",
    "        # Create an entry for the extracted entity\n",
    "        entry = {\n",
    "                    'doc_id': paper['doc_id'],                  # Document id\n",
    "                    'relation': entity[2],                      # Classification (method, dataset, metric, ...)\n",
    "                    'ner': \" \".join(entity_name),                         # Name of the entity\n",
    "                    'sentence':  \" \".join(sentence),                      # Sentence = array of words\n",
    "                    'pre_sentence': \" \".join(pre_sentence),               # Previous sentence\n",
    "                    'post_sentence': \" \".join(post_sentence),             # Next sentence\n",
    "                    'section_name': \" \".join(info['section']['name']),   # Name of the section\n",
    "                    'section_index': info['section']['index'],  # Index of the section\n",
    "                    'salient': \"1\" if \"_\".join(entity_name).lower() in salient_entities else \"0\"\n",
    "                }\n",
    "        if paper['doc_id'] == \"07f3f736d90125cb2b04e7408782af411c67dd5a\":\n",
    "            print(entry)\n",
    "        content.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relation</th>\n",
       "      <th>ner</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pre_sentence</th>\n",
       "      <th>post_sentence</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_index</th>\n",
       "      <th>salient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14347</th>\n",
       "      <td>07f3f736d90125cb2b04e7408782af411c67dd5a</td>\n",
       "      <td>Metric</td>\n",
       "      <td>matching score</td>\n",
       "      <td>The matching score of two short - texts are ca...</td>\n",
       "      <td>We first represent each short - text as the su...</td>\n",
       "      <td>uRAE + MLP :</td>\n",
       "      <td>Competitor Methods</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14348</th>\n",
       "      <td>07f3f736d90125cb2b04e7408782af411c67dd5a</td>\n",
       "      <td>Method</td>\n",
       "      <td>MLP</td>\n",
       "      <td>The matching score of two short - texts are ca...</td>\n",
       "      <td>We first represent each short - text as the su...</td>\n",
       "      <td>uRAE + MLP :</td>\n",
       "      <td>Competitor Methods</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14349</th>\n",
       "      <td>07f3f736d90125cb2b04e7408782af411c67dd5a</td>\n",
       "      <td>Method</td>\n",
       "      <td>DeepMatch</td>\n",
       "      <td>The matching score of two short - texts are ca...</td>\n",
       "      <td>We first represent each short - text as the su...</td>\n",
       "      <td>uRAE + MLP :</td>\n",
       "      <td>Competitor Methods</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>07f3f736d90125cb2b04e7408782af411c67dd5a</td>\n",
       "      <td>Method</td>\n",
       "      <td>matching model</td>\n",
       "      <td>The matching score of two short - texts are ca...</td>\n",
       "      <td>We first represent each short - text as the su...</td>\n",
       "      <td>uRAE + MLP :</td>\n",
       "      <td>Competitor Methods</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         doc_id relation             ner  \\\n",
       "14347  07f3f736d90125cb2b04e7408782af411c67dd5a   Metric  matching score   \n",
       "14348  07f3f736d90125cb2b04e7408782af411c67dd5a   Method             MLP   \n",
       "14349  07f3f736d90125cb2b04e7408782af411c67dd5a   Method       DeepMatch   \n",
       "14350  07f3f736d90125cb2b04e7408782af411c67dd5a   Method  matching model   \n",
       "\n",
       "                                                sentence  \\\n",
       "14347  The matching score of two short - texts are ca...   \n",
       "14348  The matching score of two short - texts are ca...   \n",
       "14349  The matching score of two short - texts are ca...   \n",
       "14350  The matching score of two short - texts are ca...   \n",
       "\n",
       "                                            pre_sentence post_sentence  \\\n",
       "14347  We first represent each short - text as the su...  uRAE + MLP :   \n",
       "14348  We first represent each short - text as the su...  uRAE + MLP :   \n",
       "14349  We first represent each short - text as the su...  uRAE + MLP :   \n",
       "14350  We first represent each short - text as the su...  uRAE + MLP :   \n",
       "\n",
       "             section_name  section_index salient  \n",
       "14347  Competitor Methods             19       0  \n",
       "14348  Competitor Methods             19       0  \n",
       "14349  Competitor Methods             19       0  \n",
       "14350  Competitor Methods             19       0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(content)\n",
    "df.loc[df['sentence']==\"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DeepMatch : We take the matching model in and train it on our datasets with 3 hidden layers and 1 , 000 hidden nodes in the first hidden layer ;\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107997"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. filter for method\n",
    "is_method = df['relation'] == 'Material'\n",
    "df_method = df[is_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_method['ner_list'] = [','.join(map(str, l)) for l in df_method['ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. shuffle\n",
    "df_method = df_method.sample(frac=1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method_unique = df_method.drop_duplicates(subset='ner', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method_sample = df_method_unique.sample(n = 1000, random_state = 101)\n",
    "df_method_sample.to_csv('annotation_data_train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relation</th>\n",
       "      <th>ner</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pre_sentence</th>\n",
       "      <th>post_sentence</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_index</th>\n",
       "      <th>salient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68054</th>\n",
       "      <td>2bb9f0768fac9622a0be446df69daf75a954d5ac</td>\n",
       "      <td>Material</td>\n",
       "      <td>LDC2014T12</td>\n",
       "      <td>1 ) JAMR flanigan - EtAl:2014:P14 - 1 , flanig...</td>\n",
       "      <td>For the extrinsic evaluation , we plug our ali...</td>\n",
       "      <td>We use the configuration in flanigan - EtAl:20...</td>\n",
       "      <td>Settings</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432</th>\n",
       "      <td>05357b8c05b5bc020e871fc330a88910c3177e4d</td>\n",
       "      <td>Material</td>\n",
       "      <td>PASCAL VOC protocol</td>\n",
       "      <td>Average Precision ( AP ) and the mean of AP ( ...</td>\n",
       "      <td>For testing , there are two metrics for evalua...</td>\n",
       "      <td>Correct localization ( CorLoc ) is to test our...</td>\n",
       "      <td>Datasets and evaluation measures</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>060ff1aad5619a7d6d6cdfaf8be5da29bff3808c</td>\n",
       "      <td>Material</td>\n",
       "      <td>CoNLL - 2012</td>\n",
       "      <td>subsubsection : CoNLL - 2012</td>\n",
       "      <td>We use the pre - trained ELMo models and learn...</td>\n",
       "      <td>We follow the CoNLL - 2012 split used by he201...</td>\n",
       "      <td>CoNLL - 2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>074b6fe0cc6848fb86a6703d1c52074494177c79</td>\n",
       "      <td>Material</td>\n",
       "      <td>winter images</td>\n",
       "      <td>The subset of the dataset we use contains 13 c...</td>\n",
       "      <td>We use only the front - facing views in the se...</td>\n",
       "      <td>To further demonstrate our method ’s applicabi...</td>\n",
       "      <td>Semantic Segmentation Adaptation</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101655</th>\n",
       "      <td>42764b57d0794b63487a295ce8c07eeb6961477e</td>\n",
       "      <td>Material</td>\n",
       "      <td>MS COCO segmentation dataset</td>\n",
       "      <td>We demonstrate excellent accuracy on the chall...</td>\n",
       "      <td>Thanks to the end - to - end training and the ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26343</th>\n",
       "      <td>0e37c8f19eefeb0c20d92f5cb4df4153077c116b</td>\n",
       "      <td>Material</td>\n",
       "      <td>100</td>\n",
       "      <td>C 10 / 100</td>\n",
       "      <td>C96 + 32M1 ⇥ 1 !</td>\n",
       "      <td>1 ⇥ 1 !</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45659</th>\n",
       "      <td>1b29786b7e43dda1a4d6ee93f520a2960b1e3126</td>\n",
       "      <td>Material</td>\n",
       "      <td>WikiMovies</td>\n",
       "      <td>WikiMovies contains 100k questions in the movi...</td>\n",
       "      <td>To this end , this paper introduces WikiMovies...</td>\n",
       "      <td>To bridge the gap between using a KB and readi...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43118</th>\n",
       "      <td>19839ffab4c30db1556d7fd9275d1344a6e3fa46</td>\n",
       "      <td>Material</td>\n",
       "      <td>OntoNotes</td>\n",
       "      <td>The larger CoNLL - 2012 dataset is extracted f...</td>\n",
       "      <td>The test set consists of section 23 of WSJ for...</td>\n",
       "      <td>CoNLL 2008 and 2009 CoNLL - 2008 and the Engli...</td>\n",
       "      <td>Datasets</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78410</th>\n",
       "      <td>325093f2c5b33d7507c10aa422e96aa5b10a33f1</td>\n",
       "      <td>Material</td>\n",
       "      <td>Mapillary Vistas</td>\n",
       "      <td>State - of - the - art segmentations are typic...</td>\n",
       "      <td>The goal of semantic segmentation is to assign...</td>\n",
       "      <td>Datasets used for Evaluation .</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71606</th>\n",
       "      <td>2d876ed1dd2c58058d7197b734a8e4d349b8f231</td>\n",
       "      <td>Material</td>\n",
       "      <td>TED.tst2013</td>\n",
       "      <td>Our best performance on a development set ( TE...</td>\n",
       "      <td>We remove training sentences with more than 30...</td>\n",
       "      <td>Inputs were supplied to the encoder reversed ,...</td>\n",
       "      <td>Character - level Neural Machine Translation</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          doc_id  relation  \\\n",
       "68054   2bb9f0768fac9622a0be446df69daf75a954d5ac  Material   \n",
       "9432    05357b8c05b5bc020e871fc330a88910c3177e4d  Material   \n",
       "10344   060ff1aad5619a7d6d6cdfaf8be5da29bff3808c  Material   \n",
       "13297   074b6fe0cc6848fb86a6703d1c52074494177c79  Material   \n",
       "101655  42764b57d0794b63487a295ce8c07eeb6961477e  Material   \n",
       "...                                          ...       ...   \n",
       "26343   0e37c8f19eefeb0c20d92f5cb4df4153077c116b  Material   \n",
       "45659   1b29786b7e43dda1a4d6ee93f520a2960b1e3126  Material   \n",
       "43118   19839ffab4c30db1556d7fd9275d1344a6e3fa46  Material   \n",
       "78410   325093f2c5b33d7507c10aa422e96aa5b10a33f1  Material   \n",
       "71606   2d876ed1dd2c58058d7197b734a8e4d349b8f231  Material   \n",
       "\n",
       "                                 ner  \\\n",
       "68054                     LDC2014T12   \n",
       "9432             PASCAL VOC protocol   \n",
       "10344                   CoNLL - 2012   \n",
       "13297                  winter images   \n",
       "101655  MS COCO segmentation dataset   \n",
       "...                              ...   \n",
       "26343                            100   \n",
       "45659                     WikiMovies   \n",
       "43118                      OntoNotes   \n",
       "78410               Mapillary Vistas   \n",
       "71606                    TED.tst2013   \n",
       "\n",
       "                                                 sentence  \\\n",
       "68054   1 ) JAMR flanigan - EtAl:2014:P14 - 1 , flanig...   \n",
       "9432    Average Precision ( AP ) and the mean of AP ( ...   \n",
       "10344                        subsubsection : CoNLL - 2012   \n",
       "13297   The subset of the dataset we use contains 13 c...   \n",
       "101655  We demonstrate excellent accuracy on the chall...   \n",
       "...                                                   ...   \n",
       "26343                                          C 10 / 100   \n",
       "45659   WikiMovies contains 100k questions in the movi...   \n",
       "43118   The larger CoNLL - 2012 dataset is extracted f...   \n",
       "78410   State - of - the - art segmentations are typic...   \n",
       "71606   Our best performance on a development set ( TE...   \n",
       "\n",
       "                                             pre_sentence  \\\n",
       "68054   For the extrinsic evaluation , we plug our ali...   \n",
       "9432    For testing , there are two metrics for evalua...   \n",
       "10344   We use the pre - trained ELMo models and learn...   \n",
       "13297   We use only the front - facing views in the se...   \n",
       "101655  Thanks to the end - to - end training and the ...   \n",
       "...                                                   ...   \n",
       "26343                                    C96 + 32M1 ⇥ 1 !   \n",
       "45659   To this end , this paper introduces WikiMovies...   \n",
       "43118   The test set consists of section 23 of WSJ for...   \n",
       "78410   The goal of semantic segmentation is to assign...   \n",
       "71606   We remove training sentences with more than 30...   \n",
       "\n",
       "                                            post_sentence  \\\n",
       "68054   We use the configuration in flanigan - EtAl:20...   \n",
       "9432    Correct localization ( CorLoc ) is to test our...   \n",
       "10344   We follow the CoNLL - 2012 split used by he201...   \n",
       "13297   To further demonstrate our method ’s applicabi...   \n",
       "101655                                                      \n",
       "...                                                   ...   \n",
       "26343                                             1 ⇥ 1 !   \n",
       "45659   To bridge the gap between using a KB and readi...   \n",
       "43118   CoNLL 2008 and 2009 CoNLL - 2008 and the Engli...   \n",
       "78410                      Datasets used for Evaluation .   \n",
       "71606   Inputs were supplied to the encoder reversed ,...   \n",
       "\n",
       "                                        section_name  section_index salient  \n",
       "68054                                       Settings             17       0  \n",
       "9432                Datasets and evaluation measures              8       0  \n",
       "10344                                   CoNLL - 2012             20       0  \n",
       "13297               Semantic Segmentation Adaptation              6       0  \n",
       "101655                                  Introduction              1       0  \n",
       "...                                              ...            ...     ...  \n",
       "26343                                          Title              0       0  \n",
       "45659                                   Introduction              1       0  \n",
       "43118                                       Datasets             14       1  \n",
       "78410                          Semantic Segmentation             10       0  \n",
       "71606   Character - level Neural Machine Translation              7       0  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_method_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_method_unique)\n",
    "len(df_method_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
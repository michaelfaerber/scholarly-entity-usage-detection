{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_excel('annotation_data_train_datasets.xlsx', sheet_name='a1')\n",
    "df_2 = pd.read_excel('annotation_data_train_datasets.xlsx', sheet_name='a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relation</th>\n",
       "      <th>used</th>\n",
       "      <th>anmerkung</th>\n",
       "      <th>felix</th>\n",
       "      <th>ner</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pre_sentence</th>\n",
       "      <th>post_sentence</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>68054</td>\n",
       "      <td>2bb9f0768fac9622a0be446df69daf75a954d5ac</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LDC2014T12</td>\n",
       "      <td>1 ) JAMR flanigan - EtAl:2014:P14 - 1 , flanig...</td>\n",
       "      <td>For the extrinsic evaluation , we plug our ali...</td>\n",
       "      <td>We use the configuration in flanigan - EtAl:20...</td>\n",
       "      <td>Settings</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9432</td>\n",
       "      <td>05357b8c05b5bc020e871fc330a88910c3177e4d</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>PASCAL VOC protocol</td>\n",
       "      <td>Average Precision ( AP ) and the mean of AP ( ...</td>\n",
       "      <td>For testing , there are two metrics for evalua...</td>\n",
       "      <td>Correct localization ( CorLoc ) is to test our...</td>\n",
       "      <td>Datasets and evaluation measures</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10344</td>\n",
       "      <td>060ff1aad5619a7d6d6cdfaf8be5da29bff3808c</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CoNLL - 2012</td>\n",
       "      <td>subsubsection : CoNLL - 2012</td>\n",
       "      <td>We use the pre - trained ELMo models and learn...</td>\n",
       "      <td>We follow the CoNLL - 2012 split used by he201...</td>\n",
       "      <td>CoNLL - 2012</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13297</td>\n",
       "      <td>074b6fe0cc6848fb86a6703d1c52074494177c79</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein Dataset name? -&gt; na?</td>\n",
       "      <td>na</td>\n",
       "      <td>winter images</td>\n",
       "      <td>The subset of the dataset we use contains 13 c...</td>\n",
       "      <td>We use only the front - facing views in the se...</td>\n",
       "      <td>To further demonstrate our method ’s applicabi...</td>\n",
       "      <td>Semantic Segmentation Adaptation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>101655</td>\n",
       "      <td>42764b57d0794b63487a295ce8c07eeb6961477e</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MS COCO segmentation dataset</td>\n",
       "      <td>We demonstrate excellent accuracy on the chall...</td>\n",
       "      <td>Thanks to the end - to - end training and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>23323</td>\n",
       "      <td>0ca2bd0e40a8f0a57665535ae1c31561370ad183</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>enwik8</td>\n",
       "      <td>The Hutter Prize Wikipedia ( enwik8 ) dataset ...</td>\n",
       "      <td>Hutter Prize Wikipedia</td>\n",
       "      <td>We follow the data splits used in [ reference ...</td>\n",
       "      <td>Text8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>65161</td>\n",
       "      <td>29c19276b8fff231717c3e342cb24144d2b77726</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>They use a convolutional neural network ( CNN ...</td>\n",
       "      <td>For POS tagging , santos : zadrozny:2014 were ...</td>\n",
       "      <td>ling :</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>91323</td>\n",
       "      <td>3b1b94441010615195a5c404409ce2416860508c</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>kein Dataset name? -&gt; na?</td>\n",
       "      <td>0</td>\n",
       "      <td>Large - scale Knowledge Bases</td>\n",
       "      <td>Large - scale Knowledge Bases ( KBs ) , such a...</td>\n",
       "      <td>Our framework also exploits both CNN and RNNs ...</td>\n",
       "      <td>However , VQA systems exploiting KBs are still...</td>\n",
       "      <td>Visual Question Answering</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>47618</td>\n",
       "      <td>1c7e078611c9df412e6eb3a356f31a0da0c1f99c</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>YCB - Video</td>\n",
       "      <td>To thoroughly evaluate our method , we additio...</td>\n",
       "      <td>RGB - D pose estimation ( we use depth images ...</td>\n",
       "      <td>Objects in the dataset exhibit different symme...</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>102947</td>\n",
       "      <td>432d8cba544bf7b09b0455561fea098177a85db1</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>youtube_faces</td>\n",
       "      <td>We use the Youtube Faces Database from youtube...</td>\n",
       "      <td>Finally , we provide a proof of concept for ge...</td>\n",
       "      <td>It contains videos of different people .</td>\n",
       "      <td>Youtube Faces</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>67129</td>\n",
       "      <td>2aec8d465e9a74c27f956ed1136f3e8a3ba0a833</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Kodak24 dataset</td>\n",
       "      <td>The Kodak24 dataset consists of 24 center - cr...</td>\n",
       "      <td>BSD68 dataset .</td>\n",
       "      <td>The McMaster dataset is a widely - used datase...</td>\n",
       "      <td>Dataset Generation and Network Training</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>67141</td>\n",
       "      <td>2aec8d465e9a74c27f956ed1136f3e8a3ba0a833</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>camera noise</td>\n",
       "      <td>We note that RNI6 and RNI15 cover a variety of...</td>\n",
       "      <td>The RNI15 dataset consists of 15 real noisy im...</td>\n",
       "      <td>Since the ground - truth clean images are unav...</td>\n",
       "      <td>Dataset Generation and Network Training</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>85388</td>\n",
       "      <td>36911f5fc4f4eb1221f832114946de4773cf78e6</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TREC - CAR</td>\n",
       "      <td>We follow the same procedure described for the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>However , there is an important difference .</td>\n",
       "      <td>Training</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>94329</td>\n",
       "      <td>3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MS COCO dataset</td>\n",
       "      <td>We applied Fast R - CNN ( with VGG16 ) to the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We trained on the 80k image training set for 2...</td>\n",
       "      <td>Preliminary MS COCO results</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>73013</td>\n",
       "      <td>2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Metabolic</td>\n",
       "      <td>For the MF baseline , we closely follow the ex...</td>\n",
       "      <td>We begin our empirical evaluation with the SDN...</td>\n",
       "      <td>For PowerGrid , we use 90 percent of observed ...</td>\n",
       "      <td>Datasets and Baselines</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>80678</td>\n",
       "      <td>34a6762ed8e92612ba4fdf02ee95d2ee0d587908</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>DukeMTMC</td>\n",
       "      <td>We introduced two pipelines after ResNet , one...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Each pipeline consists of two convolutional bl...</td>\n",
       "      <td>Multiple Pipelines</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>92439</td>\n",
       "      <td>3c1d781f2dab8da12e3cb0e4d7abfb440a340a09</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Flickr30k corpus</td>\n",
       "      <td>The premises are drawn from the Flickr30k corp...</td>\n",
       "      <td>The Stanford Natural Language Inference ( SNLI...</td>\n",
       "      <td>The “ - ” class indicates that there is no con...</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>21017</td>\n",
       "      <td>0b5aef2894d3248fb5ecc955d50501f0aa276036</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>IEMOCAP dataset</td>\n",
       "      <td>As the IEMOCAP dataset contains four distinct ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In order to perform classification on IEMOCAP ...</td>\n",
       "      <td>IEMOCAP</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2490</td>\n",
       "      <td>0171bdeb1c6e333287be655c667cfba5edb89b76</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5 K dataset</td>\n",
       "      <td>Our 5 K dataset is a subset of the full ImageN...</td>\n",
       "      <td>Next we evaluate our models on a larger ImageN...</td>\n",
       "      <td>The 5000 categories consist of the original Im...</td>\n",
       "      <td>Experiments on ImageNet - 5 K</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>26939</td>\n",
       "      <td>0ecd4fdce541317b38124967b5c2a259d8f43c91</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>7 - bit images</td>\n",
       "      <td>Games can easily last 10 , 000 time steps ( co...</td>\n",
       "      <td>The Atari 2600 Pong , however , is significant...</td>\n",
       "      <td>In sheer size , the Atari 2600 Pong is thus a ...</td>\n",
       "      <td>Final Remarks</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>88196</td>\n",
       "      <td>380b2c78d21ae6c43d418b6f0cb0222d5293d345</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Chinese CTB - 5</td>\n",
       "      <td>we report results in the English PTB and Chine...</td>\n",
       "      <td>lstmacl15</td>\n",
       "      <td>Table [ reference ] shows the results of the p...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>44551</td>\n",
       "      <td>1a5ea605111eb3403868d4b679315e944beee8c6</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Czech monolingual news2015</td>\n",
       "      <td>Then we continued training the model on a new ...</td>\n",
       "      <td>This took approximately 1 M minibatches , or 3...</td>\n",
       "      <td>The model used for back - translation was a ne...</td>\n",
       "      <td>English Czech</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>18565</td>\n",
       "      <td>0a3a003457f5d7758a42a0e4b7278b39a86ed0bd</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>5 - shot</td>\n",
       "      <td>Figure [ reference ] shows 1 ) MTL with HT met...</td>\n",
       "      <td>Speed of convergence of HT meta - batch .</td>\n",
       "      <td>10 - shot , on the more challenging dataset – ...</td>\n",
       "      <td>Results and analysis</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>31814</td>\n",
       "      <td>1023b20d226bd0af9fdf0fd1847accefbfa5ec84</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CNN and Daily Mail news data</td>\n",
       "      <td>black Several large cloze - style context - qu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to the size of these datasets , the ass...</td>\n",
       "      <td>Text Understanding with the Attention Sum Read...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>105972</td>\n",
       "      <td>4543052aeaf52fdb01fced9b3ccf97827582cef5</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Leeds Sports Pose</td>\n",
       "      <td>We use two benchmark human pose estimation dat...</td>\n",
       "      <td>Human Pose Datasets .</td>\n",
       "      <td>The MPII is collected from YouTube videos with...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>86150</td>\n",
       "      <td>36a03f648b40d209ce361550dbe1c823ddb715b5</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>HANDS 2017</td>\n",
       "      <td>The qualitative results of the V2V - PoseNet o...</td>\n",
       "      <td>As shown in Table 5 , the proposed system outp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comparison with state - of - the - art methods</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>64622</td>\n",
       "      <td>289e91654f6da968d625481ef21f52892052d4fc</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Kanshan - Cup dataset</td>\n",
       "      <td>We observed the following from the Table [ ref...</td>\n",
       "      <td>Table [ reference ] gives the performance of o...</td>\n",
       "      <td>That may be because in Chinese the words can o...</td>\n",
       "      <td>Performance Comparison</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>66027</td>\n",
       "      <td>2a94c84383ee3de5e6211d43d16e7de387f68878</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>minival</td>\n",
       "      <td>More object detection results using Faster R -...</td>\n",
       "      <td>In Ta - Table 5 .</td>\n",
       "      <td>Sharing features increases train time by 1.5× ...</td>\n",
       "      <td>Faster R - CNN ( on consistent proposals )</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>71522</td>\n",
       "      <td>2d876ed1dd2c58058d7197b734a8e4d349b8f231</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>IMDb movie review dataset</td>\n",
       "      <td>We evaluate the QRNN architecture on a popular...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The dataset consists of a balanced sample of 2...</td>\n",
       "      <td>Sentiment Classification</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>76583</td>\n",
       "      <td>303fef411f235e6d1125a40af1e93224f498a4d5</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>text8 dataset</td>\n",
       "      <td>We conduct experiments of CharLM using the tex...</td>\n",
       "      <td>Hence , we expect MoS to yield similar perform...</td>\n",
       "      <td>We follow mikolov2012subword and use the first...</td>\n",
       "      <td>An inverse experiment on character - level lan...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>94441</td>\n",
       "      <td>3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>extended PASCAL - Person - Part</td>\n",
       "      <td>Extensive experiments on MPII Human Pose Multi...</td>\n",
       "      <td>We implement GPN based on the Hourglass networ...</td>\n",
       "      <td>Moreover , GPN achieves new state - of - the -...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>36467</td>\n",
       "      <td>14ad9d060c1e8f0449e697ee189ac346353fbfbc</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>JNLPBA</td>\n",
       "      <td>The error analysis of our STM , which is a sin...</td>\n",
       "      <td>Error analysis was conducted on models which s...</td>\n",
       "      <td>According to the error analysis of our STM mod...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>73415</td>\n",
       "      <td>2e57bccb74bcb46cbc5b4225b62679023ed1f9da</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CNN and Daily Mail Datasets</td>\n",
       "      <td>section : CNN and Daily Mail Datasets</td>\n",
       "      <td>In this section , we evaluate the performance ...</td>\n",
       "      <td>We examine the performance of ReasoNets on CNN...</td>\n",
       "      <td>CNN and Daily Mail Datasets</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>100263</td>\n",
       "      <td>41d08fb733f3e50ac183490f84d6377dffccf350</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3D - R2N2</td>\n",
       "      <td>On many categories our method even outperforms...</td>\n",
       "      <td>3R - R2N2 is also able to predict 3D shapes fr...</td>\n",
       "      <td>To further contrast the two methods , we visua...</td>\n",
       "      <td>3D Shape Reconstruction from RGB Images</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2971</td>\n",
       "      <td>01959ef569f74c286956024866c1d107099199f7</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>abstract scenes</td>\n",
       "      <td>We collected a new dataset of \" realistic \" ab...</td>\n",
       "      <td>The MS COCO dataset has images depicting diver...</td>\n",
       "      <td>Three questions were collected for each image ...</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>96826</td>\n",
       "      <td>3febb2bed8865945e7fddc99efd791887bb7e14f</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>unlabeled data</td>\n",
       "      <td>In contrast , after pretraining the biLM with ...</td>\n",
       "      <td>Dai2015SemisupervisedSL and Ramachandran2017Im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Related work</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>40463</td>\n",
       "      <td>16cd50316e41cbb1d9dfeafeb524b31654cef37a</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Wall Street Journal task</td>\n",
       "      <td>In the area of speech recognition , much of th...</td>\n",
       "      <td>Recent years have seen human performance level...</td>\n",
       "      <td>One of last big initiatives in this area was i...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>26633</td>\n",
       "      <td>0ecd4fdce541317b38124967b5c2a259d8f43c91</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>video games</td>\n",
       "      <td>The console ’s joystick , as well as some of t...</td>\n",
       "      <td>Over 500 original games were released for the ...</td>\n",
       "      <td>Nearly all arcade games of the time – Pac - Ma...</td>\n",
       "      <td>The Atari 2600</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>45695</td>\n",
       "      <td>1b29786b7e43dda1a4d6ee93f520a2960b1e3126</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>na</td>\n",
       "      <td>question - answer pairs</td>\n",
       "      <td>Using similar resources as the dialog dataset ...</td>\n",
       "      <td>Unfortunately , these datasets are very small ...</td>\n",
       "      <td>Even though standard pipeline QA systems like ...</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>65172</td>\n",
       "      <td>29c19276b8fff231717c3e342cb24144d2b77726</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>WSJ</td>\n",
       "      <td>ea:2015:arxiv , however , they only explore wo...</td>\n",
       "      <td>Bi - LSTMs for POS tagging are also reported i...</td>\n",
       "      <td>A related study is cheng : fang : ostendorf:20...</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>102038</td>\n",
       "      <td>42e80c73867bff9eaff6beceb8730fc1276283b9</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2014 / 2016 French - English</td>\n",
       "      <td>2014 / 2016 French - English and German - Engl...</td>\n",
       "      <td>Our experiments on WMT</td>\n",
       "      <td>Our system also outperforms the supervised WMT...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>85300</td>\n",
       "      <td>3652c2d20f198dc39ad159eba55d08341c56d628</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>STL10</td>\n",
       "      <td>We show that mixing the ideas of these two con...</td>\n",
       "      <td>In this paper , we have proposed a new methodo...</td>\n",
       "      <td>Some challenges regarding our work are left op...</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>97713</td>\n",
       "      <td>40193e7ba0fbd7153a1fe15e95563463b67c71f3</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>AFLW2000 - 3D.</td>\n",
       "      <td>AFLW2000 - 3D. As can be seen , the 3D reconst...</td>\n",
       "      <td>7 ( a ) shows the comparison results on</td>\n",
       "      <td>We show some visual results of our 2DASL and c...</td>\n",
       "      <td>3D face reconstruction</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>63723</td>\n",
       "      <td>27e4b65121d3c88643d86dc91a9bdafdf223b988</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>DUC - 2003</td>\n",
       "      <td>The task of abstractive summarization has been...</td>\n",
       "      <td>As such , human summaries are abstractive in n...</td>\n",
       "      <td>The data for these tasks consists of news stor...</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>81385</td>\n",
       "      <td>34f63959ea4a13a05948274a1558c6854a051150</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>STS - B</td>\n",
       "      <td>paragraph : STS - B</td>\n",
       "      <td>Accuracy is used as the evaluation metric .</td>\n",
       "      <td>The Semantic Textual Similarity Benchmark is a...</td>\n",
       "      <td>STS - B</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>58839</td>\n",
       "      <td>23dcfda130aada27c158c0b5f394cac489c9c795</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>AFW</td>\n",
       "      <td>AFW is a popular dataset , also commonly used ...</td>\n",
       "      <td>Results can be seen in Table [ reference ] .</td>\n",
       "      <td>It contains 468 in - the - wild faces with abs...</td>\n",
       "      <td>AFLW and AFW Benchmarking</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>59779</td>\n",
       "      <td>2451db113552afb6d9ad15ef4009ec4133d28f74</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Stanford cars</td>\n",
       "      <td>For fine - grained categorization , we use thr...</td>\n",
       "      <td>As in , we report the results on the validatio...</td>\n",
       "      <td>The Birds dataset contains 11 , 788 images fro...</td>\n",
       "      <td>Datasets and Our Meta - layer Implementation</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>45923</td>\n",
       "      <td>1b29786b7e43dda1a4d6ee93f520a2960b1e3126</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WikiQA</td>\n",
       "      <td>Note that this is much larger than most existi...</td>\n",
       "      <td>The same question ( even worded differently ) ...</td>\n",
       "      <td>[ reference ] has only 1000 training pairs .</td>\n",
       "      <td>Question - Answer Pairs</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>102927</td>\n",
       "      <td>432d8cba544bf7b09b0455561fea098177a85db1</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>OMNIGLOT</td>\n",
       "      <td>In Figure [ reference ] we show two examples o...</td>\n",
       "      <td>The decoder used a Bernoulli likelihood .</td>\n",
       "      <td>The samples are mostly of a high - quality , a...</td>\n",
       "      <td>Omniglot</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>50668</td>\n",
       "      <td>1e7678467b1807777dcd9be557b79328ce9419a8</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>YFCC100 M large - scale collection of unlabell...</td>\n",
       "      <td>We add 10 K distractor images randomly sampled...</td>\n",
       "      <td>We also report the performance of our network ...</td>\n",
       "      <td>We call the combination C10k .</td>\n",
       "      <td>Datasets .</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column1                                    doc_id  relation used  \\\n",
       "0     68054  2bb9f0768fac9622a0be446df69daf75a954d5ac  Material    1   \n",
       "1      9432  05357b8c05b5bc020e871fc330a88910c3177e4d  Material    1   \n",
       "2     10344  060ff1aad5619a7d6d6cdfaf8be5da29bff3808c  Material    1   \n",
       "3     13297  074b6fe0cc6848fb86a6703d1c52074494177c79  Material   na   \n",
       "4    101655  42764b57d0794b63487a295ce8c07eeb6961477e  Material    1   \n",
       "5     23323  0ca2bd0e40a8f0a57665535ae1c31561370ad183  Material    1   \n",
       "6     65161  29c19276b8fff231717c3e342cb24144d2b77726  Material    0   \n",
       "7     91323  3b1b94441010615195a5c404409ce2416860508c  Material    0   \n",
       "8     47618  1c7e078611c9df412e6eb3a356f31a0da0c1f99c  Material    1   \n",
       "9    102947  432d8cba544bf7b09b0455561fea098177a85db1  Material    1   \n",
       "10    67129  2aec8d465e9a74c27f956ed1136f3e8a3ba0a833  Material    0   \n",
       "11    67141  2aec8d465e9a74c27f956ed1136f3e8a3ba0a833  Material   na   \n",
       "12    85388  36911f5fc4f4eb1221f832114946de4773cf78e6  Material    1   \n",
       "13    94329  3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a  Material    1   \n",
       "14    73013  2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79  Material    1   \n",
       "15    80678  34a6762ed8e92612ba4fdf02ee95d2ee0d587908  Material    1   \n",
       "16    92439  3c1d781f2dab8da12e3cb0e4d7abfb440a340a09  Material    0   \n",
       "17    21017  0b5aef2894d3248fb5ecc955d50501f0aa276036  Material    1   \n",
       "18     2490  0171bdeb1c6e333287be655c667cfba5edb89b76  Material    1   \n",
       "19    26939  0ecd4fdce541317b38124967b5c2a259d8f43c91  Material   na   \n",
       "20    88196  380b2c78d21ae6c43d418b6f0cb0222d5293d345  Material    1   \n",
       "21    44551  1a5ea605111eb3403868d4b679315e944beee8c6  Material    1   \n",
       "22    18565  0a3a003457f5d7758a42a0e4b7278b39a86ed0bd  Material   na   \n",
       "23    31814  1023b20d226bd0af9fdf0fd1847accefbfa5ec84  Material    0   \n",
       "24   105972  4543052aeaf52fdb01fced9b3ccf97827582cef5  Material    1   \n",
       "25    86150  36a03f648b40d209ce361550dbe1c823ddb715b5  Material    1   \n",
       "26    64622  289e91654f6da968d625481ef21f52892052d4fc  Material    1   \n",
       "27    66027  2a94c84383ee3de5e6211d43d16e7de387f68878  Material    1   \n",
       "28    71522  2d876ed1dd2c58058d7197b734a8e4d349b8f231  Material    1   \n",
       "29    76583  303fef411f235e6d1125a40af1e93224f498a4d5  Material    1   \n",
       "30    94441  3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a  Material    1   \n",
       "31    36467  14ad9d060c1e8f0449e697ee189ac346353fbfbc  Material    1   \n",
       "32    73415  2e57bccb74bcb46cbc5b4225b62679023ed1f9da  Material    1   \n",
       "33   100263  41d08fb733f3e50ac183490f84d6377dffccf350  Material    1   \n",
       "34     2971  01959ef569f74c286956024866c1d107099199f7  Material   na   \n",
       "35    96826  3febb2bed8865945e7fddc99efd791887bb7e14f  Material   na   \n",
       "36    40463  16cd50316e41cbb1d9dfeafeb524b31654cef37a  Material    0   \n",
       "37    26633  0ecd4fdce541317b38124967b5c2a259d8f43c91  Material   na   \n",
       "38    45695  1b29786b7e43dda1a4d6ee93f520a2960b1e3126  Material   na   \n",
       "39    65172  29c19276b8fff231717c3e342cb24144d2b77726  Material    0   \n",
       "40   102038  42e80c73867bff9eaff6beceb8730fc1276283b9  Material    1   \n",
       "41    85300  3652c2d20f198dc39ad159eba55d08341c56d628  Material    1   \n",
       "42    97713  40193e7ba0fbd7153a1fe15e95563463b67c71f3  Material    1   \n",
       "43    63723  27e4b65121d3c88643d86dc91a9bdafdf223b988  Material    0   \n",
       "44    81385  34f63959ea4a13a05948274a1558c6854a051150  Material    0   \n",
       "45    58839  23dcfda130aada27c158c0b5f394cac489c9c795  Material    0   \n",
       "46    59779  2451db113552afb6d9ad15ef4009ec4133d28f74  Material    1   \n",
       "47    45923  1b29786b7e43dda1a4d6ee93f520a2960b1e3126  Material    1   \n",
       "48   102927  432d8cba544bf7b09b0455561fea098177a85db1  Material    1   \n",
       "49    50668  1e7678467b1807777dcd9be557b79328ce9419a8  Material    1   \n",
       "\n",
       "                    anmerkung felix  \\\n",
       "0                         NaN     1   \n",
       "1                         NaN     1   \n",
       "2                         NaN     1   \n",
       "3   kein Dataset name? -> na?    na   \n",
       "4                         NaN     1   \n",
       "5                           ?     1   \n",
       "6                         NaN     0   \n",
       "7   kein Dataset name? -> na?     0   \n",
       "8                         NaN     1   \n",
       "9                         NaN     1   \n",
       "10                        NaN     0   \n",
       "11                        NaN    na   \n",
       "12                        NaN     1   \n",
       "13                        NaN     1   \n",
       "14                        NaN     1   \n",
       "15                        NaN     1   \n",
       "16                        NaN     0   \n",
       "17                        NaN     1   \n",
       "18                        NaN     1   \n",
       "19                        NaN    na   \n",
       "20                        NaN     1   \n",
       "21                        NaN     1   \n",
       "22                        NaN    na   \n",
       "23                        NaN     0   \n",
       "24                        NaN     1   \n",
       "25                          ?     1   \n",
       "26                        NaN     1   \n",
       "27                        NaN     1   \n",
       "28                        NaN     1   \n",
       "29                        NaN     1   \n",
       "30                        NaN     1   \n",
       "31                        NaN     1   \n",
       "32                        NaN     1   \n",
       "33                        NaN     1   \n",
       "34                        NaN    na   \n",
       "35                        NaN    na   \n",
       "36                        NaN     0   \n",
       "37                        NaN    na   \n",
       "38                        NaN    na   \n",
       "39                        NaN     0   \n",
       "40                        NaN     1   \n",
       "41                        NaN     1   \n",
       "42                        NaN     1   \n",
       "43                        NaN     0   \n",
       "44                        NaN     0   \n",
       "45                        NaN     0   \n",
       "46                        NaN     1   \n",
       "47                        NaN     1   \n",
       "48                        NaN     1   \n",
       "49                        NaN     1   \n",
       "\n",
       "                                                  ner  \\\n",
       "0                                          LDC2014T12   \n",
       "1                                 PASCAL VOC protocol   \n",
       "2                                        CoNLL - 2012   \n",
       "3                                       winter images   \n",
       "4                        MS COCO segmentation dataset   \n",
       "5                                              enwik8   \n",
       "6                                             English   \n",
       "7                       Large - scale Knowledge Bases   \n",
       "8                                         YCB - Video   \n",
       "9                                       youtube_faces   \n",
       "10                                    Kodak24 dataset   \n",
       "11                                       camera noise   \n",
       "12                                         TREC - CAR   \n",
       "13                                    MS COCO dataset   \n",
       "14                                          Metabolic   \n",
       "15                                           DukeMTMC   \n",
       "16                                   Flickr30k corpus   \n",
       "17                                    IEMOCAP dataset   \n",
       "18                                        5 K dataset   \n",
       "19                                     7 - bit images   \n",
       "20                                    Chinese CTB - 5   \n",
       "21                         Czech monolingual news2015   \n",
       "22                                           5 - shot   \n",
       "23                       CNN and Daily Mail news data   \n",
       "24                                  Leeds Sports Pose   \n",
       "25                                         HANDS 2017   \n",
       "26                              Kanshan - Cup dataset   \n",
       "27                                            minival   \n",
       "28                          IMDb movie review dataset   \n",
       "29                                      text8 dataset   \n",
       "30                    extended PASCAL - Person - Part   \n",
       "31                                             JNLPBA   \n",
       "32                        CNN and Daily Mail Datasets   \n",
       "33                                          3D - R2N2   \n",
       "34                                    abstract scenes   \n",
       "35                                     unlabeled data   \n",
       "36                           Wall Street Journal task   \n",
       "37                                        video games   \n",
       "38                            question - answer pairs   \n",
       "39                                                WSJ   \n",
       "40                       2014 / 2016 French - English   \n",
       "41                                              STL10   \n",
       "42                                     AFLW2000 - 3D.   \n",
       "43                                         DUC - 2003   \n",
       "44                                            STS - B   \n",
       "45                                                AFW   \n",
       "46                                      Stanford cars   \n",
       "47                                             WikiQA   \n",
       "48                                           OMNIGLOT   \n",
       "49  YFCC100 M large - scale collection of unlabell...   \n",
       "\n",
       "                                             sentence  \\\n",
       "0   1 ) JAMR flanigan - EtAl:2014:P14 - 1 , flanig...   \n",
       "1   Average Precision ( AP ) and the mean of AP ( ...   \n",
       "2                        subsubsection : CoNLL - 2012   \n",
       "3   The subset of the dataset we use contains 13 c...   \n",
       "4   We demonstrate excellent accuracy on the chall...   \n",
       "5   The Hutter Prize Wikipedia ( enwik8 ) dataset ...   \n",
       "6   They use a convolutional neural network ( CNN ...   \n",
       "7   Large - scale Knowledge Bases ( KBs ) , such a...   \n",
       "8   To thoroughly evaluate our method , we additio...   \n",
       "9   We use the Youtube Faces Database from youtube...   \n",
       "10  The Kodak24 dataset consists of 24 center - cr...   \n",
       "11  We note that RNI6 and RNI15 cover a variety of...   \n",
       "12  We follow the same procedure described for the...   \n",
       "13  We applied Fast R - CNN ( with VGG16 ) to the ...   \n",
       "14  For the MF baseline , we closely follow the ex...   \n",
       "15  We introduced two pipelines after ResNet , one...   \n",
       "16  The premises are drawn from the Flickr30k corp...   \n",
       "17  As the IEMOCAP dataset contains four distinct ...   \n",
       "18  Our 5 K dataset is a subset of the full ImageN...   \n",
       "19  Games can easily last 10 , 000 time steps ( co...   \n",
       "20  we report results in the English PTB and Chine...   \n",
       "21  Then we continued training the model on a new ...   \n",
       "22  Figure [ reference ] shows 1 ) MTL with HT met...   \n",
       "23  black Several large cloze - style context - qu...   \n",
       "24  We use two benchmark human pose estimation dat...   \n",
       "25  The qualitative results of the V2V - PoseNet o...   \n",
       "26  We observed the following from the Table [ ref...   \n",
       "27  More object detection results using Faster R -...   \n",
       "28  We evaluate the QRNN architecture on a popular...   \n",
       "29  We conduct experiments of CharLM using the tex...   \n",
       "30  Extensive experiments on MPII Human Pose Multi...   \n",
       "31  The error analysis of our STM , which is a sin...   \n",
       "32              section : CNN and Daily Mail Datasets   \n",
       "33  On many categories our method even outperforms...   \n",
       "34  We collected a new dataset of \" realistic \" ab...   \n",
       "35  In contrast , after pretraining the biLM with ...   \n",
       "36  In the area of speech recognition , much of th...   \n",
       "37  The console ’s joystick , as well as some of t...   \n",
       "38  Using similar resources as the dialog dataset ...   \n",
       "39  ea:2015:arxiv , however , they only explore wo...   \n",
       "40  2014 / 2016 French - English and German - Engl...   \n",
       "41  We show that mixing the ideas of these two con...   \n",
       "42  AFLW2000 - 3D. As can be seen , the 3D reconst...   \n",
       "43  The task of abstractive summarization has been...   \n",
       "44                                paragraph : STS - B   \n",
       "45  AFW is a popular dataset , also commonly used ...   \n",
       "46  For fine - grained categorization , we use thr...   \n",
       "47  Note that this is much larger than most existi...   \n",
       "48  In Figure [ reference ] we show two examples o...   \n",
       "49  We add 10 K distractor images randomly sampled...   \n",
       "\n",
       "                                         pre_sentence  \\\n",
       "0   For the extrinsic evaluation , we plug our ali...   \n",
       "1   For testing , there are two metrics for evalua...   \n",
       "2   We use the pre - trained ELMo models and learn...   \n",
       "3   We use only the front - facing views in the se...   \n",
       "4   Thanks to the end - to - end training and the ...   \n",
       "5                              Hutter Prize Wikipedia   \n",
       "6   For POS tagging , santos : zadrozny:2014 were ...   \n",
       "7   Our framework also exploits both CNN and RNNs ...   \n",
       "8   RGB - D pose estimation ( we use depth images ...   \n",
       "9   Finally , we provide a proof of concept for ge...   \n",
       "10                                    BSD68 dataset .   \n",
       "11  The RNI15 dataset consists of 15 real noisy im...   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14  We begin our empirical evaluation with the SDN...   \n",
       "15                                                NaN   \n",
       "16  The Stanford Natural Language Inference ( SNLI...   \n",
       "17                                                NaN   \n",
       "18  Next we evaluate our models on a larger ImageN...   \n",
       "19  The Atari 2600 Pong , however , is significant...   \n",
       "20                                          lstmacl15   \n",
       "21  This took approximately 1 M minibatches , or 3...   \n",
       "22          Speed of convergence of HT meta - batch .   \n",
       "23                                                NaN   \n",
       "24                              Human Pose Datasets .   \n",
       "25  As shown in Table 5 , the proposed system outp...   \n",
       "26  Table [ reference ] gives the performance of o...   \n",
       "27                                  In Ta - Table 5 .   \n",
       "28                                                NaN   \n",
       "29  Hence , we expect MoS to yield similar perform...   \n",
       "30  We implement GPN based on the Hourglass networ...   \n",
       "31  Error analysis was conducted on models which s...   \n",
       "32  In this section , we evaluate the performance ...   \n",
       "33  3R - R2N2 is also able to predict 3D shapes fr...   \n",
       "34  The MS COCO dataset has images depicting diver...   \n",
       "35  Dai2015SemisupervisedSL and Ramachandran2017Im...   \n",
       "36  Recent years have seen human performance level...   \n",
       "37  Over 500 original games were released for the ...   \n",
       "38  Unfortunately , these datasets are very small ...   \n",
       "39  Bi - LSTMs for POS tagging are also reported i...   \n",
       "40                             Our experiments on WMT   \n",
       "41  In this paper , we have proposed a new methodo...   \n",
       "42            7 ( a ) shows the comparison results on   \n",
       "43  As such , human summaries are abstractive in n...   \n",
       "44        Accuracy is used as the evaluation metric .   \n",
       "45       Results can be seen in Table [ reference ] .   \n",
       "46  As in , we report the results on the validatio...   \n",
       "47  The same question ( even worded differently ) ...   \n",
       "48          The decoder used a Bernoulli likelihood .   \n",
       "49  We also report the performance of our network ...   \n",
       "\n",
       "                                        post_sentence  \\\n",
       "0   We use the configuration in flanigan - EtAl:20...   \n",
       "1   Correct localization ( CorLoc ) is to test our...   \n",
       "2   We follow the CoNLL - 2012 split used by he201...   \n",
       "3   To further demonstrate our method ’s applicabi...   \n",
       "4                                                 NaN   \n",
       "5   We follow the data splits used in [ reference ...   \n",
       "6                                              ling :   \n",
       "7   However , VQA systems exploiting KBs are still...   \n",
       "8   Objects in the dataset exhibit different symme...   \n",
       "9            It contains videos of different people .   \n",
       "10  The McMaster dataset is a widely - used datase...   \n",
       "11  Since the ground - truth clean images are unav...   \n",
       "12       However , there is an important difference .   \n",
       "13  We trained on the 80k image training set for 2...   \n",
       "14  For PowerGrid , we use 90 percent of observed ...   \n",
       "15  Each pipeline consists of two convolutional bl...   \n",
       "16  The “ - ” class indicates that there is no con...   \n",
       "17  In order to perform classification on IEMOCAP ...   \n",
       "18  The 5000 categories consist of the original Im...   \n",
       "19  In sheer size , the Atari 2600 Pong is thus a ...   \n",
       "20  Table [ reference ] shows the results of the p...   \n",
       "21  The model used for back - translation was a ne...   \n",
       "22  10 - shot , on the more challenging dataset – ...   \n",
       "23  Thanks to the size of these datasets , the ass...   \n",
       "24  The MPII is collected from YouTube videos with...   \n",
       "25                                                NaN   \n",
       "26  That may be because in Chinese the words can o...   \n",
       "27  Sharing features increases train time by 1.5× ...   \n",
       "28  The dataset consists of a balanced sample of 2...   \n",
       "29  We follow mikolov2012subword and use the first...   \n",
       "30  Moreover , GPN achieves new state - of - the -...   \n",
       "31  According to the error analysis of our STM mod...   \n",
       "32  We examine the performance of ReasoNets on CNN...   \n",
       "33  To further contrast the two methods , we visua...   \n",
       "34  Three questions were collected for each image ...   \n",
       "35                                                NaN   \n",
       "36  One of last big initiatives in this area was i...   \n",
       "37  Nearly all arcade games of the time – Pac - Ma...   \n",
       "38  Even though standard pipeline QA systems like ...   \n",
       "39  A related study is cheng : fang : ostendorf:20...   \n",
       "40  Our system also outperforms the supervised WMT...   \n",
       "41  Some challenges regarding our work are left op...   \n",
       "42  We show some visual results of our 2DASL and c...   \n",
       "43  The data for these tasks consists of news stor...   \n",
       "44  The Semantic Textual Similarity Benchmark is a...   \n",
       "45  It contains 468 in - the - wild faces with abs...   \n",
       "46  The Birds dataset contains 11 , 788 images fro...   \n",
       "47       [ reference ] has only 1000 training pairs .   \n",
       "48  The samples are mostly of a high - quality , a...   \n",
       "49                     We call the combination C10k .   \n",
       "\n",
       "                                         section_name  section_index  \n",
       "0                                            Settings             17  \n",
       "1                    Datasets and evaluation measures              8  \n",
       "2                                        CoNLL - 2012             20  \n",
       "3                    Semantic Segmentation Adaptation              6  \n",
       "4                                        Introduction              1  \n",
       "5                                               Text8             10  \n",
       "6                                        Related Work             11  \n",
       "7                           Visual Question Answering              5  \n",
       "8                                        INTRODUCTION              1  \n",
       "9                                       Youtube Faces             19  \n",
       "10            Dataset Generation and Network Training             14  \n",
       "11            Dataset Generation and Network Training             14  \n",
       "12                                           Training              9  \n",
       "13                        Preliminary MS COCO results             25  \n",
       "14                             Datasets and Baselines              7  \n",
       "15                                 Multiple Pipelines              5  \n",
       "16                                            Dataset              9  \n",
       "17                                            IEMOCAP             38  \n",
       "18                      Experiments on ImageNet - 5 K             13  \n",
       "19                                      Final Remarks             35  \n",
       "20                                        Experiments              7  \n",
       "21                                      English Czech             10  \n",
       "22                               Results and analysis             12  \n",
       "23  Text Understanding with the Attention Sum Read...              0  \n",
       "24                                        Experiments              9  \n",
       "25     Comparison with state - of - the - art methods             16  \n",
       "26                             Performance Comparison             24  \n",
       "27         Faster R - CNN ( on consistent proposals )             13  \n",
       "28                           Sentiment Classification              5  \n",
       "29  An inverse experiment on character - level lan...             33  \n",
       "30                                       Introduction              1  \n",
       "31                                         Discussion             17  \n",
       "32                        CNN and Daily Mail Datasets              7  \n",
       "33            3D Shape Reconstruction from RGB Images             13  \n",
       "34                                       INTRODUCTION              2  \n",
       "35                                       Related work              2  \n",
       "36                                       Introduction              1  \n",
       "37                                     The Atari 2600              3  \n",
       "38                                       Related Work              2  \n",
       "39                                       Related Work             11  \n",
       "40                                       Introduction              1  \n",
       "41                                         Conclusion             26  \n",
       "42                             3D face reconstruction             16  \n",
       "43                                       Related Work              7  \n",
       "44                                            STS - B             19  \n",
       "45                          AFLW and AFW Benchmarking             12  \n",
       "46       Datasets and Our Meta - layer Implementation             13  \n",
       "47                            Question - Answer Pairs             16  \n",
       "48                                           Omniglot             18  \n",
       "49                                         Datasets .             27  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relation</th>\n",
       "      <th>used</th>\n",
       "      <th>anmerkung</th>\n",
       "      <th>alex</th>\n",
       "      <th>ner</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pre_sentence</th>\n",
       "      <th>post_sentence</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26515</td>\n",
       "      <td>0e8753f550350e53824358ca3f0f8cfd2f2dc2f7</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MovieLens 10 M dataset</td>\n",
       "      <td>We work with the widely used MovieLens 10 M da...</td>\n",
       "      <td>In this section , we report experiments on rea...</td>\n",
       "      <td>The density of the observations is .</td>\n",
       "      <td>Movielens dataset</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>88212</td>\n",
       "      <td>380b2c78d21ae6c43d418b6f0cb0222d5293d345</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CoNLL 2009 shared task</td>\n",
       "      <td>In order to be able to compare with similar gr...</td>\n",
       "      <td>Flattening the sampling distribution ( ) is es...</td>\n",
       "      <td>Since some of the treebanks contain nonproject...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>34201</td>\n",
       "      <td>12e20e4ea572dbe476fd894c5c9a9930cf250dd2</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Stanford Question Answering dataset</td>\n",
       "      <td>Recently , released the Stanford Question Answ...</td>\n",
       "      <td>However , these datasets are either not large ...</td>\n",
       "      <td>Moreover , this dataset consists 100 , 000 + q...</td>\n",
       "      <td>Machine Reading Comprehension Dataset .</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>94393</td>\n",
       "      <td>3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>MPII Human Pose Multi - Person</td>\n",
       "      <td>Extensive experiments on benchmarks MPII Human...</td>\n",
       "      <td>GPN is implemented with the Hourglass architec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generative Partition Networks for Multi - Pers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>67266</td>\n",
       "      <td>2aec8d465e9a74c27f956ed1136f3e8a3ba0a833</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>Real Noisy Images</td>\n",
       "      <td>subsection : Experiments on Real Noisy Images</td>\n",
       "      <td>When the ground - truth noise level is unknown...</td>\n",
       "      <td>In this subsection , real noisy images are use...</td>\n",
       "      <td>Experiments on Real Noisy Images</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>85387</td>\n",
       "      <td>36911f5fc4f4eb1221f832114946de4773cf78e6</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>English Wikipedia paragraphs</td>\n",
       "      <td>The corpus consists of all of the English Wiki...</td>\n",
       "      <td>The relevant passages are the paragraphs withi...</td>\n",
       "      <td>The released dataset has five predefined folds...</td>\n",
       "      <td>TREC - CAR</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>91451</td>\n",
       "      <td>3b1b94441010615195a5c404409ce2416860508c</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>scale knowledge base</td>\n",
       "      <td>The third input source is the textual knowledg...</td>\n",
       "      <td>Average - pooling is applied over the 5 hidden...</td>\n",
       "      <td>More details are shown in the following section .</td>\n",
       "      <td>A VQA Model with External Knowledge</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>58647</td>\n",
       "      <td>23dcfda130aada27c158c0b5f394cac489c9c795</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>AFLW dataset</td>\n",
       "      <td>presents an in - depth study of relatively sha...</td>\n",
       "      <td>Also recently , work has developed on estimati...</td>\n",
       "      <td>In KEPLER the authors present a modified Googl...</td>\n",
       "      <td>RELATED WORK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>33212</td>\n",
       "      <td>1109b663453e78a59e4f66446d71720ac58cec25</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ImageNet 2012 training set</td>\n",
       "      <td>We train the network on the ImageNet 2012 trai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our model uses the same fixed input size appro...</td>\n",
       "      <td>Model Design and Training</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>97666</td>\n",
       "      <td>40193e7ba0fbd7153a1fe15e95563463b67c71f3</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>in - the - wild \" images</td>\n",
       "      <td>This is mainly because 2DASL involves a number...</td>\n",
       "      <td>Although all the state - of - the - art method...</td>\n",
       "      <td>For fair comparison , we adopt the normalized ...</td>\n",
       "      <td>Dense face alignment</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>98443</td>\n",
       "      <td>40b4596a0ae4f4ff065f3f13f36db39543e50068</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Cityscapes datset</td>\n",
       "      <td>We conduct extensive experiments by using the ...</td>\n",
       "      <td>The two modules above can be easily integrated...</td>\n",
       "      <td>Our proposed method achieves a new state - of ...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>101913</td>\n",
       "      <td>42764b57d0794b63487a295ce8c07eeb6961477e</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2012 trainval</td>\n",
       "      <td>Finally , we train the MNC model on the union ...</td>\n",
       "      <td>Using these box layers ’ outputs ( box coordin...</td>\n",
       "      <td>As the 2007 set has no mask - level annotation...</td>\n",
       "      <td>Experiments on PASCAL VOC 2012</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>101906</td>\n",
       "      <td>42764b57d0794b63487a295ce8c07eeb6961477e</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2007 set</td>\n",
       "      <td>We note that our result is obtained with fewer...</td>\n",
       "      <td>Table [ reference ] shows that our result ( 70...</td>\n",
       "      <td>This experiment shows the effectiveness of our...</td>\n",
       "      <td>Experiments on PASCAL VOC 2012</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>75840</td>\n",
       "      <td>302207c149bdf7beb6e46e4d4afbd2fa9ac02c64</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Radboud Faces Database</td>\n",
       "      <td>The Radboud Faces Database ( RaFD ) consists o...</td>\n",
       "      <td>RaFD .</td>\n",
       "      <td>Each participant makes eight facial expression...</td>\n",
       "      <td>Datasets</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1745</td>\n",
       "      <td>01125e3c68edb420b8d884ff53fb38d9fbe4f2b8</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Florence</td>\n",
       "      <td>To measure the effect of facial expressions on...</td>\n",
       "      <td>Certain expressions are usually considered har...</td>\n",
       "      <td>This kind of extreme acted facial expressions ...</td>\n",
       "      <td>Ablation studies</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>53827</td>\n",
       "      <td>2116b2eaaece4af9c28c32af2728f3d49b792cf9</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CIFAR - 10 without dropout</td>\n",
       "      <td>Our model for CIFAR - 10 without dropout is a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pooling layers follow all three .</td>\n",
       "      <td>Models for CIFAR - 10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>65163</td>\n",
       "      <td>29c19276b8fff231717c3e342cb24144d2b77726</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>They use a convolutional neural network ( CNN ...</td>\n",
       "      <td>For POS tagging , santos : zadrozny:2014 were ...</td>\n",
       "      <td>ling :</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>12281</td>\n",
       "      <td>06c5b86b638b2f3572b9cdd9ef0be4740b16781b</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SST datasets</td>\n",
       "      <td>Our model has robust superiority over competit...</td>\n",
       "      <td>We summarize the experimental results in Table...</td>\n",
       "      <td>First , our model brings a substantial improve...</td>\n",
       "      <td>Experiment Results</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2046</td>\n",
       "      <td>0116899fce00ffa4afee08b505300bb3968faf9f</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>COCO Dataset</td>\n",
       "      <td>Note that the COCO Dataset is not a long text ...</td>\n",
       "      <td>We take the image captions as the text to gene...</td>\n",
       "      <td>Thus we apply some preprocessing on the dataset .</td>\n",
       "      <td>Middle Text Generation : COCO Image Captions</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6538</td>\n",
       "      <td>03a5b2aac53443e6078f0f63b35d4f95d6d54c5d</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BSD datasets</td>\n",
       "      <td>’s run time on Set14It should be noted our res...</td>\n",
       "      <td>In this section , we evaluated our best model</td>\n",
       "      <td>However , the use of Set14 on a single CPU cor...</td>\n",
       "      <td>Run time evaluations</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>33375</td>\n",
       "      <td>1109b663453e78a59e4f66446d71720ac58cec25</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ImageNet 2012</td>\n",
       "      <td>These were never addressed in and thus we are ...</td>\n",
       "      <td>A second important contribution of our paper i...</td>\n",
       "      <td>The scheme we propose involves substantial mod...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>91785</td>\n",
       "      <td>3b1d8eb163ffff598c2faa0d9d7cf933857a359f</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>conf / emnlp / BowmanAPM15</td>\n",
       "      <td>More recently the availability of much larger ...</td>\n",
       "      <td>Early research on natural language inference a...</td>\n",
       "      <td>These models mainly fall into two types of app...</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>97737</td>\n",
       "      <td>40193e7ba0fbd7153a1fe15e95563463b67c71f3</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>RGB face images</td>\n",
       "      <td>In this section , we perform ablation study on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>( 4 ) 2DASL ( cyc + sc ) , which contains both...</td>\n",
       "      <td>Ablation study</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>81381</td>\n",
       "      <td>34f63959ea4a13a05948274a1558c6854a051150</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SST - 2</td>\n",
       "      <td>paragraph : SST - 2</td>\n",
       "      <td>It uses Matthews correlation coefficient as th...</td>\n",
       "      <td>The Stanford Sentiment Treebank is to determin...</td>\n",
       "      <td>SST - 2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>83390</td>\n",
       "      <td>35c1668dc64d24a28c6041978e5fcca754eb2f4b</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>sentence - aligned subtitles of TED</td>\n",
       "      <td>The corpus consists of sentence - aligned subt...</td>\n",
       "      <td>We use data from the German - English machine ...</td>\n",
       "      <td>We pre - process the training data using the t...</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>92532</td>\n",
       "      <td>3c1d781f2dab8da12e3cb0e4d7abfb440a340a09</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>multinli</td>\n",
       "      <td>This design is inspired by the reported annota...</td>\n",
       "      <td>To qualitatively evaluate the performance of o...</td>\n",
       "      <td>The specifications of our annotation tags are ...</td>\n",
       "      <td>Analysis</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>8745</td>\n",
       "      <td>052443e1709c0f7d3432cca7c451534eea76b7ca</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>B100</td>\n",
       "      <td>We validate our seven techniques on standard S...</td>\n",
       "      <td>: 1 ) augmentation of data , 2 ) use of large ...</td>\n",
       "      <td>The techniques are widely applicable and requi...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>107752</td>\n",
       "      <td>46018a894d533813d67322827ca51f78aed6d59e</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BRATS 2015 challenge</td>\n",
       "      <td>Using our best performing method , we took par...</td>\n",
       "      <td>However , their method is very similar to the ...</td>\n",
       "      <td>The BRATS 2015 training dataset comprises of 2...</td>\n",
       "      <td>Cascaded architectures</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>5346</td>\n",
       "      <td>02e85d62fbd8249a046d00ac10e39546511b2a51</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BRATS )</td>\n",
       "      <td>For brain tumors , we evaluate our system on t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The training set consists of 220 cases with hi...</td>\n",
       "      <td>Material and Pre - Processing</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>99476</td>\n",
       "      <td>41232a69c0f8d4b993e6c6e00b16c223442c962f</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Gigaword benchmark</td>\n",
       "      <td>Experiments on the Gigaword benchmark demonstr...</td>\n",
       "      <td>Then , we propose the dual - attention s2s fra...</td>\n",
       "      <td>In addition , since the fact descriptions ofte...</td>\n",
       "      <td>Conclusion and Future Work</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5378</td>\n",
       "      <td>02e85d62fbd8249a046d00ac10e39546511b2a51</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BRATS test data</td>\n",
       "      <td>Table [ reference ] shows the results of our m...</td>\n",
       "      <td>DeepMedic behaves very well in preserving the ...</td>\n",
       "      <td>Results of other submissions are not accessible .</td>\n",
       "      <td>Results</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>81359</td>\n",
       "      <td>34f63959ea4a13a05948274a1558c6854a051150</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SciTail</td>\n",
       "      <td>We evaluate the proposed MT - DNN on three pop...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We compare MT - DNN with existing state - of -...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>96453</td>\n",
       "      <td>3f45d73a7b8d10a59a68688c11950e003f4852fc</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Retinex images</td>\n",
       "      <td>With the Retinex images , we apply the HSV col...</td>\n",
       "      <td>This makes person re - identification easier t...</td>\n",
       "      <td>In addition to color description , we also app...</td>\n",
       "      <td>Dealing with Illumination Variations</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>79874</td>\n",
       "      <td>33a8d0a35390fde736744d4a0dd20dff7961c777</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>IMDB - MULTI</td>\n",
       "      <td>In second round , we used social network datas...</td>\n",
       "      <td>In first round , we used bioinformatics datase...</td>\n",
       "      <td>For other datasets details can be found in .</td>\n",
       "      <td>Experiment and Results</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>11408</td>\n",
       "      <td>063ad0349f05c8aacbbb653ffcf01047a293fa30</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SentiHood dataset</td>\n",
       "      <td>As a testbed for this task , we introduce the ...</td>\n",
       "      <td>In particular , we identify the sentiment towa...</td>\n",
       "      <td>In this context units of text often mention se...</td>\n",
       "      <td>SentiHood : Targeted Aspect Based Sentiment An...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>73205</td>\n",
       "      <td>2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>Conflict</td>\n",
       "      <td>The datasets are summarized in Table [ referen...</td>\n",
       "      <td>We evaluate our proposed autoencoder models on...</td>\n",
       "      <td>{ Protein , Metabolic , Conflict , PowerGrid }...</td>\n",
       "      <td>Datasets and Baselines</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>86582</td>\n",
       "      <td>36c3972569a6949ecca90bfa6f8e99883e092845</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>VQA 2.0 test - std split</td>\n",
       "      <td>Their performance reached 70.34 % on VQA 2.0 t...</td>\n",
       "      <td>Multi - modal fusion is done through a simple ...</td>\n",
       "      <td>For presentation clarity , we present our prop...</td>\n",
       "      <td>Bottom - Up and Top - Down Attention</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>91600</td>\n",
       "      <td>3b1b94441010615195a5c404409ce2416860508c</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>VQA test split</td>\n",
       "      <td>The human ground truth answers for the actual ...</td>\n",
       "      <td>We randomly choose 5000 images from the valida...</td>\n",
       "      <td>Hence , we also apply our final model on a tes...</td>\n",
       "      <td>Evaluation on Visual Question Answering</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>40862</td>\n",
       "      <td>16cd50316e41cbb1d9dfeafeb524b31654cef37a</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>vocalized - noise</td>\n",
       "      <td>The phonetic inventory includes special models...</td>\n",
       "      <td>Additionally , we have trained several models ...</td>\n",
       "      <td>We use a 30k - vocabulary derived from the mos...</td>\n",
       "      <td>Acoustic Model Details</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2882</td>\n",
       "      <td>01959ef569f74c286956024866c1d107099199f7</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>VQA train + val dataset</td>\n",
       "      <td>: We created a filtered version of the VQA tra...</td>\n",
       "      <td>Filtered Dataset</td>\n",
       "      <td>Also , we keep only those questions for which ...</td>\n",
       "      <td>4 ) K = 2000 :</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>96952</td>\n",
       "      <td>3febb2bed8865945e7fddc99efd791887bb7e14f</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>domain specific data</td>\n",
       "      <td>In some cases , fine tuning the biLM on domain...</td>\n",
       "      <td>Once pretrained , the biLM can compute represe...</td>\n",
       "      <td>This can be seen as a type of domain transfer ...</td>\n",
       "      <td>Pre - trained bidirectional language model arc...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>71225</td>\n",
       "      <td>2d83dbf4c8eabc6bdef3326c4a30d5f33ffc944e</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>MS - COCO dataset</td>\n",
       "      <td>The images come from the MS - COCO dataset , 1...</td>\n",
       "      <td>Table [ reference ] shows that Other type has ...</td>\n",
       "      <td>The images are carefully collected to contain ...</td>\n",
       "      <td>Visual QA Dataset</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>77656</td>\n",
       "      <td>31ae4873da19b1e28eca8787a17f49bba08627e5</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>trainval35k ’ set</td>\n",
       "      <td>For the COCO dataset , we use the ‘ trainval35...</td>\n",
       "      <td>MS COCO .</td>\n",
       "      <td>During training the Fast - RCNN , we apply SGD...</td>\n",
       "      <td>Experimental settings</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2314</td>\n",
       "      <td>0171bdeb1c6e333287be655c667cfba5edb89b76</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ImageNet classification dataset</td>\n",
       "      <td>Our neural networks , named ResNeXt ( suggesti...</td>\n",
       "      <td>Experiments demonstrate that increasing cardin...</td>\n",
       "      <td>In particular , a 101 - layer ResNeXt is able ...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>96334</td>\n",
       "      <td>3f45d73a7b8d10a59a68688c11950e003f4852fc</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CUHK Campus</td>\n",
       "      <td>Experiments on four challenging person re - id...</td>\n",
       "      <td>We also present a practical computation method...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Person Re - identification by Local Maximal Oc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>41752</td>\n",
       "      <td>1751668492bac56f0ae2b6410417515ab3215945</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>PTB - WSJ dataset</td>\n",
       "      <td>paragraph : PTB - WSJ dataset .</td>\n",
       "      <td>subsection : Results</td>\n",
       "      <td>Table [ reference ] shows the POS tagging resu...</td>\n",
       "      <td>PTB - WSJ dataset .</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>84542</td>\n",
       "      <td>360cfa09b2f7c8e10b1831d899c5a51aefa1883e</td>\n",
       "      <td>Material</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CHiME</td>\n",
       "      <td>More information on CHiME data can be found in .</td>\n",
       "      <td>The experiments reported in this paper are bas...</td>\n",
       "      <td>To evaluate the proposed model on a larger sca...</td>\n",
       "      <td>Corpora and tasks</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>102241</td>\n",
       "      <td>42e80c73867bff9eaff6beceb8730fc1276283b9</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WMT 2014</td>\n",
       "      <td>Our experiments show the effectiveness of our ...</td>\n",
       "      <td>In addition to that , we use our improved SMT ...</td>\n",
       "      <td>In the future , we would like to explore learn...</td>\n",
       "      <td>Conclusions and future work</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>40860</td>\n",
       "      <td>16cd50316e41cbb1d9dfeafeb524b31654cef37a</td>\n",
       "      <td>Material</td>\n",
       "      <td>na</td>\n",
       "      <td>kein dataset</td>\n",
       "      <td>na</td>\n",
       "      <td>phonetic inventory</td>\n",
       "      <td>The phonetic inventory includes special models...</td>\n",
       "      <td>Additionally , we have trained several models ...</td>\n",
       "      <td>We use a 30k - vocabulary derived from the mos...</td>\n",
       "      <td>Acoustic Model Details</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>102179</td>\n",
       "      <td>42e80c73867bff9eaff6beceb8730fc1276283b9</td>\n",
       "      <td>Material</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>German - English</td>\n",
       "      <td>In order to make our experiments comparable to...</td>\n",
       "      <td>SMT + NMT</td>\n",
       "      <td>More concretely , our training data consists o...</td>\n",
       "      <td>Experiments and results</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column1                                    doc_id  relation used  \\\n",
       "0     26515  0e8753f550350e53824358ca3f0f8cfd2f2dc2f7  Material    1   \n",
       "1     88212  380b2c78d21ae6c43d418b6f0cb0222d5293d345  Material    1   \n",
       "2     34201  12e20e4ea572dbe476fd894c5c9a9930cf250dd2  Material    0   \n",
       "3     94393  3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a  Material    1   \n",
       "4     67266  2aec8d465e9a74c27f956ed1136f3e8a3ba0a833  Material   na   \n",
       "5     85387  36911f5fc4f4eb1221f832114946de4773cf78e6  Material   na   \n",
       "6     91451  3b1b94441010615195a5c404409ce2416860508c  Material   na   \n",
       "7     58647  23dcfda130aada27c158c0b5f394cac489c9c795  Material    0   \n",
       "8     33212  1109b663453e78a59e4f66446d71720ac58cec25  Material    1   \n",
       "9     97666  40193e7ba0fbd7153a1fe15e95563463b67c71f3  Material   na   \n",
       "10    98443  40b4596a0ae4f4ff065f3f13f36db39543e50068  Material    1   \n",
       "11   101913  42764b57d0794b63487a295ce8c07eeb6961477e  Material    1   \n",
       "12   101906  42764b57d0794b63487a295ce8c07eeb6961477e  Material    0   \n",
       "13    75840  302207c149bdf7beb6e46e4d4afbd2fa9ac02c64  Material    1   \n",
       "14     1745  01125e3c68edb420b8d884ff53fb38d9fbe4f2b8  Material    0   \n",
       "15    53827  2116b2eaaece4af9c28c32af2728f3d49b792cf9  Material    1   \n",
       "16    65163  29c19276b8fff231717c3e342cb24144d2b77726  Material    0   \n",
       "17    12281  06c5b86b638b2f3572b9cdd9ef0be4740b16781b  Material    1   \n",
       "18     2046  0116899fce00ffa4afee08b505300bb3968faf9f  Material    1   \n",
       "19     6538  03a5b2aac53443e6078f0f63b35d4f95d6d54c5d  Material    1   \n",
       "20    33375  1109b663453e78a59e4f66446d71720ac58cec25  Material    1   \n",
       "21    91785  3b1d8eb163ffff598c2faa0d9d7cf933857a359f  Material    0   \n",
       "22    97737  40193e7ba0fbd7153a1fe15e95563463b67c71f3  Material   na   \n",
       "23    81381  34f63959ea4a13a05948274a1558c6854a051150  Material    1   \n",
       "24    83390  35c1668dc64d24a28c6041978e5fcca754eb2f4b  Material   na   \n",
       "25    92532  3c1d781f2dab8da12e3cb0e4d7abfb440a340a09  Material    1   \n",
       "26     8745  052443e1709c0f7d3432cca7c451534eea76b7ca  Material    1   \n",
       "27   107752  46018a894d533813d67322827ca51f78aed6d59e  Material    1   \n",
       "28     5346  02e85d62fbd8249a046d00ac10e39546511b2a51  Material    1   \n",
       "29    99476  41232a69c0f8d4b993e6c6e00b16c223442c962f  Material    1   \n",
       "30     5378  02e85d62fbd8249a046d00ac10e39546511b2a51  Material    1   \n",
       "31    81359  34f63959ea4a13a05948274a1558c6854a051150  Material    1   \n",
       "32    96453  3f45d73a7b8d10a59a68688c11950e003f4852fc  Material    1   \n",
       "33    79874  33a8d0a35390fde736744d4a0dd20dff7961c777  Material    1   \n",
       "34    11408  063ad0349f05c8aacbbb653ffcf01047a293fa30  Material    1   \n",
       "35    73205  2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79  Material   na   \n",
       "36    86582  36c3972569a6949ecca90bfa6f8e99883e092845  Material    0   \n",
       "37    91600  3b1b94441010615195a5c404409ce2416860508c  Material    1   \n",
       "38    40862  16cd50316e41cbb1d9dfeafeb524b31654cef37a  Material   na   \n",
       "39     2882  01959ef569f74c286956024866c1d107099199f7  Material    1   \n",
       "40    96952  3febb2bed8865945e7fddc99efd791887bb7e14f  Material   na   \n",
       "41    71225  2d83dbf4c8eabc6bdef3326c4a30d5f33ffc944e  Material    0   \n",
       "42    77656  31ae4873da19b1e28eca8787a17f49bba08627e5  Material    1   \n",
       "43     2314  0171bdeb1c6e333287be655c667cfba5edb89b76  Material    1   \n",
       "44    96334  3f45d73a7b8d10a59a68688c11950e003f4852fc  Material    1   \n",
       "45    41752  1751668492bac56f0ae2b6410417515ab3215945  Material    1   \n",
       "46    84542  360cfa09b2f7c8e10b1831d899c5a51aefa1883e  Material    0   \n",
       "47   102241  42e80c73867bff9eaff6beceb8730fc1276283b9  Material    1   \n",
       "48    40860  16cd50316e41cbb1d9dfeafeb524b31654cef37a  Material   na   \n",
       "49   102179  42e80c73867bff9eaff6beceb8730fc1276283b9  Material    1   \n",
       "\n",
       "       anmerkung alex                                  ner  \\\n",
       "0            NaN    1               MovieLens 10 M dataset   \n",
       "1            NaN    1               CoNLL 2009 shared task   \n",
       "2            NaN    0  Stanford Question Answering dataset   \n",
       "3            NaN    0       MPII Human Pose Multi - Person   \n",
       "4   kein dataset   na                    Real Noisy Images   \n",
       "5   kein dataset    1         English Wikipedia paragraphs   \n",
       "6   kein dataset   na                 scale knowledge base   \n",
       "7            NaN    0                         AFLW dataset   \n",
       "8            NaN    1           ImageNet 2012 training set   \n",
       "9   kein dataset   na             in - the - wild \" images   \n",
       "10           NaN    1                    Cityscapes datset   \n",
       "11           NaN    1                        2012 trainval   \n",
       "12           NaN    0                             2007 set   \n",
       "13           NaN    0               Radboud Faces Database   \n",
       "14           NaN    1                             Florence   \n",
       "15           NaN    1           CIFAR - 10 without dropout   \n",
       "16           NaN    0                           Portuguese   \n",
       "17           NaN    1                         SST datasets   \n",
       "18           NaN    1                         COCO Dataset   \n",
       "19           NaN    1                         BSD datasets   \n",
       "20           NaN    1                        ImageNet 2012   \n",
       "21           NaN    0           conf / emnlp / BowmanAPM15   \n",
       "22  kein dataset   na                      RGB face images   \n",
       "23           NaN    0                              SST - 2   \n",
       "24  kein dataset   na  sentence - aligned subtitles of TED   \n",
       "25           NaN    0                             multinli   \n",
       "26           NaN    1                                 B100   \n",
       "27           NaN    1                 BRATS 2015 challenge   \n",
       "28           NaN    1                              BRATS )   \n",
       "29           NaN    1                   Gigaword benchmark   \n",
       "30           NaN    1                      BRATS test data   \n",
       "31           NaN    1                              SciTail   \n",
       "32           NaN    1                       Retinex images   \n",
       "33           NaN    1                         IMDB - MULTI   \n",
       "34           NaN    1                    SentiHood dataset   \n",
       "35  kein dataset   na                             Conflict   \n",
       "36           NaN    0             VQA 2.0 test - std split   \n",
       "37           NaN    1                       VQA test split   \n",
       "38  kein dataset   na                    vocalized - noise   \n",
       "39           NaN    1              VQA train + val dataset   \n",
       "40  kein dataset   na                 domain specific data   \n",
       "41           NaN    0                    MS - COCO dataset   \n",
       "42           NaN    1                    trainval35k ’ set   \n",
       "43           NaN    1      ImageNet classification dataset   \n",
       "44           NaN    1                          CUHK Campus   \n",
       "45           NaN    1                    PTB - WSJ dataset   \n",
       "46           NaN    0                                CHiME   \n",
       "47           NaN    1                             WMT 2014   \n",
       "48  kein dataset   na                   phonetic inventory   \n",
       "49           NaN    1                     German - English   \n",
       "\n",
       "                                             sentence  \\\n",
       "0   We work with the widely used MovieLens 10 M da...   \n",
       "1   In order to be able to compare with similar gr...   \n",
       "2   Recently , released the Stanford Question Answ...   \n",
       "3   Extensive experiments on benchmarks MPII Human...   \n",
       "4       subsection : Experiments on Real Noisy Images   \n",
       "5   The corpus consists of all of the English Wiki...   \n",
       "6   The third input source is the textual knowledg...   \n",
       "7   presents an in - depth study of relatively sha...   \n",
       "8   We train the network on the ImageNet 2012 trai...   \n",
       "9   This is mainly because 2DASL involves a number...   \n",
       "10  We conduct extensive experiments by using the ...   \n",
       "11  Finally , we train the MNC model on the union ...   \n",
       "12  We note that our result is obtained with fewer...   \n",
       "13  The Radboud Faces Database ( RaFD ) consists o...   \n",
       "14  To measure the effect of facial expressions on...   \n",
       "15  Our model for CIFAR - 10 without dropout is a ...   \n",
       "16  They use a convolutional neural network ( CNN ...   \n",
       "17  Our model has robust superiority over competit...   \n",
       "18  Note that the COCO Dataset is not a long text ...   \n",
       "19  ’s run time on Set14It should be noted our res...   \n",
       "20  These were never addressed in and thus we are ...   \n",
       "21  More recently the availability of much larger ...   \n",
       "22  In this section , we perform ablation study on...   \n",
       "23                                paragraph : SST - 2   \n",
       "24  The corpus consists of sentence - aligned subt...   \n",
       "25  This design is inspired by the reported annota...   \n",
       "26  We validate our seven techniques on standard S...   \n",
       "27  Using our best performing method , we took par...   \n",
       "28  For brain tumors , we evaluate our system on t...   \n",
       "29  Experiments on the Gigaword benchmark demonstr...   \n",
       "30  Table [ reference ] shows the results of our m...   \n",
       "31  We evaluate the proposed MT - DNN on three pop...   \n",
       "32  With the Retinex images , we apply the HSV col...   \n",
       "33  In second round , we used social network datas...   \n",
       "34  As a testbed for this task , we introduce the ...   \n",
       "35  The datasets are summarized in Table [ referen...   \n",
       "36  Their performance reached 70.34 % on VQA 2.0 t...   \n",
       "37  The human ground truth answers for the actual ...   \n",
       "38  The phonetic inventory includes special models...   \n",
       "39  : We created a filtered version of the VQA tra...   \n",
       "40  In some cases , fine tuning the biLM on domain...   \n",
       "41  The images come from the MS - COCO dataset , 1...   \n",
       "42  For the COCO dataset , we use the ‘ trainval35...   \n",
       "43  Our neural networks , named ResNeXt ( suggesti...   \n",
       "44  Experiments on four challenging person re - id...   \n",
       "45                    paragraph : PTB - WSJ dataset .   \n",
       "46   More information on CHiME data can be found in .   \n",
       "47  Our experiments show the effectiveness of our ...   \n",
       "48  The phonetic inventory includes special models...   \n",
       "49  In order to make our experiments comparable to...   \n",
       "\n",
       "                                         pre_sentence  \\\n",
       "0   In this section , we report experiments on rea...   \n",
       "1   Flattening the sampling distribution ( ) is es...   \n",
       "2   However , these datasets are either not large ...   \n",
       "3   GPN is implemented with the Hourglass architec...   \n",
       "4   When the ground - truth noise level is unknown...   \n",
       "5   The relevant passages are the paragraphs withi...   \n",
       "6   Average - pooling is applied over the 5 hidden...   \n",
       "7   Also recently , work has developed on estimati...   \n",
       "8                                                 NaN   \n",
       "9   Although all the state - of - the - art method...   \n",
       "10  The two modules above can be easily integrated...   \n",
       "11  Using these box layers ’ outputs ( box coordin...   \n",
       "12  Table [ reference ] shows that our result ( 70...   \n",
       "13                                             RaFD .   \n",
       "14  Certain expressions are usually considered har...   \n",
       "15                                                NaN   \n",
       "16  For POS tagging , santos : zadrozny:2014 were ...   \n",
       "17  We summarize the experimental results in Table...   \n",
       "18  We take the image captions as the text to gene...   \n",
       "19      In this section , we evaluated our best model   \n",
       "20  A second important contribution of our paper i...   \n",
       "21  Early research on natural language inference a...   \n",
       "22                                                NaN   \n",
       "23  It uses Matthews correlation coefficient as th...   \n",
       "24  We use data from the German - English machine ...   \n",
       "25  To qualitatively evaluate the performance of o...   \n",
       "26  : 1 ) augmentation of data , 2 ) use of large ...   \n",
       "27  However , their method is very similar to the ...   \n",
       "28                                                NaN   \n",
       "29  Then , we propose the dual - attention s2s fra...   \n",
       "30  DeepMedic behaves very well in preserving the ...   \n",
       "31                                                NaN   \n",
       "32  This makes person re - identification easier t...   \n",
       "33  In first round , we used bioinformatics datase...   \n",
       "34  In particular , we identify the sentiment towa...   \n",
       "35  We evaluate our proposed autoencoder models on...   \n",
       "36  Multi - modal fusion is done through a simple ...   \n",
       "37  We randomly choose 5000 images from the valida...   \n",
       "38  Additionally , we have trained several models ...   \n",
       "39                                   Filtered Dataset   \n",
       "40  Once pretrained , the biLM can compute represe...   \n",
       "41  Table [ reference ] shows that Other type has ...   \n",
       "42                                          MS COCO .   \n",
       "43  Experiments demonstrate that increasing cardin...   \n",
       "44  We also present a practical computation method...   \n",
       "45                               subsection : Results   \n",
       "46  The experiments reported in this paper are bas...   \n",
       "47  In addition to that , we use our improved SMT ...   \n",
       "48  Additionally , we have trained several models ...   \n",
       "49                                          SMT + NMT   \n",
       "\n",
       "                                        post_sentence  \\\n",
       "0                The density of the observations is .   \n",
       "1   Since some of the treebanks contain nonproject...   \n",
       "2   Moreover , this dataset consists 100 , 000 + q...   \n",
       "3                                                 NaN   \n",
       "4   In this subsection , real noisy images are use...   \n",
       "5   The released dataset has five predefined folds...   \n",
       "6   More details are shown in the following section .   \n",
       "7   In KEPLER the authors present a modified Googl...   \n",
       "8   Our model uses the same fixed input size appro...   \n",
       "9   For fair comparison , we adopt the normalized ...   \n",
       "10  Our proposed method achieves a new state - of ...   \n",
       "11  As the 2007 set has no mask - level annotation...   \n",
       "12  This experiment shows the effectiveness of our...   \n",
       "13  Each participant makes eight facial expression...   \n",
       "14  This kind of extreme acted facial expressions ...   \n",
       "15                  Pooling layers follow all three .   \n",
       "16                                             ling :   \n",
       "17  First , our model brings a substantial improve...   \n",
       "18  Thus we apply some preprocessing on the dataset .   \n",
       "19  However , the use of Set14 on a single CPU cor...   \n",
       "20  The scheme we propose involves substantial mod...   \n",
       "21  These models mainly fall into two types of app...   \n",
       "22  ( 4 ) 2DASL ( cyc + sc ) , which contains both...   \n",
       "23  The Stanford Sentiment Treebank is to determin...   \n",
       "24  We pre - process the training data using the t...   \n",
       "25  The specifications of our annotation tags are ...   \n",
       "26  The techniques are widely applicable and requi...   \n",
       "27  The BRATS 2015 training dataset comprises of 2...   \n",
       "28  The training set consists of 220 cases with hi...   \n",
       "29  In addition , since the fact descriptions ofte...   \n",
       "30  Results of other submissions are not accessible .   \n",
       "31  We compare MT - DNN with existing state - of -...   \n",
       "32  In addition to color description , we also app...   \n",
       "33       For other datasets details can be found in .   \n",
       "34  In this context units of text often mention se...   \n",
       "35  { Protein , Metabolic , Conflict , PowerGrid }...   \n",
       "36  For presentation clarity , we present our prop...   \n",
       "37  Hence , we also apply our final model on a tes...   \n",
       "38  We use a 30k - vocabulary derived from the mos...   \n",
       "39  Also , we keep only those questions for which ...   \n",
       "40  This can be seen as a type of domain transfer ...   \n",
       "41  The images are carefully collected to contain ...   \n",
       "42  During training the Fast - RCNN , we apply SGD...   \n",
       "43  In particular , a 101 - layer ResNeXt is able ...   \n",
       "44                                                NaN   \n",
       "45  Table [ reference ] shows the POS tagging resu...   \n",
       "46  To evaluate the proposed model on a larger sca...   \n",
       "47  In the future , we would like to explore learn...   \n",
       "48  We use a 30k - vocabulary derived from the mos...   \n",
       "49  More concretely , our training data consists o...   \n",
       "\n",
       "                                         section_name  section_index  \n",
       "0                                   Movielens dataset              9  \n",
       "1                                         Experiments              7  \n",
       "2             Machine Reading Comprehension Dataset .             17  \n",
       "3   Generative Partition Networks for Multi - Pers...              0  \n",
       "4                    Experiments on Real Noisy Images             18  \n",
       "5                                          TREC - CAR              8  \n",
       "6                 A VQA Model with External Knowledge              9  \n",
       "7                                        RELATED WORK              2  \n",
       "8                           Model Design and Training              4  \n",
       "9                                Dense face alignment             15  \n",
       "10                                       Introduction              1  \n",
       "11                     Experiments on PASCAL VOC 2012             11  \n",
       "12                     Experiments on PASCAL VOC 2012             11  \n",
       "13                                           Datasets              9  \n",
       "14                                   Ablation studies             11  \n",
       "15                              Models for CIFAR - 10             20  \n",
       "16                                       Related Work             11  \n",
       "17                                 Experiment Results             10  \n",
       "18       Middle Text Generation : COCO Image Captions             13  \n",
       "19                               Run time evaluations             14  \n",
       "20                                         Discussion             15  \n",
       "21                                       Related Work              2  \n",
       "22                                     Ablation study             17  \n",
       "23                                            SST - 2             18  \n",
       "24                                Machine Translation             13  \n",
       "25                                           Analysis             15  \n",
       "26                                           Abstract              1  \n",
       "27                             Cascaded architectures             15  \n",
       "28                      Material and Pre - Processing             22  \n",
       "29                         Conclusion and Future Work             17  \n",
       "30                                            Results             24  \n",
       "31                                        Experiments             15  \n",
       "32               Dealing with Illumination Variations              4  \n",
       "33                             Experiment and Results             12  \n",
       "34  SentiHood : Targeted Aspect Based Sentiment An...              0  \n",
       "35                             Datasets and Baselines              7  \n",
       "36               Bottom - Up and Top - Down Attention              2  \n",
       "37            Evaluation on Visual Question Answering             17  \n",
       "38                             Acoustic Model Details             21  \n",
       "39                                     4 ) K = 2000 :             16  \n",
       "40  Pre - trained bidirectional language model arc...              7  \n",
       "41                                  Visual QA Dataset              9  \n",
       "42                              Experimental settings             11  \n",
       "43                                       Introduction              2  \n",
       "44  Person Re - identification by Local Maximal Oc...              0  \n",
       "45                                PTB - WSJ dataset .             20  \n",
       "46                                  Corpora and tasks              9  \n",
       "47                        Conclusions and future work             13  \n",
       "48                             Acoustic Model Details             21  \n",
       "49                            Experiments and results              9  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>used_alex</th>\n",
       "      <th>used_felix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  used_alex used_felix\n",
       "0         1          1\n",
       "1         1          1\n",
       "2         1          1\n",
       "3        na         na\n",
       "4         1          1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_annotation = df_1[['used_1', 'used_2']][0:50]\n",
    "df_1_annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>used_alex</th>\n",
       "      <th>used_felix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  used_alex used_felix\n",
       "0         1          1\n",
       "1         1          1\n",
       "2         0          0\n",
       "3         0          1\n",
       "4        na         na"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2_annotation = df_2[['used_1', 'used_2']][0:50]\n",
    "df_2_annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>used_alex</th>\n",
       "      <th>used_felix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  used_alex used_felix\n",
       "0         1          1\n",
       "1         1          1\n",
       "2         1          1\n",
       "3        na         na\n",
       "4         1          1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = df_1_annotation.append(df_2_annotation)\n",
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation[annotation['used_1'] == \"na\"] = 2\n",
    "annotation[annotation['used_2'] == \"na\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>used_alex</th>\n",
       "      <th>used_felix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   used_alex  used_felix\n",
       "0          1           1\n",
       "1          1           1\n",
       "2          1           1\n",
       "3          2           2\n",
       "4          1           1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotation = annotation.loc[annotation['used_alex'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9087258123402702"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.cohen_kappa_score(annotation['used_1'], annotation['used_2'])\n",
    "# kappa > 0.8: perfect\n",
    "#see https://www.cs.brandeis.edu/~cs140b/CS140b_slides/CS140_Lect_7_InterAnnotatorAgreement.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
;doc_id;relation;used;used_Felix;Anmerkung;ner;sentence;pre_sentence;post_sentence;section_name;section_index
68054;2bb9f0768fac9622a0be446df69daf75a954d5ac;Material;1;;1;LDC2014T12;1 ) JAMR flanigan - EtAl:2014:P14 - 1 , flanigan - EtAl:2016:SemEval and 2 ) CAMR and evaluate the final performances of the AMR parsers on both the newswire proportion and the entire dataset of LDC2014T12 .;For the extrinsic evaluation , we plug our alignment into two open - sourced AMR parsers :;We use the configuration in flanigan - EtAl:2016:SemEval for JAMR and the configuration in wang - xue - pradhan:2015:ACL - IJCNLP without semantic role labeling ( SRL ) features for CAMR .;Settings;17
9432;05357b8c05b5bc020e871fc330a88910c3177e4d;Material;1;;1;PASCAL VOC protocol;Average Precision ( AP ) and the mean of AP ( mAP ) is the evaluation metric to test our model on the testing set , which follows the standard PASCAL VOC protocol .;For testing , there are two metrics for evaluation : mAP and CorLoc .;Correct localization ( CorLoc ) is to test our model on the training set measuring the localization accuracy .;Datasets and evaluation measures;8
10344;060ff1aad5619a7d6d6cdfaf8be5da29bff3808c;Material;1;;1;CoNLL - 2012;subsubsection : CoNLL - 2012;We use the pre - trained ELMo models and learn task - specific combinations of the ELMo representations which are provided as input instead of GloVe embeddings to the D & M parser with otherwise default settings .;We follow the CoNLL - 2012 split used by he2018jointly to evaluate our models , which uses the annotations from here but the subset of those documents from the CoNLL - 2012 co - reference split described here pradhan2013towards .;CoNLL - 2012;20
13297;074b6fe0cc6848fb86a6703d1c52074494177c79;Material;na;kein Dataset name? -> na?;?;winter images;The subset of the dataset we use contains 13 classes and consists of 10 , 852 fall images and 7 , 654 winter images .;We use only the front - facing views in the sequences so as to mimic dashcam imagery , and adapt from fall to winter .;To further demonstrate our method ’s applicability to real - world adaptation scenarios , we also evaluate our model in a challenging synthetic - to - real adaptation setting .;Semantic Segmentation Adaptation;6
101655;42764b57d0794b63487a295ce8c07eeb6961477e;Material;1;;1;MS COCO segmentation dataset;We demonstrate excellent accuracy on the challenging MS COCO segmentation dataset using an extremely deep 101 - layer residual net ( ResNet - 101 ) , and also report our 1st - place result in the COCO segmentation track in ILSVRC & COCO 2015 competitions .;Thanks to the end - to - end training and the independence of external modules , the three sub - tasks and the entire system easily benefit from stronger features learned by deeper models .;;Introduction;1
23323;0ca2bd0e40a8f0a57665535ae1c31561370ad183;Material;1;?;1;enwik8;The Hutter Prize Wikipedia ( enwik8 ) dataset [ reference ] contains 205 symbols including XML markups and special characters .;Hutter Prize Wikipedia;We follow the data splits used in [ reference ] where the first 90 M characters are used to train the model , the next 5 M characters for validation , and the remainders for the test set .;Text8;10
65161;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;;0;English;"They use a convolutional neural network ( CNN ; or convnet ) and evaluated their model on English ( PTB ) and Portuguese , showing that the model achieves state - of - the - art performance close to taggers using carefully designed feature templates .";For POS tagging , santos : zadrozny:2014 were the first to propose character - based models .;ling :;Related Work;11
91323;3b1b94441010615195a5c404409ce2416860508c;Material;na;kein Dataset name? -> na?;0;Large - scale Knowledge Bases;Large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia , have been used successfully in several natural language Question Answering ( QA ) systems .;Our framework also exploits both CNN and RNNs , but in contrast to preceding approaches which use only image features extracted from a CNN in answering a question , we employ multiple sources , including image content , generated image captions and mined external knowledge , to feed to an RNN to answer questions .;However , VQA systems exploiting KBs are still relatively rare .;Visual Question Answering;5
47618;1c7e078611c9df412e6eb3a356f31a0da0c1f99c;Material;1;;1;YCB - Video;To thoroughly evaluate our method , we additionally collected a large scale RGB - D video dataset named YCB - Video , which contains 6D poses of 21 objects from the YCB object set in 92 videos with a total of 133 , 827 frames .;RGB - D pose estimation ( we use depth images in the Iterative Closest Point ( ICP ) algorithm for pose refinement ) .;Objects in the dataset exhibit different symmetries and are arranged in various poses and spatial configurations , generating severe occlusions between them .;INTRODUCTION;1
102947;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;1;youtube_faces;We use the Youtube Faces Database from youtube_faces .;Finally , we provide a proof of concept for generating faces of a particular person .;It contains videos of different people .;Youtube Faces;19
67129;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;0;Kodak24 dataset;The Kodak24 dataset consists of 24 center - cropped images of size 500 500 from the original Kodak dataset .;BSD68 dataset .;The McMaster dataset is a widely - used dataset for color demosaicing , which contains 18 cropped images of size 500 500 .;Dataset Generation and Network Training;14
67141;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;?;camera noise;We note that RNI6 and RNI15 cover a variety of real noise types , such as camera noise and JPEG compression noise .;The RNI15 dataset consists of 15 real noisy images .;Since the ground - truth clean images are unavailable for real noisy images , we thus only provide the visual comparisons on these images .;Dataset Generation and Network Training;14
85388;36911f5fc4f4eb1221f832114946de4773cf78e6;Material;1;;1;TREC - CAR;We follow the same procedure described for the MS MARCO dataset to fine - tune our models on TREC - CAR .;;However , there is an important difference .;Training;9
94329;3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a;Material;1;;1;MS COCO dataset;We applied Fast R - CNN ( with VGG16 ) to the MS COCO dataset [ reference ] to establish a preliminary baseline .;;"We trained on the 80k image training set for 240k iterations and evaluated on the "" test - dev "" set using the evaluation server .";Preliminary MS COCO results;25
73013;2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79;Material;1;;1;Metabolic;For the MF baseline , we closely follow the experimental protocol in , where we randomly sample 10 percent of the observed links for training and evaluate link prediction performance on the other disjoint 90 percent for the { Protein , Metabolic , Conflict } datasets .;We begin our empirical evaluation with the SDNE baseline , where we compare the representation capacity of our models on the network reconstruction task using the Arxiv - GRQC and BlogCatalog datasets .;For PowerGrid , we use 90 percent of observed links for training and evaluate on the remaining 10 percent .;Datasets and Baselines;7
80678;34a6762ed8e92612ba4fdf02ee95d2ee0d587908;Material;1;;1;DukeMTMC;We introduced two pipelines after ResNet , one is meant to work on Market1501 and the other works on DukeMTMC .;;Each pipeline consists of two convolutional blocks followed by a Fully connected layer .;Multiple Pipelines;5
92439;3c1d781f2dab8da12e3cb0e4d7abfb440a340a09;Material;0;;0;Flickr30k corpus;The premises are drawn from the Flickr30k corpus , and then the hypotheses are manually composed for each relationship class ( entailment , neutral , contradiction , and - ) .;The Stanford Natural Language Inference ( SNLI ) dataset contains human annotated sentence pairs .;The “ - ” class indicates that there is no consensus decision among the annotators , consequently , we remove them during the training and evaluation following the literature .;Dataset;9
21017;0b5aef2894d3248fb5ecc955d50501f0aa276036;Material;1;;1;IEMOCAP dataset;As the IEMOCAP dataset contains four distinct emotion categories , in the last layer of the network we used a softmax classifier whose output dimension is set to 4 .;;In order to perform classification on IEMOCAP dataset we feed the fused features ( where and ) to a softmax layer with outputs .;IEMOCAP;38
2490;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;1;5 K dataset;Our 5 K dataset is a subset of the full ImageNet - 22 K set [ reference ] .;Next we evaluate our models on a larger ImageNet subset that has 5000 categories .;The 5000 categories consist of the original ImageNet - 1 K categories and additional 4000 categories that have the largest number of images in the full ImageNet set .;Experiments on ImageNet - 5 K;13
26939;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;?;7 - bit images;"Games can easily last 10 , 000 time steps ( compared to 200–1000 in other domains ) ; observations are composed of 7 - bit images ( compared to black and white images in the work of stober08pixels , or 5 - 6 input features elsewhere ) ; observations are also more complex , containing the two players ’ score and side walls .";The Atari 2600 Pong , however , is significantly more complex than Pong domains developed for research .;In sheer size , the Atari 2600 Pong is thus a larger domain .;Final Remarks;35
88196;380b2c78d21ae6c43d418b6f0cb0222d5293d345;Material;1;;1;Chinese CTB - 5;we report results in the English PTB and Chinese CTB - 5 .;lstmacl15;Table [ reference ] shows the results of the parser in its different configurations .;Experiments;7
44551;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;1;Czech monolingual news2015;Then we continued training the model on a new parallel corpus , comprising 8.2 M sentences back - translated from the Czech monolingual news2015 , 5 copies of news - commentary v11 , and 9 M sentences sampled from Czeng 1.6pre .;This took approximately 1 M minibatches , or 3 weeks .;The model used for back - translation was a neural MT model from earlier experiments , trained on WMT15 data .;English Czech;10
18565;0a3a003457f5d7758a42a0e4b7278b39a86ed0bd;Material;na;;?;5 - shot;"Figure [ reference ] shows 1 ) MTL with HT meta - batch consistently achieves higher performances than MTL with the conventional meta - batch , in terms of the recognition accuracy in all settings ; and 2 ) it is impressive that MTL with HT meta - batch achieves top performances early , after about iterations for 1 - shot , for 5 - shot and for";Speed of convergence of HT meta - batch .;10 - shot , on the more challenging dataset – FC100 .;Results and analysis;12
31814;1023b20d226bd0af9fdf0fd1847accefbfa5ec84;Material;0;;0;CNN and Daily Mail news data;black Several large cloze - style context - question - answer datasets have been introduced recently : the CNN and Daily Mail news data and the Children ’s Book Test .;;Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches .;Text Understanding with the Attention Sum Reader Network;0
105972;4543052aeaf52fdb01fced9b3ccf97827582cef5;Material;1;;1;Leeds Sports Pose;We use two benchmark human pose estimation datasets : MPII Human Pose and Leeds Sports Pose ( LSP ) .;Human Pose Datasets .;The MPII is collected from YouTube videos with a broad range of human activities .;Experiments;9
86150;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;1;?;1;HANDS 2017;The qualitative results of the V2V - PoseNet on the ICVL , NYU , MSRA , HANDS 2017 , ITOP front - view , and ITOP top - view datasets are shown in Figure 9 , 10 , 11 , 12 , 13 , and 14 , respectively .;As shown in Table 5 , the proposed system outperforms all the existing methods by a large margin in both of views , which indicates that our model can be applied to not only 3D hand pose estimation , but also other challenging problems such as 3D human pose estimation from the front - and top - views .;;Comparison with state - of - the - art methods;16
64622;289e91654f6da968d625481ef21f52892052d4fc;Material;1;;1;Kanshan - Cup dataset;We observed the following from the Table [ reference ] : Word - based models are better than char - based models in Kanshan - Cup dataset .;Table [ reference ] gives the performance of our model and baselines over two different datasets with respect to Precision , Recall@5 and .;That may be because in Chinese the words can offer more supervisions than characters and the question tagging task needs more word supervision .;Performance Comparison;24
66027;2a94c84383ee3de5e6211d43d16e7de387f68878;Material;1;;1;minival;More object detection results using Faster R - CNN and our FPNs , evaluated on minival .;In Ta - Table 5 .;Sharing features increases train time by 1.5× ( using 4 - step training [ reference ] ) , but reduces test time .;Faster R - CNN ( on consistent proposals );13
71522;2d876ed1dd2c58058d7197b734a8e4d349b8f231;Material;1;;1;IMDb movie review dataset;We evaluate the QRNN architecture on a popular document - level sentiment classification benchmark , the IMDb movie review dataset Maas2011 .;;The dataset consists of a balanced sample of 25 , 000 positive and 25 , 000 negative reviews , divided into equal - size train and test sets , with an average document length of 231 words Wang2012 .;Sentiment Classification;5
76583;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;1;text8 dataset;We conduct experiments of CharLM using the text8 dataset mahoney2011large , which consists of 100 M characters including only alphabetical characters and spaces derived from Wikipedia .;Hence , we expect MoS to yield similar performance to Softmax on CharLM .;We follow mikolov2012subword and use the first 90 M characters for training , the next 5 M for validation and the final 5 M for testing .;An inverse experiment on character - level language modeling;33
94441;3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a;Material;1;;1;extended PASCAL - Person - Part;Extensive experiments on MPII Human Pose Multi - Person , extended PASCAL - Person - Part and WAF benchmarks evidently show the efficiency and effectiveness of the proposed GPN .;We implement GPN based on the Hourglass network for learning joint detector and dense regressor , simultaneously .;Moreover , GPN achieves new state - of - the - art on all these benchmarks .;Introduction;1
36467;14ad9d060c1e8f0449e697ee189ac346353fbfbc;Material;1;;1;JNLPBA;The error analysis of our STM , which is a single BiLSTM - CRF model , shows that the majority of errors are classified as bio - entity errors which comprise up to 49.3 % of the total errors in JNLPBA .;Error analysis was conducted on models which showed best performance in our experiments .;According to the error analysis of our STM model , bio - entity errors constitute 1333 errors out of 4334 errors , comprising 30.8 % of all the errors .;Discussion;17
73415;2e57bccb74bcb46cbc5b4225b62679023ed1f9da;Material;1;;1;CNN and Daily Mail Datasets;section : CNN and Daily Mail Datasets;In this section , we evaluate the performance of ReasoNets in machine comprehension datasets , including unstructured CNN and Daily Mail datasets , the Stanford SQuAD dataset , and a structured Graph Reachability dataset .;We examine the performance of ReasoNets on CNN and Daily Mail datasets .;CNN and Daily Mail Datasets;7
100263;41d08fb733f3e50ac183490f84d6377dffccf350;Material;1;;1;3D - R2N2;On many categories our method even outperforms the 3D - R2N2 's prediction given 5 views .;3R - R2N2 is also able to predict 3D shapes from more than one views .;To further contrast the two methods , we visualize some typical examples .;3D Shape Reconstruction from RGB Images;13
2971;01959ef569f74c286956024866c1d107099199f7;Material;na;;?;abstract scenes;"We collected a new dataset of "" realistic "" abstract scenes to enable research focused only on the high - level reasoning required for VQA by removing the need to parse real images .";The MS COCO dataset has images depicting diverse and complex scenes that are effective at eliciting compelling and diverse questions .;Three questions were collected for each image or scene .;INTRODUCTION;2
96826;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;na;;?;unlabeled data;In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional task - specific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .;Dai2015SemisupervisedSL and Ramachandran2017ImproveSeq2SeqLMGal2016ATG pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .;;Related work;2
40463;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;0;Wall Street Journal task;"In the area of speech recognition , much of the pioneering early work was driven by a series of carefully designed tasks with DARPA - funded datasets publicly released by the LDC and NIST : first simple ones like the “ resource management ” task with a small vocabulary and carefully controlled grammar ; then read speech recognition in the Wall Street Journal task ; then Broadcast News ; each progressively more difficult for automatic systems .";Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go to simple speech recognition tasks like carefully read newspaper speech and rigidly constrained small - vocabulary tasks in noise .;One of last big initiatives in this area was in conversational telephone speech ( CTS ) , which is especially difficult due to the spontaneous ( neither read nor planned ) nature of the speech , its informality , and the self - corrections , hesitations and other disfluencies that are pervasive .;Introduction;1
26633;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;?;video games;The console ’s joystick , as well as some of the original games such as Adventure and Pitfall ! , are iconic symbols of early video games .;"Over 500 original games were released for the console ; “ homebrew ” games continue to be developed today , over thirty years later .";Nearly all arcade games of the time – Pac - Man and;The Atari 2600;3
45695;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;na;;?;question - answer pairs;Using similar resources as the dialog dataset of , our new benchmark WikiMovies addresses both deficiencies by providing a substantial corpus of question - answer pairs that can be answered by either using a KB or a corresponding set of documents .;Unfortunately , these datasets are very small ( hundreds of examples ) and , because of their answer selection setting , do not offer the option to directly compare answering from a KB against answering from pure text .;Even though standard pipeline QA systems like AskMR have been recently revisited , the best published results on TrecQA and WikiQA have been obtained by either convolutional neural networks or recurrent neural networks —both usually with attention mechanisms inspired by .;Related Work;2
65172;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;;0;WSJ;ea:2015:arxiv , however , they only explore word embeddings , orthographic information and evaluate on WSJ only .;Bi - LSTMs for POS tagging are also reported in wang :;A related study is cheng : fang : ostendorf:2015 who propose a multi - task RNN for named entity recognition by jointly predicting the next token and current token ’s name label .;Related Work;11
102038;42e80c73867bff9eaff6beceb8730fc1276283b9;Material;1;;1;2014 / 2016 French - English;2014 / 2016 French - English and German - English show the effectiveness of our approach , as our proposed system outperforms the previous state - of - the - art in unsupervised machine translation by 5 - 7 BLEU points in all these datasets and translation directions .;Our experiments on WMT;Our system also outperforms the supervised WMT 2014 shared task winner in English - to - German , and is around 2 BLEU points behind it in the rest of translation directions , suggesting that unsupervised machine translation can be a usable alternative in practical settings .;Introduction;1
85300;3652c2d20f198dc39ad159eba55d08341c56d628;Material;1;;1;STL10;We show that mixing the ideas of these two concepts is fruitful , since we achieve near state - of - the - art performance on several datasets such as MNIST , CIFAR - 10 , and STL10 , with simple architectures and no data augmentation .;In this paper , we have proposed a new methodology for combining kernels and convolutional neural networks .;Some challenges regarding our work are left open for the future .;Conclusion;26
97713;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;1;;1;AFLW2000 - 3D.;AFLW2000 - 3D. As can be seen , the 3D reconstruction results of 2DASL outperforms 3DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .;7 ( a ) shows the comparison results on;We show some visual results of our 2DASL and compare with PRNet and VRN - Guided in Fig .;3D face reconstruction;16
63723;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;0;;0;DUC - 2003;The task of abstractive summarization has been standardized using the DUC - 2003 and DUC - 2004 competitions .;As such , human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document .;The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans .;Related Work;7
81385;34f63959ea4a13a05948274a1558c6854a051150;Material;0;;0;STS - B;paragraph : STS - B;Accuracy is used as the evaluation metric .;The Semantic Textual Similarity Benchmark is a collection of sentence pairs collected from multiple data resources including news headlines , video , and image captions , and NLI data .;STS - B;19
58839;23dcfda130aada27c158c0b5f394cac489c9c795;Material;0;;0;AFW;AFW is a popular dataset , also commonly used to test landmark detection , which contains rough pose annotations .;Results can be seen in Table [ reference ] .;It contains 468 in - the - wild faces with absolute yaw degree;AFLW and AFW Benchmarking;12
59779;2451db113552afb6d9ad15ef4009ec4133d28f74;Material;1;;1;Stanford cars;For fine - grained categorization , we use three popular fine - grained benchmarks , i.e. , CUB - 200 - 2011 ( Birds ) , FGVC - aircraft ( Aircrafts ) and Stanford cars ( Cars ) .;As in , we report the results on the validation set .;The Birds dataset contains 11 , 788 images from 200 species , with large intra - class variation but small inter - class variation .;Datasets and Our Meta - layer Implementation;13
45923;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;1;;1;WikiQA;"Note that this is much larger than most existing datasets ; for example , the WikiQA dataset for which we also conduct experiments in Sec .";The same question ( even worded differently ) can not appear in both train and test sets .;[ reference ] has only 1000 training pairs .;Question - Answer Pairs;16
102927;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;1;OMNIGLOT;In Figure [ reference ] we show two examples of few - shot learning by conditioning on samples of unseen characters from OMNIGLOT , and conditioning on samples of digits from MNIST .;The decoder used a Bernoulli likelihood .;The samples are mostly of a high - quality , and this shows that the neural statistician can generalize even to new datasets .;Omniglot;18
50668;1e7678467b1807777dcd9be557b79328ce9419a8;Material;1;;1;YFCC100 M large - scale collection of unlabelled images;We add 10 K distractor images randomly sampled from the YFCC100 M large - scale collection of unlabelled images .;We also report the performance of our network in a copy detection setting , indicating the mean average precision on the “ strong ” subset of the INRIA Copydays dataset .;We call the combination C10k .;Datasets .;27
83388;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;1;;;German - English machine translation track;We use data from the German - English machine translation track of the IWSLT 2014 evaluation campaign cettolo2014 .;For the translation task , our generative model is an LSTM with hidden units and it uses the same attentive encoder architecture as the one used for summarization .;The corpus consists of sentence - aligned subtitles of TED and TEDx talks .;Machine Translation;13
101135;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;union set of PASCAL VOC 2007 trainval;We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval .;For the feature - shared variant , the result is 69.9%—better than the strong SS baseline , yet with nearly cost - free proposals .;The mAP is 73.2 % .;Experiments on PASCAL VOC;11
91244;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;human readable text;converted image parsing results into a semantic representation in the form of Web Ontology Language , which is converted to human readable text .;Zhu et al .;A more sophisticated CRF - based method use of attribute detections beyond triplets was proposed by Kulkarni et al .;Image Captioning;4
13049;072fd0b8d471f183da0ca9880379b3bb29031b6a;Material;1;;;Cityscapes training set;Cityscapes labels→photo 2975 training images from the Cityscapes training set , trained for 200 epochs , with random jitter and mirroring .;Weights were initialized from a Gaussian distribution with mean 0 and standard deviation 0.02 .;We used the Cityscapes validation set for testing .;Training details;24
85292;3652c2d20f198dc39ad159eba55d08341c56d628;Material;1;;;STL - 10;The standard deviations for STL - 10 was always below .;Since CKN - PM and CKN - GM exploit a different information , we also report a combination of such two models , CKN - CO , by concatenating normalized image representations together .;Our approach appears to be competitive with the state of the art , especially on STL - 10 where only one method does better than ours , despite the fact that our models only use layers and require learning few parameters .;Visual Recognition on CIFAR - 10 and STL - 10;25
36022;14908a18ff831005b6b4fc953ce61e1b4e7b54ee;Material;0;;;Binary Sentiment Tweets;subsection : Binary Sentiment Tweets;section : Results;For binary sentiment , we compare our model on two tasks :;Binary Sentiment Tweets;16
40564;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;na;;;development data;Both base models are independently trained , and then the score fusion weight is optimized on development data .;We also trained a fused model by combining a ResNet model and a VGG model at the senone posterior level .;The fused system is our best single system .;CNNs;4
59772;2451db113552afb6d9ad15ef4009ec4133d28f74;Material;1;;;ImageNet LSVRC2012 dataset;Datasets For large - scale image classification , we adopt ImageNet LSVRC2012 dataset with 1 , 000 object categories .;;The dataset contains 1.28 M images for training , 50 K images for validation and 100 K images for testing ( without published labels ) .;Datasets and Our Meta - layer Implementation;13
105376;4508f81033c9a7cec785ce4d16f1193920c1b341;Material;1;;;English and German strings;"The English and German strings are encoded as sequences of characters ; no explicit segmentation into words or morphemes is applied to the strings .";We use NewsTest 2013 for validation and NewsTest 2014 and 2015 for testing .;The outputs of the network are strings of characters in the target language .;Character - Level Machine Translation;15
41134;16da4d6503e17f8597602437358461c252244bf7;Material;1;;;ICDAR 2015 competition test dataset;[ reference ] . ICDAR 2015 competition test dataset consists of 500 images containing incidental scene texts with arbitrary orientations .;This section introduces our performances on ICDAR 2015;Our method could achieve competitive results of Recall 79.68 % , Precision 85.62 % , and F - measure 82.54 % .;A. ICDAR 2015;10
53755;2116b2eaaece4af9c28c32af2728f3d49b792cf9;Material;1;;;Pascal Visual Object Challenge;In 2010 , a subset of roughly 1000 images in each of 1000 classes was the basis of an object recognition competition , a part of the Pascal Visual Object Challenge .;The images were collected from the web and labelled by human labellers using Amazon ’s Mechanical Turk crowd - sourcing tool .;This is the version of ImageNet on which we performed our experiments .;ImageNet;11
57523;2393447b8b0b79046afea1c88a8ed3949338949e;Material;1;;;WFA set;Our model , which was trained with the ALL set consisting of the two styles , outperformed the model trained with the WFA set consisting of the single style .;set .;Multi - style learning allowed our model to improve NLG performance by also using non - sentence answers .;Does our multi - style learning improve NLG performance ?;37
52539;207e0ac5301a3c79af862951b70632ed650f74f7;Material;1;;;Market1501;Note that for the Market1501 dataset , since there are on average 14.8 cross - camera ground truth matches for each query , we additionally use mean average precision ( mAP ) as in to evaluate the performance .;Due to space limitation and for easier comparison with published results , we only report the cumulated matching accuracy at selected ranks in tables rather than plotting the actual curves .;Parameter setting There is no free parameter to tune for our model .;Datasets and Settings;11
73228;2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79;Material;1;;;Protein;For featureless link prediction , our LoNGAE model marginally outperforms MF on { Protein , Metabolic , Conflict } and is significantly better than MF on PowerGrid .;The datasets under consideration for link prediction exhibit varying degrees of class imbalance .;Consistent with MF results , we observe that incorporating external node features provides a boost in link prediction accuracy , especially for the Protein dataset where we achieve a 6 percent increase in performance .;Results and Analysis;9
67271;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;real noisy image;Both the ground - truth clean image and noise level are unknown for real noisy image .;( i );( ii );Experiments on Real Noisy Images;18
84824;364da079f91a6cb385997be990af06e9ddf6e888;Material;1;;;IMDB;Following the generation of IMDB , we chose the training set and the test set so that one half of each set consists of positive reviews and the other half is negative , regarding rating 1 and 2 as negative and 4 and 5 as positive , and that the reviewed products are disjoint between the training set and test set .;We chose electronics as it seemed to be very different from movies .;Note that to extract text from the original data , we only used the text section , and we did not use the summary section .;Elec : electronics product reviews;17
23313;0ca2bd0e40a8f0a57665535ae1c31561370ad183;Material;0;;;Text8 dataset;The Text8 dataset ( Mahoney , 2009 ) consists of 100 M characters extracted from the Wikipedia corpus .;;Text8 contains only alphabets and spaces , and thus we have total 27 symbols .;Text8;10
91595;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;large - scale VQA data;Two large - scale VQA data are constructed both based on MS COCO images .;This dataset is constrained to 37 object categories and uses only 25 test images .;The Toronto COCO - QA Dataset contains 78 , 736 training and 38 , 948 testing examples , which are generated from 117 , 684 images .;Evaluation on Visual Question Answering;17
61146;25c108a56e4cb757b62911639a40e9caf07f1b4f;Material;0;;;Multi - Attribute Labelled Faces;Multi - Attribute Labelled Faces ( MALF ) includes 5 , 250 images with 11 , 931 annotated faces collected from the Internet .;It is larger and more challenging than AFW .;The annotation set is cleaner than that of AFW;Setup and Implementation Details;9
50790;1e7678467b1807777dcd9be557b79328ce9419a8;Material;0;;;imagenet - 2012;It obtains what appear to be the best reported classification results on imagenet - 2012 for a convnet with publicly available weights .;Finally , Section [ reference ] shows how to use the ingredients of MultiGrain to improve the accuracy of an off - the - shelf pre - trained ConvNet at almost no additional training cost .;;References;35
9587;05ee231749c9ce97f036c71c1d2d599d660a8c81;Material;1;;;public IJB - A;They outperform state - of - the - art methods by a large margin on the public IJB - A [ reference ] and IJB - B [ reference ] face recognition benchmarks .;The networks are trained in an end - to - end fashion with only identity - level labels .;These datasets are currently the most challenging in the community , and we evaluate on these in this paper .;Introduction;2
81427;34f63959ea4a13a05948274a1558c6854a051150;Material;0;;;Flickr30 corpus;The Stanford Natural Language Inference ( SNLI ) dataset contains 570k human annotated sentence pairs , in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated .;;This is the most widely used entailment dataset for NLI .;SNLI;26
34404;12f008bea798a05ebfa2864ec026999cb375bcd9;Material;1;;;CNN and Daily Mail websites;The first two , CNN and Daily Mail news stories consist of articles from the popular CNN and Daily Mail websites hermann2015teaching .;We evaluate the GA reader on five large - scale datasets recently proposed in the literature .;A query over each article is formed by removing an entity from the short summary which follows the article .;Datasets;10
87384;372bc106c61e7eb004835e85bbfee997409f176a;Material;1;;;MNIST domain;We trained the CoGAN by jointly solving the digit classification problem in the MNIST domain which used the images and labels in and the CoGAN learning problem which used the images in both and .;For classifying digits , we attached a softmax layer to the last hidden layer of the discriminative models .;This produced two classifiers : for MNIST and for USPS .;Applications;5
57559;2393447b8b0b79046afea1c88a8ed3949338949e;Material;na;;;answerable data;set , where the positive class is the answerable data .;Figure [ reference ] shows the precision - recall curve of answer possibility classification on the ALL dev .;Our model identified the answerable questions well .;Does our model accurately identify answerable questions ?;41
28601;0f810eb4777fd05317951ebaa7a3f5835ee84cf4;Material;0;;;high - dimensional RL benchmarks;Several very recent extensions of count - based exploration methods have produced impressive results on high - dimensional RL benchmarks .;In the high - dimensional setting – where the agent can not hope to visit every state during training – this bound offers no guarantee that the trained agent will perform well .;These algorithms closely resemble MBIE - EB , but they substitute the state - action visit - count for a generalised count which quantifies the similarity of a state to previously visited states .;Exploration in Large MDPs;5
91335;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;structured information;DBpedia has been created by extracting structured information from Wikipedia , and is thus significantly larger and more general than a hand - crafted KB .;Instead of building a problem - specific KB , we use a pre - built large - scale KB ( DBpedia ) from which we extract information using a standard RDF query language .;Rather than having a user pose their question in a formal query language , our VQA system is able to encode questions written in natural language automatically .;Visual Question Answering;5
83955;3600aac8edc5bc015e69f2ffa893c21b6d4e1057;Material;0;;;MAFA;The performance of FAN is evaluated across multiple face datasets : WiderFace and MAFA .;;WiderFace dataset [ ] :;Datasets;11
88218;380b2c78d21ae6c43d418b6f0cb0222d5293d345;Material;1;;;German;We also include results with pretrained word embeddings for English , Chinese , German , and Spanish following the same training setup as Dyer et al .;We used predicted part - of - speech tags provided by the CoNLL 2009 shared task organizers .;"( 2015 ) ; for English and Chinese we used the same pretrained word embeddings as in Table [ reference ] , for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3 .";Experiments;7
100432;420c46d7cafcb841309f02ad04cf51cb1f190a48;Material;1;;;VOC - 2012;For FCN - 8s and DeepLab , we evaluate the public models trained by the original authors on VOC - 2012 .;We now compare the accuracy of our front - end module to the FCN - 8s design of and the DeepLab network of .;Segmentations produced by the different models on images from the VOC - 2012 dataset are shown in Figure [ reference ] .;Front End;4
3574;0209389b8369aaa2a08830ac3b2036d4901ba1f1;Material;1;;;COCO - DensePose dataset;[ reference ] by comparing the COCO - DensePose dataset to other sources of supervision for dense pose estimation and then in Sec .;We start in Sec .;[ reference ] compare the performance of the model - based system of with our discriminatively - trained system .;Single - Person Dense Pose Estimation;14
104023;435259c5f3cffd75ef837a8e638cc8f6244e25c4;Material;0;;;iSEG website;Moreover , as mentioned on the iSEG website , manually correcting the data of a single subject took approximately one week .;Confidence maps show that regions with lowest confidence typically correspond to the borders between these two tissues .;Taking into account that nearly 500 typically developing children will be scanned for the Baby Connectome Project , the adoption of an automatic segmentation tool is highly needed .;Discussion;19
55835;22aab110058ebbd198edb1f1e7b4f69fb13c0613;Material;1;;;ILSVRC2015;We evaluate our models on ImageNet ILSVRC 2012 ILSVRC2015 at 128 128 , 256 256 , and 512 512 resolutions , employing the settings from Table [ reference ] , row 8 .;;The samples generated by our models are presented in Figure [ reference ] , with additional samples in Appendix [ reference ] , and online .;Evaluation on ImageNet;11
86048;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;0;;;ICVL Hand Posture Dataset;ICVL Hand Posture Dataset .;;The ICVL dataset [ reference ] consists of 330 K training and 1.6 K testing depth images .;Datasets;13
63840;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;0;;;NLP annotations;On this corpus too , since we have only a single sentence from source and no NLP annotations , we ran just the models words - lvt2k - 1sent and words - lvt5k - 1sent .;The only change we made to the decoder is to suppress the model from emitting the end - of - summary tag , and force it to emit exactly 30 words for every summary , since the official evaluation on this corpus is based on limited - length Rouge recall .;The performance of this model on the test set is compared with ABS and ABS + models , RAS - Elman from , as well as TOPIARY , the top performing system on DUC - 2004 in Table [ reference ] .;DUC Corpus;10
93516;3d5d9d8e74b215609eabba80ef79a35ebf460e49;Material;1;;;Yosemite dataset;We conduct the experiment using winter summer translation with the Yosemite dataset .;Here we have the quantitative evaluation on the realism and diversity of the generated images .;For realism , we conduct a user study using pairwise comparison .;Quantitative Evaluation;9
62423;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;na;;;3D meshes;Surface Correspondence on 3D meshes .;The 3DMatch model without pre - training on reconstructions yields a lower performance , demonstrating the importance of pre - training on reconstruction data .;In our final experiment , we test 3DMatch 's ability to generalize even further to other modalities .;Method;19
72697;2e10643c3759f97b673ff8c297778c0b6c20032b;Material;0;;;DBPedia ontology dataset;DBPedia ontology dataset .;The fields used are title and content .;DBpedia is a crowd - sourced community effort to extract structured information from Wikipedia .;Large - scale Datasets and Results;11
96778;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;0;;;word2vec;Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors Turian2010WordRA , word2vec , Pennington2014GloveGV are a standard component of most state - of - the - art NLP architectures , including for question answering liu2017stochastic , textual entailment Chen2017EnhancedLF and semantic role labeling He2017DeepSR .;;However , these approaches for learning word vectors only allow a single context - independent representation for each word .;Related work;2
54907;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;0;;;MNIST - M;MNIST - M : top feature extractor layer Syn Numbers;MNIST →;→ SVHN : last hidden layer of the label predictor;Experiments;7
69779;2cf6a8389135f682b0cb727a07f4e77c097d5434;Material;1;;;Waveform;[ reference ] , where there are three sub - figures showing the speed of four methods on the benchmark datasets of Letter , MNIST and Waveform respectively .;The results are illustrated in Fig .;As we shall see , both selection time of TED and RRSS increases dramatically as increases .;Speed vs. increasing;22
98041;4087ebc37a1650dbb5d8205af0850bee74f3784b;Material;0;;;Cifar;The two Cifar ( i.e. , Cifar - 10 / Cifar - 100 ) datasets [ reference ];Cifar ( image classification ) .;contain 50k training images and 10k testing images , and 10 / 100 label classes .;A Training Details;11
41683;1751668492bac56f0ae2b6410417515ab3215945;Material;1;;;PTB - WSJ development set;We observed that the character - level BiLSTM outperformed the CNN by 0.1 % on the PTB - WSJ development set , and hence in all of our experiments we use the character - level BiLSTM .;A Bi - directional LSTM ( BiLSTM ) processes each sequence both forward and backward to capture sequential information , while preventing the vanishing / exploding gradient problem .;Specifically , we generate a character - level representation for each word by feeding its character embeddings into the BiLSTM and obtaining the concatenated final states .;Character - level BiLSTM .;7
48636;1d5d0a41b720bc51fd568cf78f8aa4ec5af4f802;Material;na;;;RGB image;The first three channels is the input RGB image .;The rgbd baseline is a CNN architecture that takes as input a 5 - channel tensor .;The fourth channel is the depth channel .;Input representation;29
98052;4087ebc37a1650dbb5d8205af0850bee74f3784b;Material;0;;;Penn Tree Bank dataset;The Wikitext 2 dataset is modeled after the Penn Tree Bank dataset and consists of preprocessed and tokenized sentences from Wikipedia .;WikiText 2 ( language modeling ) .;The training set is 2089k words , the validation set 218k words , and the test set 246k words .;A Training Details;11
106991;45e8ef229fae18b0a2ab328037d8e520866c3c81;Material;1;;;MPII validation set;We conduct ablation study on the MPII validation set used in with a 2 - stack hourglass network as the basic model .;;Architectures of PRM .;Ablation Study;14
62212;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;0;;;Point Cloud Library;Many geometric descriptors have been proposed including Spin Images [ reference ] , Geometry Histograms [ reference ] , and Signatures of Histograms [ reference ] , Feature Histograms [ reference ] . Many of these descriptors are now available in the Point Cloud Library [ reference ] .;Hand - crafted 3D Local Descriptors .;While these methods have made significant progress , they still struggle to handle noisy , low - resolution , and incomplete real - world data from commodity range sensors .;Related Work;3
95842;3f3a483402a3a2b800cf2c86506a37f6ef1a5332;Material;0;;;LSPET;In some cases LSP training and LSPET are added to MPII ( marked as MPII + LSPET in the experiments ) .;The MPII training set ( people ) is used as default .;Evaluation measures .;Evaluation of Part Detectors;11
99221;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;1;;;depth images;We can see that compared with KITTI LiDAR data , depth images can be popped up to much more dense point clouds .;[ reference ] we visualize some representative detection results on SUN - RGBD data .;However even with such dense point cloud , strong occlusions of indoor objects as well as the tight arrangement present new challenges for detection in indoor scenes .;Visualizations for SUN - RGBD ( Sec 5.1 );43
50663;1e7678467b1807777dcd9be557b79328ce9419a8;Material;1;;;Holidays dataset;For image retrieval , we report the mean average precision on the Holidays dataset , with images rotated manually when necessary , as in prior evaluations on this dataset .;Classification accuracies are reported on the validation images of this dataset .;"We also report the accuracy on the UKB object recognition benchmark , which contains instances of objects under varying viewpoints each ; each image is used as a query to find its 4 closest neighbors in embedding space ; the number of correct neighbors is averaged across all images , yielding a maximum score of .";Datasets .;27
88674;38d7920f0e8a3a672ea37c8612b2b2947b9ba9d1;Material;0;;;OpenImagesV4 containing million images;In very large datasets like OpenImagesV4 containing million images , most objects are large and images provided are high resolution ( x ) , so it is less important to upsample images by .;It is also possible that the effective receptive field of deep neural networks is not large enough to leverage far away pixels in the image , as suggested in .;In this case , with SNIPER , we generate million chips of size x using scales of ( / ms , 1 ) .;Benefits;8
84420;360cfa09b2f7c8e10b1831d899c5a51aefa1883e;Material;1;;;reverberated version of WSJ;Training , in fact , is performed with a reverberated version of WSJ , while test is characterized by both non - stationary noises and reverberation .;A challenging aspect of this dataset is the acoustic mismatch between training and testing conditions .;Tables [ reference ] and [ reference ] summarize the results obtained with the simulated and real parts of this dataset .;Recognition performance on DIRHA English WSJ;19
79654;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;na;;;local data;The filters employed in graph convolutions are in essence local in nature and hence can only provide an “ average / aggregate view ” of the local data .;Our last concern with the standard GCNN model is their limited ability in exploiting global information for the purpose of graph classification .;"This shortcoming poses a serious difficulty in handling graphs where node labels are not present ; approaches which initialize ( node ) feature values using , e.g. , node degree , are not much helpful in this respect .";Introduction;1
63922;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;0;;;DUC - 2004;The best performing system on the DUC - 2004 task , called TOPIARY , used a combination of linguistically motivated compression techniques , and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output .;The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans .;Some of the other notable work in the task of abstractive summarization includes using traditional phrase - table based machine translation approaches , compression using weighted tree - transformation rules and quasi - synchronous grammar approaches .;Related Work;7
56625;231af7dc01a166cac3b5b01ca05778238f796e41;Material;0;;;ImageNet classes;From each of the 1 , 000 ImageNet classes , 5 images are randomly chosen , which gives 5 , 000 ImageNet images .;ImageNet contamination :;The images are ensured to be RGB and to have a minimal size of 256x256 .;Fréchet Inception Distance ( FID );18
79870;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;NCI109;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;To evaluate our GCAPS - CNN model , we perform graph classification tasks on variety of benchmark datasets .;In second round , we used social network datasets namely : COLLAB , IMDB - BINARY , IMDB - MULTI , REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset contains enzymes and non - enzymes proteins structures .;Experiment and Results;12
33110;1109b663453e78a59e4f66446d71720ac58cec25;Material;0;;;1000 - category ImageNet dataset;Recognizing the category of the dominant object in an image is a tasks to which Convolutional Networks ( ConvNets ) have been applied for many years , whether the objects were handwritten characters , house numbers , textureless toys , traffic signs , objects from the Caltech - 101 dataset , or objects from the 1000 - category ImageNet dataset .;;The accuracy of ConvNets on small datasets such as Caltech - 101 , while decent , has not been record - breaking .;Introduction;1
79929;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;na;;;bioinformatics datasets;From Table [ reference ] , it is clear that our GCAPS - CNN model consistently outperforms most of the considered deep learning methods on bioinformatics datasets ( except on D & D dataset ) with a significant margin of classification accuracy gain ( highest being on NCI1 dataset ) .;Graph Classification Results :;Again , this trend is continued to be the same on social network datasets as shown in Table [ reference ] .;Experiment and Results;12
55018;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;1;;;Office;For the Office dataset , we directly compare the performance of our full network ( feature extractor and label predictor ) against recent DA approaches using previously published results .;"Since the SA baseline requires to train a new classifier after adapting the features , and in order to put all the compared settings on an equal footing , we retrain the last layer of the label predictor using a standard linear SVM for all four considered methods ( including ours ; the performance on the target domain remains approximately the same after the retraining ) .";CNN architectures .;Experiments;7
75214;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;0;;;YouTubeFaces dataset;Face recognition performance evaluation has traditionally focused on one - to - one verification , popularized by the Labeled Faces in the Wild dataset for imagery and the YouTubeFaces dataset for videos .;;In contrast , the newly released IJB - A face recognition dataset unifies evaluation of one - to - many face identification with one - to - one face verification over templates , or sets of imagery and videos for a subject .;Template Adaptation for Face Verification and Identification;0
32966;10fd174fefd5e36a523805e4c2d2fbf1d12a3ae8;Material;na;;;colour images;Thus , as well as using the full ImageNet data set , we devised a new data set – mini ImageNet – consisting of colour images of size with classes , each having examples .;ImageNet is a notoriously large data set which can be quite a feat of engineering and infrastructure to run experiments upon it , requiring many resources .;This dataset is more complex than CIFAR10 , but fits in memory on modern machines , making it very convenient for rapid prototyping and experimentation .;ImageNet;13
97204;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;na;;;task specific data;[ reference ] , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .;As noted in Sec .;To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .;Fine tuning biLM;18
103561;434bf475addfb580707208618f99c8be0c55cf95;Material;0;;;MMI;paragraph : MMI;[ b ] 0.46 [ b ] 0.46;The MMI Database contains videos of people showing emotions .;MMI;19
78212;325093f2c5b33d7507c10aa422e96aa5b10a33f1;Material;1;;;COCO - Stuff;image classification on ImageNet - 1k showing approximately on - par performance with state - of - the - art models and ii ) semantic segmentation on COCO - Stuff , Cityscapes and Mapillary Vistas , considerably benefiting from the additional available memory and generating new high - scores on the challenging Vistas dataset .;Experimental evaluations for i );;Introduction;1
100027;41d08fb733f3e50ac183490f84d6377dffccf350;Material;1;;;RGB - D;Our goal is to reconstruct the complete 3D shape of an object from a single 2D image ( RGB or RGB - D ) .;;We represent the 3D shapes in the form of unordered point set;Problem and Notations;5
63175;27c761258329eddb90b64d52679ff190cb4527b5;Material;0;;;Medical Imaging data;Meanwhile , different variants of U - Net models have been proposed , including a very simple variant of U - Net for CNN - based segmentation of Medical Imaging data [ reference ] .;[ reference ][ reference ] .;In this model , two modifications are made to the original design of U - Net :;II . RELATED WORK;3
47367;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;0;;;PASCAL;Differently from AP , which is measured on the PASCAL test set , CorLoc is evaluated on the union of the training and validation subset of PASCAL .;[ reference ] . CorLoc is the percentage of images that contain at least one instance of the target object class for which the most confident detected bounding box overlaps by at least 50 % with one of these instances .;For classification , we use the standard PASCAL VOC protocol and report AP .;Benchmark data .;10
44585;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;English Russian;For English Russian , we can not effectively learn BPE on the joint vocabulary because alphabets differ .;;We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the BPE operations on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .;English Russian;12
27063;0ee850dd6640a96531ac5ad21da5438db04d8b3c;Material;1;;;homogeneous news collection;We train our networks using tens of millions of training instances and evaluate it on two standard collections : a homogeneous news collection ( Robust ) and a heterogeneous large - scale web collection ( ClueWeb ) .;We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using word embedding representation ) .;Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance , with over 13 % and 35 % MAP improvements over the BM25 model on the Robust and the ClueWeb collections .;Neural Ranking Models with Weak Supervision;0
44496;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;newstest2013;We validate the model every 10000 minibatches via Bleu on a validation set ( newstest2013 , newstest2014 , or half of newsdev2016 for EN RO ) .;We train the models with Adadelta , reshuffling the training corpus between epochs .;We perform early stopping for single models , and use the 4 last saved models ( with models saved every 30000 minibatches ) for the ensemble results .;Baseline System;2
53442;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;na;;;datasets;Next , we provide brief descriptions of the various datasets used in our paper .;Our code and models are available at github.com / abhimanyudubey / confusion .;Table 2 .;Experimental Details;11
40521;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;na;;;audio segments;The transcribers were given the same audio segments as were provided to the speech recognition system , which results in short sentences or sentence fragments from a single channel .;Aside from the standard two - pass checking in place , we did not do a complex multi - party transcription and adjudication process .;This makes the task easier since the speakers are more clearly separated , and more difficult since the two sides of the conversation are not interleaved .;Human Performance;2
32613;10f62af29c3fc5e2572baddca559ffbfd6be8787;Material;1;;;Stanford Sentiment Treebank;We use the Stanford Sentiment Treebank ( SST ) benchmark .;Our task in this regard is to predict the sentiment polarity of movie reviews .;This dataset consists of 11855 movie reviews and are split into train ( 8544 ) , dev ( 1101 ) , and test ( 2210 ) .;Datasets;10
81419;34f63959ea4a13a05948274a1558c6854a051150;Material;0;;;Winograd NLI;The Winograd NLI ( WNLI ) is a natural language inference dataset derived from the Winograd Schema dataset .;;This is a reading comprehension task .;WNLI;25
101649;42764b57d0794b63487a295ce8c07eeb6961477e;Material;1;;;PASCAL VOC dataset;We comprehensively evaluate our method on the PASCAL VOC dataset .;Meanwhile , under this training framework , our cascade model can be extended to more stages , leading to improvements on accuracy .;Our method results in 63.5 % mean Average Precision ( mAP ) , about 3.0 % higher than the previous best results using the same VGG network .;Introduction;1
21673;0be9ca65ad318ee3729928882ef2c403d4b6d24e;Material;1;;;PTB test set;Table [ reference ] shows the bracketing F1 scores on the PTB test set .;;"This table is divided into three parts by horizontal lines ; the upper part describes the scores by single language modeling based rerankers , the middle part shows the results by ensembling five rerankers , and the lower part represents the current state - of - the - art scores in the setting without external data .";Results;16
56644;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;FIDs;In Figure [ reference ] , [ reference ] , [ reference ] , and [ reference ] we show examples of images generated with DCGAN trained on CelebA with FIDs 500 , 300 , 133 , 100 , 45 , 13 , and FID 3 achieved with WGAN - GP on CelebA.;The larger the disturbance level is , the larger the FID and IND should be .;;Fréchet Inception Distance ( FID );18
40523;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;NIST 2000 test set;The resulting numbers are 5.9 % for the Switchboard portion , and 11.3 % for the CallHome portion of the NIST 2000 test set , using the NIST scoring protocol .;Thus , it is the same condition as reported for our automated systems .;These numbers should be taken as an indication of the “ error rate ” of a trained professional working in industry - standard speech transcript production .;Human Performance;2
2465;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;;ImageNet validation set;Table 5 shows more results of single - crop testing on the ImageNet validation set .;Comparisons with state - of - the - art results .;In addition to testing a 224×224 crop , we also evaluate a 320×320 crop following [ reference ] . Our results compare favorably with ResNet , Inception - v3 / v4 , and;Experiments on ImageNet - 1 K;11
105988;4543052aeaf52fdb01fced9b3ccf97827582cef5;Material;1;;;IBUG;The testing is done on the common subset ( testing images of HELEN and LFPW ) , challenge subset ( all images from IBUG ) and their union .;Following and , we use the training images of HELEN , LFPW and all images of AFW , totally 3148 images , as the training set .;We use the provided bounding boxes from the 300 - W challenge to crop faces .;Experiments;9
62259;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;7 - Scenes;Specifically , we use a total of over 200 K RGB - D images of 62 different scenes collected from Analysis - by - Synthesis [ 37 ] , 7 - Scenes [ reference ] , SUN3D [ reference ] , RGB - D Scenes v.2;Third , by learning from multiple reconstruction datasets , we can optimize 3DMatch to generalize and robustly match local geometry in real - world partial 3D data under a variety of conditions .;[ reference ] , and Halber et al .;Learning From Reconstructions;4
89262;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;Pascal VOC challenge evaluation server;Annotations of the VOC 2012 test set , which consists of 1456 images , are not publicly available and hence the final results on the test set were obtained by submitting the results to the Pascal VOC challenge evaluation server [ reference ] . Regardless of the smaller number of images , we found that the relative improvements of the accuracy on our validation set were in good agreement with the test set .;We call this set the reduced validation set in the sequel .;As a first step we directly compared the potential advantage of learning the model end - to - end with respect to alternative learning strategies .;Pascal VOC Datasets;18
86056;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;0;;;First - Person Hand Action;consists of 957 K training and 295 K testing depth images that are sampled from BigHand2.2 M [ reference ] and First - Person Hand Action [ reference ] datasets .;The HANDS 2017 frame - based 3D hand pose estimation challenge dataset [ reference ];There are five subjects in the training set and ten subjects in the testing stage , including five unseen subjects .;Datasets;13
76422;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;;Switchboard dataset;We use the Switchboard dataset godfrey1997switchboard preprocessed by zhao2017learning to train a Seq2Seq sutskever2014sequence model with MoS added to the decoder RNN .;To show that the MoS is a generic structure that can be used to model other context - dependent distributions , we additionally conduct experiments in the dialog domain .;Then , a Seq2Seq model using Softmax and another one augmented by MoC with comparable parameter sizes are used as baselines .;Main Results;14
61554;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;celebrity +;Specifically , WebFace , celebrity + and CACD form the training set to train a CNN base model .;Secondly , for ensuring a fair comparison with other methods , we neglect the CAF dataset and conduct an experiment with the same training data as related work has used .;The trained model is later fine - tuned with Morph training data .;Experiments on the MORPH Album 2 Dataset;11
58788;23dcfda130aada27c158c0b5f394cac489c9c795;Material;1;;;AFLW2000;We run both of these landmark detectors on the AFLW2000 and BIWI datasets .;Dlib implements a landmark detector which uses an ensemble of regression trees and which is described in .;AFLW2000 images are small and are cropped around the face .;Fine - Grained Pose Estimation on the AFLW2000 and BIWI Datasets;10
53147;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;0;;;FGVC;We aim to solve both of these issues in FGVC ( overfitting and sample - specific artifacts ) by bringing the different class - conditional probability distributions closer together and confusing the deep network , subsequently reducing its prediction over - confidence , thus improving generalization performance .;As a result , the neural network may learn to latch on to sample - specific artifacts in the image , instead of learning a versatile representation for the target object .;"Let us formalize the idea of "" confusing "" the conditional probability distributions .";Method;4
63747;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;0;;;English corpora;They show promising results on their Chinese dataset using an encoder - decoder RNN , but do not report experiments on English corpora .;In another paper that is closely related to our work , hu:2015:EMNLP introduce a large dataset for Chinese short text summarization .;In another very recent work , jianpeng used RNN based encoder - decoder for extractive summarization of documents .;Related Work;7
61538;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;Morph Album 2;Table [ reference ] compares the rank 1 identification rates testing on 10 , 000 subjects of Morph Album 2 over Softmax , A - Softmax , and OE - CNNs , with and without CAF dataset .;Firstly , we compare the proposed approach to baseline algorithms that are most related to the proposed algorithm to demonstrate its effectiveness .;As shown in the table , The proposed OE - CNNs significantly outperforms both Softmax and A - Softmax under both settings .;Experiments on the MORPH Album 2 Dataset;11
89281;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;VOC 2012 training data;In this case , we first fine - tuned the plain FCN - 32s network ( without the CRF - RNN part ) on COCO data , then we built an FCN - 8s network with the learnt weights and finally train the CRF - RNN network end - to - end using VOC 2012 training data only .;The same reduced validation set was used in this second experiment as well .;Since the MS COCO ground truth segmentation data contains somewhat coarse segmentation masks where objects are not delineated properly , we found that fine - tuning our model with COCO did not yield significant improvements .;Pascal VOC Datasets;18
95542;3e95925d2bca43223453010ff8516a492287ce19;Material;0;;;WoZ.;The delexicalisation models , which replace slots and values in the utterance with generic tags , are from henderson2014word for DSTC2 and wen2017NetworkBasedEndToEndDialogueSystem for WoZ. Semantic dictionaries map slot - value pairs to hand - engineered synonyms and phrases .;Table [ reference ] shows the performance of GLAD compared to previous state - of - the - art models .;The NBT mrkvsic2016neural applies CNN over word embeddings learned over a paraphrase database wieting2015paraphrase instead of delexicalised n - gram features .;Comparison to Existing Methods;10
88792;38d7920f0e8a3a672ea37c8612b2b2947b9ba9d1;Material;0;;;ImageNet - 5k;It is difficult to fairly compare different detectors as they differ in backbone architectures ( like ResNet , ResNext , Xception ) , pre - training data ( e.g. ImageNet - 5k , JFT , OpenImages ) , different structures in the underlying network ( e.g multi - scale features , deformable convolutions , heavier heads , anchor sizes , path aggregation ) , test time augmentations like flipping , mask tightening , iterative bounding box regression etc .;;Therefore , we compare our results with SNIP , which is a recent method for training object detectors on an image pyramid .;Comparison with State - of - the - art;14
101902;42764b57d0794b63487a295ce8c07eeb6961477e;Material;1;;;PASCAL VOC 2012 trainval set;We train our model on the PASCAL VOC 2012 trainval set , and evaluate on the PASCAL VOC 2012 test set for object detection .;We are also interested in the box - level object detection performance ( mAP ) , so that we can compare with more systems that are designed for object detection .;Given mask - level instances generated by our model , we simply assign a tight bounding box to each instance .;Experiments on PASCAL VOC 2012;11
45790;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;1;;;WikiMovies dataset;We conducted experiments on the WikiMovies dataset described in Sec .;;[ reference ] .;WikiMovies;18
79940;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;PTC dataset;It again show a consistent performance gain of accuracy ( highest being on PTC dataset ) on many bioinformatic datasets when compared against with strong graph kernels .;Our GCAPS - CNN is also very competitive with state - of - art graph kernel methods .;While other considered deep learning methods are not even close enough to beat graph kernels on many of these datasets .;Experiment and Results;12
34726;1329206dbdb0a2b9e23102e1340c17bd2b2adcf5;Material;1;;;bird dataset;Following the protocol of , we use two semantic parts for the bird dataset : head and body .;We train and test on the splits included with the dataset , which contain around 30 training samples for each species .;We use the open - source package Caffe to extract deep features and fine - tune our CNNs .;Evaluation;11
55825;22aab110058ebbd198edb1f1e7b4f69fb13c0613;Material;na;;;validation sets;As a simple test for D ’s memorization ( related to ) , we evaluate uncollapsed discriminators on the ImageNet training and validation sets , and measure what percentage of samples are classified as real or generated .;One possible explanation for this behavior is that D is overfitting to the training set , memorizing training examples rather than learning some meaningful boundary between real and generated images .;While the training accuracy is consistently above 98 % , the validation accuracy falls in the range of 50 - 55 % , no better than random guessing ( regardless of regularization strategy ) .;Characterizing Instability : The Discriminator;8
101042;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;PASCAL VOC 2007 detection benchmark;We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark .;;This dataset consists of about 5k trainval images and 5k test images over 20 object categories .;Experiments on PASCAL VOC;11
78756;32a93598e8a338496f04a0ace81b0768c2ef059d;Material;1;;;Thai - English data;The Thai - English data comes from IWSLT 2015 .;The teacher model is a LSTM ( as in Luong2015 ) and we train two student models : and .;There are k sentences in the training set and we take 2010 / 2011 / 2012 data as the dev set and 2012 / 2013 as the test set , with a vocabulary size is;Experimental Setup;9
81422;34f63959ea4a13a05948274a1558c6854a051150;Material;0;;;Winograd Schema dataset;The Winograd NLI ( WNLI ) is a natural language inference dataset derived from the Winograd Schema dataset .;;This is a reading comprehension task .;WNLI;25
102220;42e80c73867bff9eaff6beceb8730fc1276283b9;Material;1;;;WMT 2014 test set;So as to put our results into perspective , Table [ reference ] reports the results of different supervised systems in the same WMT 2014 test set .;;"More concretely , we include the best results from the shared task itself , which reflect the state - of - the - art in machine translation back in 2014 ; those of vaswani2017attention , who introduced the now predominant transformer architecture ; and those of edunov2018understanding , who apply back - translation at a large scale and hold the current best results in the test set .";Comparison with supervised systems;11
35181;13ea9a2ed134a9e238d33024fba34d3dd6a010e0;Material;1;;;DukeMTMC - reID dataset;The DukeMTMC - reID dataset is collected with 8 cameras and used for cross - camera tracking .;For CUHK03 , 20 random train / test splits are performed , and the averaged results are reported .;We adopt its re - ID version benchmarked in .;Datasets and Settings;8
88219;380b2c78d21ae6c43d418b6f0cb0222d5293d345;Material;1;;;Spanish;We also include results with pretrained word embeddings for English , Chinese , German , and Spanish following the same training setup as Dyer et al .;We used predicted part - of - speech tags provided by the CoNLL 2009 shared task organizers .;"( 2015 ) ; for English and Chinese we used the same pretrained word embeddings as in Table [ reference ] , for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3 .";Experiments;7
69449;2cb8497f9214735ffd1bd57db645794459b8ff41;Material;0;;;Daily Mail corpus;To understand that distinction consider for instance the following Cloze form queries ( created from headlines in the Daily Mail validation set ) : An ngram language model trained on the Daily Mail would easily correctly predict that ( X = cancer ) , regardless of the contents of the context document , simply because this is a very frequently cured entity in the Daily Mail corpus .;’s ability to read and comprehend a single document , not world knowledge or co - occurrence .;To prevent such degenerate solutions and create a focused task we anonymise and randomise our corpora with the following procedure , Compare the original and anonymised version of the example in Table [ reference ] .;Entity replacement and permutation;3
67150;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;spatially invariant AWGN;In this subsection , we test FFDNet on noisy images corrupted by spatially invariant AWGN .;;For grayscale image denoising , we mainly compare FFDNet;Experiments on AWGN Removal;15
54255;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;CoNLL;To distinguish between these two annotation settings , we use CoNLL - 2014 to denote the original annotations , and CoNLL - 10 to denote the 10 - human annotations .;We evaluate systems ’ performance using both annotation settings for the CoNLL dataset .;As previous studies , we use CoNLL - 2013 test set and JFLEG dev set as our development sets for CoNLL - 2014 and JFLEG test set respectively .;Dataset and evaluation;12
89288;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;VOC 2010;We also evaluated our models on the VOC 2010 , and VOC 2011 test set ( see Table 2 ) .;Note that in both setups , our approach outperforms competing methods due to the end - to - end training of the CNN and CRF in the unified CRF - RNN framework .;In all cases our method achieves the stateof - the - art performance .;Pascal VOC Datasets;18
46176;1b9472907f5b7a1815c98b4562dce6c46dd2cf34;Material;0;;;MORPH - 2;The CACD database was preprocessed similar to MORPH - 2 such that the faces spanned the whole image with the nose tip being in the center .;The age labels used in this study ranged between 16 - 70 years .;The total number of images is 159 , 449 in the age range 14 - 62 years .;MORPH - 2 and CACD .;19
54227;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;CoNLL - 2014;The powerful learning and inference mechanism enables our system to achieve state - of - the - art results and reach human - level performance on both CoNLL - 2014 and JFLEG benchmark datasets .;Fluency boost learning fully exploits both error - corrected data and native data by generating diverse error - corrected sentence pairs during training , which benefits model learning and improves the performance over the base seq2seq model , while fluency boost inference utilizes the characteristic of GEC to progressively improve a sentence ’s fluency through round - way correction .;;Conclusion;16
27338;0ee850dd6640a96531ac5ad21da5438db04d8b3c;Material;1;;;TREC Disks;In our experiments , we used two standard TREC collections : The first collection ( called Robust04 ) consists of over 500k news articles from different news agencies , that is available in TREC Disks 4 and 5 ( excluding Congressional Records ) .;Collections .;This collection , which was used in TREC Robust Track 2004 , is considered as a homogeneous collection , because of the nature and the quality of documents .;Data;9
97953;4087ebc37a1650dbb5d8205af0850bee74f3784b;Material;1;;;WikiText 2;The CBS ensembles on PTB and WikiText 2 result in test set perplexity of 76.14 and 88.47 , outperforming baseline ensembles on both datasets ( 76.52 , 89.99 respectively ) and CBS single models ( 77.39 , 91.78 respectively ) .;We perform experiments on snapshot ensembling with the L2 model with the respective best performing CBS schedules on PTB and WikiText 2 ( CBS - 1 - T and CBS - 1 ) , as well as the fixed batch size baseline .;To further explore the properties of cyclical batch size schedules , we also evaluate these schedules on natural language inference tasks , as shown in Table 2 .;Language Results;6
66448;2ad7cef781f98fd66101fa4a78e012369d064830;Material;1;;;IJB - A dataset;section : Results on IJB - A dataset;These two methods as well as our NAN produce a 128 - d feature representation for each video and compute the similarity in O ( 1 ) time .;The IJB - A dataset [ reference ] contains face images and videos captured from unconstrained environments .;Results on IJB - A dataset;16
86171;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;1;;;MSRA;Note that our model outperforms previous works by a large margin without epoch ensemble on the ICVL , NYU , MSRA , and ITOP datasets while running in real - time using a single GPU .;The next step is network forwarding , which takes 5 ms and takes 0.5 ms to extract 3D coordinates from the 3D heatmap .;;Computational complexity;17
79041;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;1;;;Adult dataset;For the Adult dataset both encoders , for and , and both decoders , for and , had one hidden layer of 100 units .;;For the Health dataset we had one hidden layer of 300 units for the encoder and decoder and one hidden layer of 150 units for the encoder and decoder .;Experimental Setup;10
47352;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;1;;;VOC 2010 dataset;While the VOC 2007 dataset consists of 2501 training , 2510 validation , and 5011 test images containing bounding box annotations for 20 object categories , VOC 2010 dataset contains 4998 training , 5105 validation , and 9637 test images for the same number of categories .;We evaluate our method on the PASCAL VOC 2007 and 2010 datasets [ reference ] , as they are the most widely - used benchmark in weakly supervised object detection .;We use the suggested training and validation splits and report results evaluated on test split .;Benchmark data .;10
65107;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;;;Nordic languages;Slavic and Nordic languages ) .;, it improves over TnT on 9 languages ( incl .;The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre - trained embeddings .;Results;7
99284;41232a69c0f8d4b993e6c6e00b16c223442c962f;Material;1;;;Gigaword benchmark dataset;Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80 % .;The dual - attention sequence - to - sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions .;Notably , the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text .;Faithful to the Original : Fact Aware Neural Abstractive Summarization;0
19242;0a6d7e8e61c54c796f53120fdb86a25177e00998;Material;1;;;WN18 datasets;We next evaluate the performance of our model on the FB15 K and WN18 datasets .;;FB15 K is a subset of Freebase , a curated KB of general facts , whereas WN18 is a subset of Wordnet , a database featuring lexical relations between words .;Datasets : FB15 K and WN18;8
50192;1e5b9e512c01e244287fe7afb05e03c96d5c1cd0;Material;1;;;CoNLL Shared Task 2017;For the experiments , we use the data sets as provided by the CoNLL Shared Task 2017;;[ reference ] . For training , we use the training sets which were denoted as big treebanks 2 .;Data Sets;12
40538;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;Switchboard data;The more informal CallHome data has almost double the human error rate of the Switchboard data .;Perhaps the most important point is the extreme variability between the two test subsets .;Interestingly , the same informality , multiple speakers per channel , and recording conditions that make CallHome hard for computers make it difficult for people as well .;Human Performance;2
91471;3b1b94441010615195a5c404409ce2416860508c;Material;1;;;SPARQL query;Since the text returned by the SPARQL query is typically much longer than the captions generated in the section [ reference ] , we turn to Doc2Vec to extract the semantic meanings .;Figure [ reference ] shows an example of the query language and returned text .;Doc2Vec , also known as Paragraph Vector , is an unsupervised algorithm that learns fixed - length feature representations from variable - length pieces of texts , such as sentences , paragraphs , and documents .;Relating to the Knowledge Base;10
91888;3b1d8eb163ffff598c2faa0d9d7cf933857a359f;Material;0;;;Freebase (;"Note that in addition to relation embedding trained on WordNet , other relational embedding resources exist ; e.g. , that trained on Freebase ( WikiData ) DBLP : conf / aaai / BollackerCT07 , but such knowledge resources are mainly about facts ( e.g. , relationship between Bill Gates and Microsoft ) and are less for commonsense knowledge used in general natural language inference ( e.g. , the color yellow potentially contradicts red ) .";’s word embedding ( trained on WordNet ) to obtain relation embedding , through the objective function used in TransE , i.e. , , where indicates relation embedding , indicates tail entity embedding , and indicates head entity embedding .;;Relation Embeddings;12
45675;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;1;;;reading Wikipedia;Our experiments on WikiMovies indicate that , thanks to its key - value memory , the KV - MemNN consistently outperforms the original Memory Network , and reduces the gap between answering from a human - annotated KB , from an automatically extracted KB or from directly reading Wikipedia .;This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .;We confirm our findings on WikiQA , another Wikipedia - based QA benchmark where no KB is available , where we demonstrate that KV - MemNN can reach state - of - the - art results;Introduction;1
18416;0a3a003457f5d7758a42a0e4b7278b39a86ed0bd;Material;0;;;miniImageNet;miniImageNet was proposed by Vinyals et al . for few - shot learning evaluation .;FC100 is newly proposed in and is more challenging in terms of lower image resolution and stricter training - test splits than miniImageNet .;Its complexity is high due to the use of ImageNet images , but requires less resource and infrastructure than running on the full ImageNet dataset .;Datasets and implementation details;10
32622;10f62af29c3fc5e2572baddca559ffbfd6be8787;Material;1;;;TREC;For this task , we use the benchmark TREC .;” is a question that belongs to “ location ” .;TREC divides all questions into 6 categories , including location , human , entity , abbreviation , description and numeric .;Datasets;10
31945;1023b20d226bd0af9fdf0fd1847accefbfa5ec84;Material;0;;;CNN;CNN and Daily Mail .;Table [ reference ] then measures accuracy as the proportion of test cases where the ground truth was among the top answers proposed by the greedy ensemble model for .;blackThe CNN dataset is the most widely used dataset for evaluation of text comprehension systems published so far .;Results;20
95838;3f3a483402a3a2b800cf2c86506a37f6ef1a5332;Material;0;;;LSP Extended;“ LSP Extended ” ( LSPET ) , and “ MPII Human Pose ”;“ Leeds Sports Poses ” ( LSP ) ( person - centric ( PC ) ) ,;( “ Single Person ” ) .;Evaluation of Part Detectors;11
65106;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;;;Slavic;Slavic and Nordic languages ) .;, it improves over TnT on 9 languages ( incl .;The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre - trained embeddings .;Results;7
40666;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;in - domain;In order to make use of LM training data that is not fully matched to the target conversational speech domain , we start RNN - LM training with the union of in - domain ( here , CTS ) and out - of - domain ( e.g. , Web ) data .;The two RNN - LMs and the N - gram LM for each direction are interpolated with weights of ( 0.375 , 0.375 , 0.25 ) .;Upon convergence , the network undergoes a second training phase using the in - domain data only .;RNN - LM setup;10
89252;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;Pascal Context dataset;We use these datasets : the Pascal VOC 2012 dataset , and the Pascal Context dataset .;We present experimental results with the proposed CRF - RNN framework .;We use the Pascal VOC 2012 dataset as it has become the golden standard to comprehensively evaluate any new semantic segmentation approach in comparison to existing methods .;Experiments;17
79022;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;1;;;domain - adaptation ” literature;Furthermore , we also experimented with the Amazon reviews dataset to make a connection with the “ domain - adaptation ” literature .;In these datasets the “ nuisance ” or sensitive variable is significantly correlated with the label thus making the proper removal of challenging .;"Finally , we also experimented with a more general task on the extended Yale B dataset ; that of learning invariant representations .";Experiments;8
97139;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;PTB ) Marcus1993BuildingAL;POS tagging To examine whether the biLM captures basic syntax , we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) Marcus1993BuildingAL .;"The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher overall performance at the second layer compared to the first ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet first sense baseline .";As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM ’s representations .;What information is captured by the biLM ’s representations ?;12
74115;2ebfc12285f5d426e0d0e8d2befa1af27f99a56e;Material;0;;;Weizmann Horse dataset;WH - SYMMAX contains 328 cropped images from the Weizmann Horse dataset , with skeleton annotations .;SK506 ( aka SK - SMALL ) , is an earlier version of SK - LARGE containing 300 train images and 206 test images .;It is split into 228 train images and 100 test images .;Dataset and evaluation protocol;9
94111;3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a;Material;1;;;VOC07;On VOC07 , we compare Fast R - CNN to R - CNN and SPPnet .;;All methods start from the same pre - trained VGG16 network and use bounding - box regression .;VOC 2007 results;16
101280;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;0;;;Large - scale data;Large - scale data is of crucial importance for improving deep neural networks .;;Next , we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC .;From MS COCO to PASCAL VOC;13
62967;2788a2461ed0067e2f7aaa63c449a24a237ec341;Material;0;;;Market - 1501;Similar to Market - 1501 , it contains 16 , 522 training images of 702 identities , 2 , 228 query images of the other 702 identities and 17 , 661 gallery images .;DukeMTMC - reID contains 36 , 411 images of 1 , 812 identities shot by 8 high - resolution cameras .;CUHK03 contains 14 , 096 images of 1 , 467 identities .;Datasets;3
56894;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;CIFAR - 10 training dataset;The Large - scale CelebFaces Attributes ( CelebA ) dataset , aligned and cropped , the training dataset of the bedrooms category of the large scale image database ( LSUN ) , the CIFAR - 10 training dataset , the Street View House Numbers training dataset ( SVHN ) , and the One Billion Word Benchmark .;We used the following datasets to evaluate GANs :;All experiments rely on the respective reference implementations for the corresponding GAN model .;Used Software , Datasets , Pretrained Models , and Implementations;67
36982;151313065d71b49dbf07289c002c887d7b5a0a6b;Material;1;;;Company;Compared to these two models , DeepFM achieves more than 0.48 % and 0.33 % in terms of AUC ( 0.61 % and 0.66 % in terms of Logloss ) on Company and Criteo datasets .;DeepFM outperforms the models that learn high - and low - order feature interactions using separate feature embeddings ( namely , LR & DNN and FM & DNN ) .;Overall , our proposed DeepFM model beats the competitors by more than 0.37 % and 0.42 % in terms of AUC and Logloss on Company dataset , respectively .;Effectiveness Comparison;15
40496;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;NIST eval 2000 test set;This paper is an expanded version of , with the following additional material : A comprehensive assessment of human performance on the NIST eval 2000 test set The description of a novel spatial regularization method which significantly boosts our LSTM acoustic model performance;In the meantime , ensemble learning has become commonly used in several neural models , to improve robustness by reducing bias and variance .;The use of LSTM rather than RNN - LMs , and the use of a letter - trigram input representation A two - level system combination , based on a subsystem of BLSTM - system variants that , by itself , surpasses the best previously reported results Expanded coverage of the Microsoft Cognitive Toolkit ( CNTK ) used to build our models An analysis of human versus machine errors , which indicates substantial equivalence , with the exception of the word classes of backchannel acknowledgments ( e.g. “ uh - huh ” ) and hesitations ( e.g.;Introduction;1
72430;2dc32f9e0a7870b272a2a51082202a9fa52fb854;Material;0;;;manga109;"Among these datasets , Set5 , Set14 and BSDS100 consist of natural scenes ; Urban100 contains challenging urban scenes images with details in different frequency bands ; and manga109 is a dataset of Japanese manga .";Urban100 and manga109 .;We train the LapSRN until the learning rate decreases to and the training time is around three days on a Titan X GPU .;Comparisons with the state - of - the - arts;14
67269;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;ground - truth clean image;Both the ground - truth clean image and noise level are unknown for real noisy image .;( i );( ii );Experiments on Real Noisy Images;18
107939;462d4e265c9cbe9ad5feeb9a7736184a90b36fed;Material;1;;;U.S. census 2010;The PERSON gazetteer is collected from U.S. census 2000 , U.S. census 2010 and DBpedia whereas GeoNames is the main source for LOCATION , taking in both official and alternative names .;For the gazetteer feature , we focus on PERSON and LOCATION and compile a list for each .;All the tokens on both lists are then filtered to exclude frequently occurring common words .;Hand - crafted Features;8
26951;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;;released games;All of these consoles have hundreds of released games , and older platforms have readily available emulators .;A natural progression , for example , would be to move on to the Commodore 64 , then to the Nintendo , and so forth towards current generation consoles .;With the ultra - realism of current generation consoles , each console represents a natural stepping stone toward general real - world competency .;Final Remarks;35
67216;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;Kodak24 , and McMaster datasets;Table [ reference ] reports the performance of different methods on CBSD68 , Kodak24 , and McMaster datasets , and Fig .;For color image denoising , we compare FFDNet with CBM3D and CDnCNN .;[ reference ] presents the visual comparisons .;Experiments on AWGN Removal;15
57180;2329a46590b2036d508097143e65c1b77e571e8c;Material;1;;;Fisher corpora;Since the Switchboard and Fisher corpora are distributed at a sample rate of 8kHz , we compute spectrograms of 80 linearly spaced log filter banks and an energy term .;Using the techniques mentioned in Section [ reference ] our system is able perform a full pass over the 2300 hours of data in just a few hours .;The filter banks are computed over windows of 20ms strided by 10ms .;Conversational speech : Switchboard Hub5’00 ( full );13
57799;23ae5fa0e8d581b184a8749d764d2ded128fd87e;Material;0;;;SVHN dataset;The relatively larger SVHN dataset provides the sole exception ( SVHN has 600k training images versus 50k for MNIST , CIFAR10 , and CIFAR100 );/ rg / ch “ non - responsive ” mixed max - avg strategy .;we found baseline 1.91 % , 50 / 50 mix 1.84 % , mixed ( 1 per lyr ) 1.76 % , mixed ( 1 per lyr / ch / rg ) 1.64 % , and gated ( 1 per lyr ) 1.74 % .;Quick comparison : mixed and gated pooling;7
20269;0b544dfe355a5070b60986319a3f51fb45d1348e;Material;1;;;WMT’14 workshop;We evaluate our approach on the English / French translation task of the WMT’14 workshop .;;;Experiments;9
89902;3a28fe49e7a856ddd60d134696a891ed7bca5962;Material;1;;;Citypersons;We examine the proposed approach on widely used benchmarks including Caltech and Citypersons .;;We follow the standard evaluation metric : log miss rate is averaged over the false positive per image ( FPPI ) in [ ] , denoted as MR .;Experiment Settings;13
82672;357776cd7ee889af954f0dfdbaee71477c09ac18;Material;na;;;NVIDIA;We also thank NVIDIA for GPU donations .;We thank the developers of TensorFlow tensorflow2015 - whitepaper , which we used for all of our experiments .;;Acknowledgments;13
25824;0dcde9f2c5149f0e4c806db7b4cc4915bed077da;Material;1;;;Google Street View House Numbers;We apply our network to several image classification databases , including MNIST , CIFAR - 10 , Google Street View House Numbers ( SVHN ) and NORB .;Therefore , we have devised a network architecture ( see Figure [ reference ] ) that consists of three key elements that work together to ensure fast learning and good classification performance : namely , the use of ( a ) convolutional feature extraction , ( b ) random - valued input weights for classification , ( c ) least squares training of output weights that feed in to ( d ) linear output units .;The network produces state - of - the - art classification results on MNIST and NORB - small databases and near state - of - the - art performance on SVHN .;Introduction;1
97357;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;CoNLL 2003 NER data set;As the CoNLL 2003 NER data set is relatively small , we found the best performance by constraining the trainable layer weights to be effectively constant by setting with ( [ reference ] ) .;ELMo was added to the input of the lowest layer task biLSTM .;Table [ reference ] compares test set F scores of our ELMo enhanced biLSTM - CRF tagger with previous results .;Named Entity Recognition;24
27885;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;1;;;‘ Photo ’ and ‘ Art ’ style object depictions;Addressing these aspects enables us to achieve state - of - the - art results on a challenging dataset containing ‘ Photo ’ and ‘ Art ’ style object depictions .;A key aspect of SwiDeN is the ‘ deep ’ depictive style - based switching mechanism which judiciously addresses depiction - specific and depiction - invariant aspects of the problem .;In future , we plan to explore unsupervised network learning approaches .;Conclusion;16
87346;372bc106c61e7eb004835e85bbfee997409f176a;Material;1;;;CelebFaces Attributes dataset;We used the CelebFaces Attributes dataset for the experiments .;We trained several CoGANs , each for generating a face with an attribute and a corresponding face without the attribute .;The dataset covered large pose variations and background clutters .;Experiments;4
105037;44da806ae67ae9885592492202b3dc5f50182cc8;Material;0;;;ICDAR 2017 MLT;Similarly with ICDAR 2015 , the text regions in ICDAR 2017 MLT are also annotated by 4 vertices of the quadrangle .;The dataset is composed of complete scene images which come from 9 languages .;SCUT - CTW1500 is a challenging dataset for curve text detection , which is constructed by Yuliang et al . .;Benchmark Datasets;10
76912;309acdd149f5f0ea12acb103b36bb59e6e631671;Material;1;;;Leeds dataset;Figure [ reference ] shows some qualitative results on the MPII dataset and on the Leeds dataset , including failure cases .;The proposed approach trained exclusively on the Human3.6 M dataset can be used to identify 2D and 3D landmarks of images contained in different datasets .;Notice how the probabilistic 3D pose model generates anatomically plausible poses even though the 2D landmark estimations are not all correct .;MPII and Leeds datasets :;16
40671;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;in - domain validation data;Both training phases use in - domain validation data to regulate the learning rate schedule and termination .;Upon convergence , the network undergoes a second training phase using the in - domain data only .;Because the size of the out - of - domain data is a multiple of the in - domain data , a standard training on a simple union of the data would not yield a well - matched model , and have poor perplexity in the target domain .;RNN - LM setup;10
107670;46018a894d533813d67322827ca51f78aed6d59e;Material;1;;;brain tumor segmentation challenge;The experiments were carried out on real patient data obtained from the 2013 brain tumor segmentation challenge ( BRATS2013 ) , as part of the MICCAI conference .;;The BRATS2013 dataset is comprised of 3 sub - datasets .;Experiments and Results;13
103530;434bf475addfb580707208618f99c8be0c55cf95;Material;0;;;MMI Dataset;In the MMI Dataset ( Fig .;;[ reference ] );Comparison;13
96802;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;monolingual data;In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences Chelba2014OneBW .;Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .;We also generalize these approaches to deep contextual representations , which we show work well across a broad range of diverse NLP tasks .;Related work;2
67081;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;source images;We collected a large dataset of source images , including 400 BSD images , 400 images selected from the validation set of ImageNet , and the 4 , 744 images from the Waterloo Exploration Database .;, we use the matlab function imnoise to generate the quantized noisy image from a clean one .;In each epoch , we randomly crop patches from these images for training .;Dataset Generation and Network Training;14
2489;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;;ImageNet subset;Next we evaluate our models on a larger ImageNet subset that has 5000 categories .;But we argue that this is not because of the capability of the models but because of the complexity of the dataset .;Our 5 K dataset is a subset of the full ImageNet - 22 K set [ reference ] .;Experiments on ImageNet - 5 K;13
23564;0cb8f50580cc69191144bd503e268451ce966fa6;Material;1;;;QM9 Dataset;section : QM9 Dataset;Finally define a message passing process on graphs which is run until convergence , instead of for a finite number of time steps as in MPNNs .;To investigate the success of MPNNs on predicting chemical properties , we use the publicly available QM9 dataset .;QM9 Dataset;5
74180;2ebfc12285f5d426e0d0e8d2befa1af27f99a56e;Material;1;;;SYM - PASCAL;We study the contribution of the two main modules ( ASPP module and flux representation ) to skeleton detection on SK - LARGE and SYM - PASCAL .;;We first remove the ASPP module and study the effect of the proposed context flux representation compared to a baseline model with the same architecture , but trained for binary classification .;Ablation study;13
57583;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;CoQA ReddyCM18;"Moreover , DuoRC KhapraSSA18 and CoQA ReddyCM18 contain abstractive answers ; most of the answers are short phrases .";The documents are long , averaging 62 , 528 ( 659 ) words in stories ( summaries ) , while the answers are relatively short , averaging 4.73 words .;;RC with NLG .;45
60714;258ec208f9c55371a67ebac68aa51bd7f7800a7b;Material;0;;;LIVE;The average PSNR and SSIM for font size 10 and 20 on LIVE are : 38.24dB , 0.9869 and 34.99dB , 0.9828 using 30 - layer RED - Net , and they are much better than those of FoE , which are 34.59dB , 0.9762 and 31.10dB , 0.9510 .;For FoE , we provide both images with text and masks indicating which pixel is corrupted .;For scratch removal , we randomly draw scratch on the clean image and test with our network and FoE. The PSNR and SSIM for our network are 39.41dB and 0.9923 , which is much better than 32.92dB and 0.9686 of FoE. Figure [ reference ] shows some visual comparisons of our method between FoE. We can observe from the examples that our network is better at recovering text , logos , faces and edges in the natural images .;Image inpainting;21
61556;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;Morph training data;The trained model is later fine - tuned with Morph training data .;Specifically , WebFace , celebrity + and CACD form the training set to train a CNN base model .;Table [ reference ] depicts our result compared with other methods .;Experiments on the MORPH Album 2 Dataset;11
60710;258ec208f9c55371a67ebac68aa51bd7f7800a7b;Material;1;;;LIVE1 dataset;Text is added to the original image from the LIVE1 dataset with font size of 10 and 20 .;In this section , we conduct text removal for experiments of image inpainting .;We have compared our method with FoE .;Image inpainting;21
86050;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;0;;;NYU Hand Pose Dataset;NYU Hand Pose Dataset .;The annotation of hand pose contains 16 joints , which include three joints for each finger and one joint for the palm .;The NYU dataset [ reference ] consists of 72 K training and 8.2 K testing depth images .;Datasets;13
57348;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;training data;Generative models such as S - Net TanWYDLZ18 suffer from a dearth of training data to cover open - domain questions .;While datasets such as MS MARCO Bajaj18 have been proposed for providing abstractive answers in natural language , the state - of - the - art methods WuWLHWLLL18 , YanAAAI19 are based on answer span extraction , even for the datasets .;Moreover , to satisfy various information needs , intelligent agents should be capable of answering one question in multiple styles , such as concise phrases that do not contain the context of the question and well - formed sentences that make sense even without the context of the question .;Introduction;1
37125;1518039b5001f1836565215eb047526b3ac7f462;Material;1;;;WMT 2015;We perform experiments on data from the shared translation task of WMT 2015 .;Which segmentation into subword units performs best in terms of vocabulary size , text size , and translation quality ?;For English→German , our training set consists of 4.2 million sentence pairs , or approximately 100 million tokens .;Evaluation;7
2428;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;;1000 - class ImageNet classification task;We conduct ablation experiments on the 1000 - class ImageNet classification task [ reference ] .;;We follow [ reference ] to construct 50 - layer and 101 - layer residual networks .;Experiments on ImageNet - 1 K;11
56420;231af7dc01a166cac3b5b01ca05778238f796e41;Material;na;;;Images;Images with meaningful objects are supposed to have low label ( output ) entropy , that is , they belong to few object classes .;Generated samples are fed into an inception model that was trained on ImageNet .;On the other hand , the entropy across images should be high , that is , the variance over the images should be large .;Performance Measure .;8
21540;0be9ca65ad318ee3729928882ef2c403d4b6d24e;Material;0;;;preprocessed PTB;/ MerityXBS16 respectively published preprocessed PTB and WikiText - 2 datasets .;journals / corr;Table [ reference ] describes their statistics .;Datasets;6
87600;37a18be8c599b781cc28b6a62d8f11e8a6a75169;Material;0;;;BraTS 2018 training dataset;This year , BraTS 2018 training dataset included 285 cases ( 210 HGG and 75 LGG ) , each with four 3D MRI modalities ( T1 , T1c , T2 and FLAIR ) rigidly aligned , resampled to 1x1x1 mm isotropic resolution and skull - stripped .;Multimodal Brain Tumor Segmentation Challenge ( BraTS ) aims to evaluate state - of - the - art methods for the segmentation of brain tumors by providing a 3D MRI dataset with ground truth tumor segmentation labels annotated by physicians .;The input image size is 240x240x155 .;Introduction;1
77998;31e5dab321066712cdc8b30943f7950066840ee1;Material;1;;;LDC2015E86 dataset;Table [ reference ] shows the comparison on the development split of the LDC2015E86 dataset between sequential , tree and graph encoders .;We first evaluate the overall performance of the models , after which we focus on two phenomena that we expect to benefit most from structural encoders : reentrancies and long - range dependencies .;The sequential encoder ( Seq ) is a re - implementation of konstas2017neural .;Experiments;13
73289;2e57bccb74bcb46cbc5b4225b62679023ed1f9da;Material;0;oder 1 wegen Abstract?;;structured Graph Reachability dataset;ReasoNets achieve superior performance in machine comprehension datasets , including unstructured CNN and Daily Mail datasets , the Stanford SQuAD dataset , and a structured Graph Reachability dataset .;With the use of reinforcement learning , ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results , or to terminate reading when it concludes that existing information is adequate to produce an answer .;;ABSTRACT;1
27738;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;na;;;Photo;( c ) ) , henceforth referred to as Switch , that determines the depiction style of the input image and passes the image to corresponding depiction sub - network ( Photo or Art ) .;To realize the switching mechanism mentioned in Section [ reference ] , we design and train a switch network ( see Figure [ reference ];The Switch has two convolution layers which capture depiction - discriminative features such as edges , textures , corners , colors and their conjunctions .;Switch;5
86628;36c3972569a6949ecca90bfa6f8e99883e092845;Material;1;;;VisDial datasets;Since both VG and VisDial datasets only have a single ground - truth answer while VQA has 10 , we simply replicated the answer to each question in VG and VisDial 10 times to make the data format compatible with the VQA evaluation protocol .;For VisDial , we converted the 10 turns in a dialog to 10 independent question - answer pairs .;We also performed additional data augmentation by mirroring the images in the VQA dataset .;Data Augmentation;6
72691;2e10643c3759f97b673ff8c297778c0b6c20032b;Material;1;;;SogouCA;This dataset is a combination of the SogouCA and SogouCS news corpora , containing in total 2 , 909 , 551 news articles in various topic channels .;Sogou news corpus .;We then labeled each piece of news using its URL , by manually classifying the their domain names .;Large - scale Datasets and Results;11
103880;435259c5f3cffd75ef837a8e638cc8f6244e25c4;Material;na;;;iSEG - 2017 organizers;The manual labels were prepared by the iSEG - 2017 organizers .;;Instead of starting from scratch , an initial automatic segmentation for 6 - month subjects was generated with the guidance from follow - up 24 - month scans with high tissue contrast , using the publicly - available iBEAT tool .;Ground truth generation;12
44476;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;monolingual News corpus;We experimented with using automatic back - translations of the monolingual News corpus as additional training data , pervasive dropout , and target - bidirectional models .;Our systems are based on an attentional encoder - decoder , using BPE subword segmentation for open - vocabulary translation with a fixed vocabulary .;All reported methods give substantial improvements , and we see improvements of 4.3–11.2 Bleu over our baseline systems .;Edinburgh Neural Machine Translation Systems for WMT 16;0
43557;19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7;Material;1;;;Pong;We selected hyperparameter values by performing an informal search on the games of Breakout , Pong and Seaquest which were then fixed for all the games .;The random agent selected actions uniformly at random at 10Hz and it was evaluated using the same starting states as the agents for both kinds of evaluations ( null op starts and human starts ) .;We have trained Gorila DQN 5 times on each game using the same fixed hyperparameter settings and random network initializations .;Evaluation;12
55920;22aab110058ebbd198edb1f1e7b4f69fb13c0613;Material;1;;;Google TPU v3 Pod;We train on a Google TPU v3 Pod , with the number of cores proportional to the resolution : 128 for 128 128 , 256 for 256 256 , and 512 for 512 512 .;Spectral Normalization miyato2018spectral is used in both G and D , following SA - GAN zhang2018sagan .;Training takes between 24 and 48 hours for most models .;Experimental Details;18
103135;43428880d75b3a14257c3ee9bda054e61eb869c0;Material;0;;;WMT'14 English - French;WMT'14 English - French .;As vocabulary we use 40 K sub - word types based on BPE .;We use the full training set of 36 M sentence pairs , and remove sentences longer than 175 words as well as pairs with a source / target length ratio exceeding 1.5 .;3;13
40674;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;na;;;in - domain data;Because the size of the out - of - domain data is a multiple of the in - domain data , a standard training on a simple union of the data would not yield a well - matched model , and have poor perplexity in the target domain .;Both training phases use in - domain validation data to regulate the learning rate schedule and termination .;We found best results with an RNN - LM configuration that had a second , non - recurrent hidden layer .;RNN - LM setup;10
32066;10a36dea0167511b66deca65fdca978aa9afdb11;Material;1;;;COCO val2014;To generate the training set and validation set for our model , we first randomly split the images of COCO val2014 into 70 % subset A and 30 % subset B. To avoid potential overfitting , questions sharing the same image will be placed into the same split .;Here train2014 and val2014 are the standard splits of the image set in the COCO dataset .;The question - answer pairs from the images of COCO train2014 + val2014;Experiments;3
107756;46018a894d533813d67322827ca51f78aed6d59e;Material;0;;;BRATS’13;The ground truth for the rest of the training brains is generated by a voted average of segmented results of the top performing methods in BRATS’13 and BRATS’12 .;BRATS’15 contains the training data of 2013 .;Some of these automatically generated ground truths have been refined manually by a user .;Cascaded architectures;15
107674;46018a894d533813d67322827ca51f78aed6d59e;Material;0;;;BRATS 2014 dataset;Please note that we could not use the BRATS 2014 dataset due to problems with both the system performing the evaluation and the quality of the labeled data .;As mentioned before during the first phase training , the distribution of examples introduced to the model from all 5 classes is uniform .;For these reasons the old BRATS 2014 dataset has been removed from the official website and , at the time of submitting this manuscript , the BRATS website still showed : “ Final data for BRATS 2014 to be released soon ” .;Experiments and Results;13
15514;08d55271589f989d90a7edce3345f78f2468a7e0;Material;1;;;YouTube Face Database;For face verification , we train our base model on extended version of VGG Face dataset , in which we extend the identity number from 2.6 K to 90 K and image number from 2.6 M to 5M. The model is evaluated on YouTube Face Database and IARPA Janus Benchmark A ( IJB - A ) dataset .;Datasets .;YouTube Face contains 3425 videos of 1595 identities .;Unconstrained face verification;13
48629;1d5d0a41b720bc51fd568cf78f8aa4ec5af4f802;Material;1;;;Car only;Table [ reference ] shows the results on models that are trained on Car only , with the exception of final ( all - class ) , which is trained on all categories , and Table [ reference ] shows the results of models that are trained on all categories .;We use the official evaluation protocol for the KITTI dataset , i.e. , the 3D IoU thresholds are 0.7 , 0.5 , 0.5 for Car , Cyclist , Pedestrian respectively .;;3D Localization AP in KITTI;27
27032;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;;Atari 2600 game programmers;Atari 2600 game programmers often used these bits not as individual values , but as part of 4 - bit or 8 - bit words .;The first part of the generated feature vector simply includes the 1024 bits of RAM .;Linear function approximation on the individual bits can capture the value of these multi - bit words .;RAM - based Feature Generation;42
87377;372bc106c61e7eb004835e85bbfee997409f176a;Material;0;;;MNIST digits;We resized them to have the same resolution as the MNIST digits .;The USPS digits have a different resolution .;We employed the CoGAN used for the digit generation task .;Applications;5
54073;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;public Lang - 8 Corpus;As previous studies ji2017nested , we use the public Lang - 8 Corpus mizumoto2011mining , tajiri2012tense , Cambridge Learner Corpus ( CLC ) nicholls2003cambridge and NUS Corpus of Learner English ( NUCLE ) dahlmeier2013building as our original error - corrected training data .;;Table [ reference ] shows the stats of the datasets .;Dataset and evaluation;12
2857;01959ef569f74c286956024866c1d107099199f7;Material;1;;;VQA test - dev;Table 4 shows the accuracy of different ablated versions of our best model ( deeper LSTM Q + norm I ) for both the openended and multiple - choice tasks on the VQA test - dev for real images .;It is interesting to see that the relative proportions of different age groups is consistent across all accuracy bins with questions belonging to the age group 5 - 8 comprising the majority of the predictions which is expected because 5 - 8 is the most common age group in the dataset ( see Fig . 7 ) .;The different ablated versions are as follows - 1 );Results;15
21832;0c278ecf472f42ec1140ca2f1a0a3dd60cbe5c48;Material;1;;;Atari 2600 games;Finally , we extend these ideas to a deep RL algorithm and show that it achieves state - of - the - art performance in Atari 2600 games .;We use this result to introduce an algorithm that performs as well as some theoretically sample - efficient approaches .;Count - BasedExplorationwiththeSuccessorRepresentation;Title;0
44594;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;Russian data;We apply the Latin BPE operations to the English data ( training data and input ) , and both the Cyrillic and Latin BPE operations to the Russian data .;We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the BPE operations on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .;Translation results are shown in Table [ reference ] .;English Russian;12
45610;1abf6491d1b0f6e8af137869a01843931996a562;Material;0;;;SiftFlow;pool6 , it instantly helps improve by about , which means that context is useful here as opposed to the observation in SiftFlow .;As shown in Table [ reference ] , by adding global context;Context becomes more important proportionally to the image size .;Combining Local and Global Features;9
82811;357776cd7ee889af954f0dfdbaee71477c09ac18;Material;na;;;TFD;A video showing the learnt TFD manifold can be found at .;shows samples drawn from the adversarial autoencoder trained on these datasets .;To determine whether the model is over - fitting by copying the training data points , we used the last column of these figures to show the nearest neighbors , in Euclidean distance , to the generative model samples in the second - to - last column .;Likelihood Analysis of Adversarial Autoencoders;7
56509;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;LSUN Bedrooms dataset;We test TTUR for the deep convolutional GAN ( DCGAN ) at the CelebA , CIFAR - 10 , SVHN and LSUN Bedrooms dataset .;;Fig .;DCGAN on Image Data .;11
59253;23f5854b38a15c2ae201e751311665f7995b5e10;Material;0;;;Netflix;Netflix Prize ( Netflix ) :;We only keep users who have watched at least five movies .;This is the user - movie ratings data from the Netflix Prize 7 .;Datasets;14
40863;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;na;;;laughter;The phonetic inventory includes special models for noise , vocalized - noise , laughter and silence .;Additionally , we have trained several models with 27k tied states .;We use a 30k - vocabulary derived from the most common words in the Switchboard and Fisher corpora .;Acoustic Model Details;21
53854;2116b2eaaece4af9c28c32af2728f3d49b792cf9;Material;1;;;ImageNet with dropout;Our model for ImageNet with dropout is a CNN which is trained on patches randomly extracted from the images , as well as their horizontal reflections .;;This is a form of data augmentation that reduces the network;Models for ImageNet;21
54980;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;1;;;Amazon Webcam scenario;Under this transductive setting , our method is able to improve previously - reported state - of - the - art accuracy for unsupervised adaptation very considerably ( Table [ reference ] ) , especially in the most challenging Amazon Webcam scenario ( the two domains with the largest domain shift ) .;Unlike those works and similarly to e.g. DLID we use the whole unlabeled target domain ( as the premise of our method is the abundance of unlabeled data in the target domain ) .;;Results;8
99970;41d08fb733f3e50ac183490f84d6377dffccf350;Material;0;;;3D data;That is why the majority of extant works on using deep nets for 3D data resort to either volumetric grids or collections of images ( 2D views of the geometry ) .;Reconstructed 3D point cloud for weight sharing , etc .;Such representations , however , lead to difficult trade offs between sampling resolution and net efficiency .;Input;3
103720;435259c5f3cffd75ef837a8e638cc8f6244e25c4;Material;1;;;iSEG - 2017 Grand Challenge;iSEG - 2017 Grand Challenge on 6 - month infant brain MRI Segmentation .;We report evaluations of our method on the publicly available data of the MICCAI;We obtained very competitive results among 21 teams , ranking first and second in most metrics .;Contributions and outline;3
54930;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;1;;;Syn Numbers;The latter ( Syn Numbers ) consists of 500 , 000 images generated by ourselves from Windows fonts by varying the text ( that includes different one - , two - , and three - digit numbers ) , positioning , orientation , background and stroke colors , and the amount of blur .;To address a common scenario of training on synthetic data and testing on real data , we use Street - View House Number dataset SVHN as the target domain and synthetic digits as the source .;The degrees of variation were chosen manually to simulate SVHN , however the two datasets are still rather distinct , the biggest difference being the structured clutter in the background of SVHN images .;Results;8
96968;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;0;;;crowd sourced question - answer pairs;Rajpurkar2016SQuAD10 contains 100K + crowd sourced question - answer pairs where the answer is a span in a given Wikipedia paragraph .;Question answering The Stanford Question Answering Dataset ( SQuAD );"Our baseline model ClarkAdvancingRC is an improved version of the Bidirectional Attention Flow model in [ BiDAF ; ]";Evaluation;8
27891;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;na;;;art;Current state - of - the - art object recognition architectures do achieve good performance but they are specialized for a single depiction style ( e.g. photos , sketches ) .;Human beings easily accomplish depiction - invariant recognition but machine - based systems are nowhere close to a similar level of performance .;Therefore , designing architectures which recognize objects regardless of depiction style can facilitate progress towards matching human - level abilities .;Introduction;1
97017;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;CoNLL 2012 shared task Pradhan2012CoNLL2012ST;In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task Pradhan2012CoNLL2012ST , adding ELMo improved the average F by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F .;It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .;Named entity extraction;Evaluation;8
79149;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;1;;;Extended Yale B dataset;"Regarding the more general task of learning invariant representations ; our results on the Extended Yale B dataset also demonstrate our model ’s ability to learn such representations .";;As expected , on the original representation the lighting conditions , , are well identifiable with almost perfect accuracy from both RF and LR .;Learning Invariant Representations;14
24624;0d24a0695c9fc669e643bad51d4e14f056329dec;Material;1;;;German - English machine translation track of the IWSLT 2014 evaluation campaign;For our first translation experiment , we use data from the German - English machine translation track of the IWSLT 2014 evaluation campaign cettolo2014report , as used in ranzato2015sequence , and closely follow the pre - processing described in that work .;;The training data comprises about 153 , 000 German - English sentence pairs .;Machine Translation;15
44481;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;English Czech;We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs : English Czech , English German , English Romanian and English Russian .;;Our systems are based on an attentional encoder - decoder , using BPE subword segmentation for open - vocabulary translation with a fixed vocabulary .;Introduction;1
66972;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;;noises;Although the blind model performs favorably for synthetic AWGN removal without knowing the noise level , it does not generalize well to real noisy images whose noises are much more complex than AWGN ( see the results of DnCNN - B in Fig .;First , the generalization ability is different .;[ reference ] ) .;FFDNet vs. a Single Blind Model;10
59249;23f5854b38a15c2ae201e751311665f7995b5e10;Material;0;;;MovieLens - 20 M;MovieLens - 20 M ( ML - 20 M ) :;We study three medium - to large - scale user - item consumption datasets from various domains :;These are user - movie ratings collected from a movie recommendation service .;Datasets;14
72700;2e10643c3759f97b673ff8c297778c0b6c20032b;Material;0;;;Yelp reviews;Yelp reviews .;The fields we used for this dataset contain title and abstract of each Wikipedia article .;The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 .;Large - scale Datasets and Results;11
22261;0c36c988acc9ec239953ff1b3931799af388ef70;Material;1;;;hard set;Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .;Several techniques were employed including multi - scale training , multi - scale testing , light - designed RCNN , some tricks for inference and a vote - based ensemble method .;;Abstract;1
73437;2e57bccb74bcb46cbc5b4225b62679023ed1f9da;Material;0;;;CNN and Daily Mail dataset;Memory and Attention : The memory of the ReasoNet on CNN and Daily Mail dataset is composed of query memory and passage memory .;We use a shared GRU model for both query and passage .;M =;CNN and Daily Mail Datasets;7
79557;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;0;;;bioinformatics;Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .;;In this paper , we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in and propose our Graph Capsule Network ( GCAPS - CNN ) model .;Title;0
106052;4543052aeaf52fdb01fced9b3ccf97827582cef5;Material;1;;;300 - W dataset;In order to find appropriate choices of bit - widths , we try a series of bit - width combinations on the 300 - W dataset based on - DU - Net ( 4 ) .;Through network quantization , high precision operations and parameters can be efficiently represented by a few discrete values .;The performance and balance ability of these combinations on several methods are shown in Table [ reference ] , where DU - Net ( 4 ) is DU - Net with 4 blocks , BW and TW respectively represents binarized weight and ternarized weight without , BW - is binarized weight with float scaling factor , the suffix QIG means quantized inputs and gradients .;Evaluation of Network Quantization;14
48180;1cf6bc0866226c1f8e282463adc8b75d92fba9bb;Material;1;;;VQA dataset;The VQA dataset is a recent large dataset based on MS COCO .;;We use the full release ( V1.0 ) open - ended dataset , which contains a train set and a val set .;Results on VQA;12
83401;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;1;;;tst2012;The test set is a concatenation of dev2010 , dev2012 , tst2010 , tst2011 and tst2012 which results in sentence pairs .;Our validation set comprises of sentence pairs which was taken from the training data .;The English dictionary has words while the German has words .;Machine Translation;13
88225;380b2c78d21ae6c43d418b6f0cb0222d5293d345;Material;1;;;Spanish Gigaword;"( 2015 ) ; for English and Chinese we used the same pretrained word embeddings as in Table [ reference ] , for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3 .";We also include results with pretrained word embeddings for English , Chinese , German , and Spanish following the same training setup as Dyer et al .;See Table [ reference ] .;Experiments;7
97628;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;na;;;"inthe - wild "" face image";"α * is the 3DMM ground truth , I is the input "" inthe - wild "" face image , and z is its latent representation .";where z * is the latent representation of a 3D annotated image J ,;The above self - critic loss encourages the model to output 3D faces that lie on the manifold of human faces , and predict landmarks that have the same distribution with the true facial landmarks .;Self - critic learning;12
47393;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;1;;;PASCAL VOC training;The WSDDNs are trained on the PASCAL VOC training and validation data by using fine - tuning on all layers , a widely - adopted technique to improve the performance of a CNN on a target domain [ reference ] . Here , however , fine tuning performs the essential function of learning the classification and detection streams , effectively causing the network to learn to detect objects , but using only weak image - level supervision .;"Note that this layer assesses the classification performance for the 20 classes together , but each of them is treated as a different binary classification problem ; the reason is that classes can co - occur in the PASCAL VOC , such that the softmax log loss used in AlexNet is not appropriate .";The experiments are run for 20 epochs and all the layers are fine - tuned with the learning rate 10 −5 for the first ten epochs and 10 −6 for the last ten epochs .;Experimental setup .;11
76178;303065c44cf847849d04da16b8b1d9a120cef73a;Material;1;;;iBUG;We trained the “ in - the - wild ” texture model on the images of iBUG , LFPW & AFW datasets as described in Sec .;To train our model , which we label as ITW , we use a variant of the Basel Face Model ( BFM ) that we trained to contain both identities drawn from the original BFM model along with expressions provided by .;[ reference ] using the 3D shape fits provided by .;Experiments;12
34886;139768cf7714beb9309efba734460f8562c60c78;Material;1;;;FCE;Grammatically correct text is needed as the starting point for inserting artificial errors , and we used two different sources : 1 ) the corrected version of the same FCE training set on which the system is trained ( 450 K tokens ) , and 2 ) example sentences extracted from the English Vocabulary Profile ( 270 K tokens ) .;We trained our error generation models on the public FCE training set [ reference ] and used them to generate additional artificial training data .;1 .;Evaluation;11
100174;41d08fb733f3e50ac183490f84d6377dffccf350;Material;1;;;RGBD input;When the neural network is given RGBD input our system can be viewed as a 3D shape completion method .;One interesting feature of our approach is that we can easily inject additional input information into the system .;Fig 8 visualizes examples of the predictions .;3D Shape Reconstruction from RGB Images;13
84284;360cfa09b2f7c8e10b1831d899c5a51aefa1883e;Material;1;;;TST 2011;The development test is composed of 81 talks ( 16 hours ) , while the test sets ( TST 2011 and TST 2012 ) are based on 8 talks ( 1.5 hours ) and 32 talks ( 6.5 hours ) , respectively .;The training set is composed of 820 talks with a total of about 166 hours of speech .;;Corpora and tasks;9
12901;072fd0b8d471f183da0ca9880379b3bb29031b6a;Material;1;;;cityscapes dataset;To this end , we adopt the popular FCN - 8s architecture for semantic segmentation , and train it on the cityscapes dataset .;The intuition is that if the generated images are realistic , classifiers trained on real images will be able to classify the synthesized image correctly as well .;We then score synthesized photos by the classification accuracy against the labels these photos were synthesized from .;Evaluation metrics;10
67349;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;;denoising noisy images;[ reference ] shows a more challenging example to demonstrate the advantage of FFDNet for denoising noisy images with spatially variant noise .;Fig .;We select five typical regions to estimate the noise levels , including two background regions , the coffee region , the milk - foam region , and the specular reflection region .;Experiments on Real Noisy Images;18
83399;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;1;;;tst2010;The test set is a concatenation of dev2010 , dev2012 , tst2010 , tst2011 and tst2012 which results in sentence pairs .;Our validation set comprises of sentence pairs which was taken from the training data .;The English dictionary has words while the German has words .;Machine Translation;13
52095;2019ede61cc0be14859908312e18458a7c79908f;Material;1;;;WikiProject Biography;It comprises all biography articles listed by WikiProject Biography 1 which also have a table ( infobox ) .;[ reference ] .;We extract and tokenize the first sentence of each article with Stanford CoreNLP;Biography dataset;15
14666;0875fc92cce33df5cf7df169590dbf0ca00d2652;Material;1;;;Microsoft COCO;For the Microsoft COCO task , we trained our model for epochs .;The model was trained using RMSprop with an initial learning rate of .;The learning rate was reduced to after epochs .;Hyperparameters;17
22418;0c36c988acc9ec239953ff1b3931799af388ef70;Material;0;;;Easy set;( a ) Easy set : validation;We believe that more kinds of data augmentation and hard example mining [ reference ] would further boost detection performance .;;Comparison on Benchmarks;9
44498;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;newsdev2016;We validate the model every 10000 minibatches via Bleu on a validation set ( newstest2013 , newstest2014 , or half of newsdev2016 for EN RO ) .;We train the models with Adadelta , reshuffling the training corpus between epochs .;We perform early stopping for single models , and use the 4 last saved models ( with models saved every 30000 minibatches ) for the ensemble results .;Baseline System;2
107671;46018a894d533813d67322827ca51f78aed6d59e;Material;1;;;BRATS2013;The experiments were carried out on real patient data obtained from the 2013 brain tumor segmentation challenge ( BRATS2013 ) , as part of the MICCAI conference .;;The BRATS2013 dataset is comprised of 3 sub - datasets .;Experiments and Results;13
47394;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;na;;;validation data;The WSDDNs are trained on the PASCAL VOC training and validation data by using fine - tuning on all layers , a widely - adopted technique to improve the performance of a CNN on a target domain [ reference ] . Here , however , fine tuning performs the essential function of learning the classification and detection streams , effectively causing the network to learn to detect objects , but using only weak image - level supervision .;"Note that this layer assesses the classification performance for the 20 classes together , but each of them is treated as a different binary classification problem ; the reason is that classes can co - occur in the PASCAL VOC , such that the softmax log loss used in AlexNet is not appropriate .";The experiments are run for 20 epochs and all the layers are fine - tuned with the learning rate 10 −5 for the first ten epochs and 10 −6 for the last ten epochs .;Experimental setup .;11
70269;2d294bde112b892068636f3a48300b3c033d98da;Material;1;;;COFW;Randomly testing faces from COFW are processed with left eyes being occluded , where the tight bounding box covering landmarks of left eye is filled with gray color , as shown in Fig .;To investigate the influence of occlusions , we directly use trained WM and AM without any additional processing for partially occluded faces .;[ reference ] .;MCL for Partially Occluded Faces;24
86140;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;1;;;ITOP 3D human pose estimation dataset;We also evaluated the performance of the proposed system on the ITOP 3D human pose estimation dataset [ reference ] .;A more detailed analysis of the challenge results is covered in [ reference ] .;We compared the system with state - of - the - art works , which include random forest - based method ( RF ) [ reference ] , RTW [ reference ] method ( VI ) [ reference ] , and REN - 9x6x6 [ reference ] .;Comparison with state - of - the - art methods;16
46172;1b9472907f5b7a1815c98b4562dce6c46dd2cf34;Material;0;;;MORPH - 2 dataset;The MORPH - 2 dataset ricanek2006morph ( 55 , 608 face images ) was preprocessed by locating the average eye - position in the respective dataset using facial landmark detection sagonas2016300 via MLxtend raschka2018mlxtend and then aligning each image in the dataset to the average eye position .;;The faces were then re - aligned such that the tip of the nose was located in the center of each image .;MORPH - 2 and CACD .;19
66861;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;;downsampled sub - images;Since FFDNet operates on downsampled sub - images , it is not necessary to employ the dilated convolution to further increase the receptive field .;[ reference ] .;By considering the balance of complexity and performance , we empirically set the number of convolution layers as 15 for grayscale image and 12 for color image .;Network Architecture;6
74555;2f0c30d6970da9ee9cf957350d9fa1025a1becb4;Material;1;;;COCO datasets;We use PASCAL VOC and COCO datasets .;Object Detection;For PASCAL VOC , following the protocol in , training is performed on the union of VOC 2007 trainval and VOC 2012 trainval .;Experiment Setup and Implementation;9
26802;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;;;Freeway;The issue of scale is best seen in a game such as Freeway , for which the random agent achieves a score close to 0 : scores achieved by learning agents , in the 10 - 20 range , are normalized into thousands .;Two issues arise with this approach : the scale of normalized scores may be excessively large and normalized scores are generally not translation invariant .;By contrast , no learning agent achieves a random - normalized score greater than 1 in Asteroids .;Normalization to a Reference Score;24
64593;289e91654f6da968d625481ef21f52892052d4fc;Material;1;;;FastText;For the word - based models , we reported the results from TextCNN , TextRNN and FastText .;Word - based Model .;The three models got the best performance in the KanShan - Cup competition , so we applied them as the word - based baselines .;Baselines;21
27847;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;na;;;photographic test images;( ‘ Overall ’ ) and style - wise accuracies for ‘ Photo ’ ( i.e. accuracy on photographic test images only ) and ‘ Art ’ ) .;We collate the results into three groups – accuracy regardless of depictive style;The results can be seen in seen in Table [ reference ] .;Results;15
74914;2f92b10acf7c405e55c74c1043dabd9ded1b1800;Material;1;;;DBpedia comments;In the area of visual question answering Wu2016 utilize external knowledge in form of DBpedia comments ( short abstracts / definitions ) to improve the answering ability of a model .;Previous efforts have focused on specific tasks or certain kinds of knowledge , whereas we take a step towards a more general - purpose solution for the integration of heterogeneous knowledge for NLU systems by providing a simple , general - purpose reading architecture that can read background knowledge encoded in simple natural language statements , e.g. , “ abdication is a type of resignation ” .;marino2016more explicitly incorporate knowledge graphs into an image classification model .;Related Work;21
16774;09ec60f2eea5d43792b2bc9da63b1c9b7719f666;Material;1;;;AFLW2000 dataset;We compare face alignment performance with state - of - the - art methods , SDM and 3DDFA , on the AFLW2000 dataset .;[ reference ] ) .;The alignment accuracy is evaluated by the Normalized Mean Error ( NME ) , the average of visible landmark error normalized by the bounding box size .;Applications;13
93480;3d5d9d8e74b215609eabba80ef79a35ebf460e49;Material;1;;;WikiArt dataset;We evaluate our model on several datasets include Yosemite ( summer and winter scenes ) , artworks ( Monet and van Gogh ) , edge - to - shoes and photo - to - portrait cropped from subsets of the WikiArt dataset and the CelebA dataset .;Datasets .;We also perform domain adaptation on the classification task with MNIST to MNIST - M , and on the classification and pose estimation tasks with Synthetic Cropped LineMod to Cropped LineMod .;Experimental Results;7
45013;1a6b67622d04df8e245575bf8fb2066fb6729720;Material;1;;;WikiText - 2 datasets;With negligible overhead in the number of parameters and training time , our Past Decode Regularization ( PDR ) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText - 2 datasets using a single softmax .;This biases the model towards retaining more contextual information , in turn improving its ability to predict the next token .;We also show gains by using PDR in combination with a mixture - of - softmaxes , achieving a word level perplexity of 53.8 and 60.5 on these datasets .;Improved Language Modeling by Decoding the Past;0
79875;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset;In second round , we used social network datasets namely : COLLAB , IMDB - BINARY , IMDB - MULTI , REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset contains enzymes and non - enzymes proteins structures .;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;For other datasets details can be found in .;Experiment and Results;12
52739;20926884a62778a2bf3f9f3c56f30976749ad763;Material;1;;;semi - synthetic IHDP dataset;"One is by using synthetic or semi - synthetic datasets , where the outcome or treatment assignment are fully known ; we use the semi - synthetic IHDP dataset from .";Existing literature mostly deals with this in two ways .;The other is using real - world data from randomized controlled trials ( RCT ) .;Experiments;17
89296;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;Pascal Context Dataset;section : Pascal Context Dataset;The methods from the second group use both COCO and VOC datasets for training .;We conducted an experiment on the Pascal Context dataset [ reference ] , which differs from the previous one in the larger number of classes considered , 59 .;Pascal Context Dataset;19
40440;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;Switchboard portion;The error rate of professional transcribers is 5.9 % for the Switchboard portion of the data , in which newly acquainted pairs of people discuss an assigned topic , and 11.3 % for the CallHome portion where friends and family members have open - ended conversations .;In this paper , we measure the human error rate on the widely used NIST 2000 test set , and find that our latest automated system has reached human parity .;In both cases , our automated system establishes a new state of the art , and edges past the human benchmark , achieving error rates of 5.8 % and 11.0 % , respectively .;Achieving Human Parity in Conversational Speech Recognition;0
36156;14908a18ff831005b6b4fc953ce61e1b4e7b54ee;Material;1;;;SemEval;Our models gets lower F1 scores on the company tweets dataset than on equivalent SemEval categories .;;As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .;Plutchik on Company Tweets;19
61516;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;CASIA - WebFace;The cross - age face dataset that we use is the collected CAF dataset introduced in Section [ reference ] while the general face dataset consists of three public face datasets : CASIA - WebFace , VGG Face and celebrity + .;The training set is composed of two parts : a cross - age face dataset and a general face dataset ( without cross - age face data ) .;The same identities appeared in different datasets are carefully merged together .;Implementation Details;10
78718;32a93598e8a338496f04a0ace81b0768c2ef059d;Material;1;;;German English;In fact , in experiments we find that with beam of size , ( on average ) accounts for of the distribution for German English , and for Thai English ( Table 1 : ) .;However , previous results showing the effectiveness of beam search decoding for NMT lead us to belief that a large portion of ’s mass lies in a single output sequence .;To summarize , sequence - level knowledge distillation suggests to : ( 1 ) train a teacher model , ( 2 ) run beam search over the training set with this model , ( 3 ) train the student network with cross - entropy on this new dataset .;Sequence - Level Knowledge Distillation;7
85672;36973330ae638571484e1f68aaf455e3e6f18ae9;Material;1;;;ETH;subsubsection : INRIA and ETH;It can be observed that SAF R - CNN outperforms other methods by a large margin and achieves the lowest log - average miss rate of 9.32 , which is significantly lower than the current state - of - the - art approach CompACT - Deep , by 2.43 .;We train our model using the INRIA training set and evaluate the model on both INRIA and ETH testing sets .;INRIA and ETH;17
79032;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;1;;;Amazon reviews dataset;For the domain adaptation task we used the Amazon reviews dataset ( with similar preprocessing ) that was also employed by chen2012marginalized and 2015arXiv150507818G.;Finally we also binarized the data and used a multivariate Bernoulli distribution for , where is the sigmoid function .;It is composed from text reviews about particular products , where each product belongs to one out of four different domains :;Datasets;9
86636;36c3972569a6949ecca90bfa6f8e99883e092845;Material;1;;;ResNet152;We follow the same procedure as to extract grid - level features from ResNet152 .;To test this hypothesis , we combined grid - level image features together with bottom - up features .;Object - level features and grid - level features are separately fused with features from questions and then are concatenated to fed to classification .;Post - Challenge Improvements;7
50206;1e5b9e512c01e244287fe7afb05e03c96d5c1cd0;Material;1;;;CoNLL 2017 Shared Task;In our first experiment , we used our model in the setting of the CoNLL 2017 Shared Task to annotate words with XPOS 3 tags;In this section , we present the results of the application of our model to part - of - speech tagging .;[ reference ] . We compare our results against the top systems of the CoNLL 2017 Shared Task .;Part - of - Speech Tagging Results;13
91464;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;comment text;We therefore retrieve the comment text for each query term .;Inspecting the database shows that the ‘ comment ’ field is the most generally informative about an attribute , as it contains a general text description of it .;The KB + SPARQL combination is very general , however , and could be applied problem specific KBs , or a database of common sense information , and can even perform basic inference over RDF .;Relating to the Knowledge Base;10
104948;44da806ae67ae9885592492202b3dc5f50182cc8;Material;1;;;ICDAR 2017;Furthermore , the proposed PSENet achieves better or at least comparable performance on the ordinary quadrangular text datasets : ICDAR 2015 and ICDAR 2017;Among these datasets , SCUT - CTW1500 is explicitly designed for curve text detection , and on this dataset we surpass the previous state - of - the - art result by absolute 6.37 % .;MLT , when compared with the existing state - of - the - art methods .;Introduction;1
81771;3526555fa0178c101ee9896252c818f9e03532a5;Material;1;;;Tobacco - 3482;subsection : Results on Tobacco - 3482;This is similar to the evaluation scheme that was also used by Kang et al . and Kumar et al . and allows for a fair comparison with their approaches .;We have trained the four deep s described in Section [ reference ] on the two datasets with different weight initializations to investigate the benefits of transfer learning .;Results on Tobacco - 3482;14
13827;07cca2bdd0dc2fee02889e17789748eba9d06ffa;Material;1;;;GPS data;Lastly , we describe the GPS data collected for this research with a brief account on its descriptive statistics .;Then , we also define the sample characteristics that would lead to an efficient validation of our proposed inference framework , overcoming the data limitations that exist in past research .;2.1 .;Title;0
91450;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;textual knowledge;The third input source is the textual knowledge which is mined from a large - scale knowledge base , the DBpedia .;Average - pooling is applied over the 5 hidden - state vectors , to obtain a vector representation for the image .;More details are shown in the following section .;A VQA Model with External Knowledge;9
78750;32a93598e8a338496f04a0ace81b0768c2ef059d;Material;1;;;Thai English;To test out these approaches , we conduct two sets of NMT experiments : high resource ( English German ) and low resource ( Thai English ) .;;The English - German data comes from WMT 2014 .;Experimental Setup;9
20015;0b0cf7e00e7532e38238a9164f0a8db2574be2ea;Material;1;;;English - French;For English - French , we used the significantly larger WMT 2014 English - French dataset consisting of 36 M sentences and split tokens into a 32000 word - piece vocabulary wu2016google .;Sentences were encoded using byte - pair encoding DBLP : journals / corr / BritzGLL17 , which has a shared source - target vocabulary of about 37000 tokens .;Sentence pairs were batched together by approximate sequence length .;Training Data and Batching;16
99085;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;na;;;synchronized RGB images;The object detection benchmark in KITTI provides synchronized RGB images and LiDAR point clouds with ground truth amodal 2D and 3D box annotations for vehicles , pedestrians and cyclists .;;The training set contains 7 , 481 frames and an undisclosed test set contains 7 , 581 frames .;KITTI Training;32
45106;1a6b67622d04df8e245575bf8fb2066fb6729720;Material;1;;;PTBC;For the latter , we use the Penn Treebank Character ( PTBC ) ( ) and the Hutter Prize Wikipedia Prize ( ) ( also known as Enwik8 ) datasets .;For the former , we evaluate our method on the Penn Treebank ( PTB ) ( ) and the WikiText - 2 ( WT2 ) ( ) datasets .;Key statistics for these datasets is presented in Table [ reference ] .;Experiments;3
9421;05357b8c05b5bc020e871fc330a88910c3177e4d;Material;1;;;PASCAL VOC 2007;We evaluate our method on the challenging PASCAL VOC 2007 and 2012 datasets which have and images respectively for object classes .;;These two datasets are divided into train , val , and test sets .;Datasets and evaluation measures;8
40170;1672ffebacadf849188668f24bcd377a19ae4051;Material;1;;;Amazon Dataset;Amazon Datasethttp: // jmcauley.ucsd.edu / data / amazon / . Amazon Dataset contains product reviews and metadata from Amazon , which is used as benchmark dataset .;;We conduct experiments on a subset named Electronics , which contains 192 , 403 users , 63 , 001 goods , 801 categories and 1 , 689 , 188 samples .;Datasets and Experimental Setup;16
95676;3e95925d2bca43223453010ff8516a492287ce19;Material;1;;;WoZ;We did not find word dropout to be helpful for the WoZ task , which does not contain noisy ASR outputs .;This accounts for the poor quality of ASR outputs in DSTC2 , which frequently miss words in the user utterance .;;Implementation Details;9
35693;143a3186c368544ded00a444be33153420baa254;Material;na;;;Omniglot;Accuracy Omniglot [ reference ];5 - way Accuracy 20 - way;1 - shot 5 - shot 1 - shot 5 - shot MANN , no conv [ reference ] 82.8 % 94.9 % -- MAML , no conv ( ours ) 89.7 ± 1.1 % 97.5 ± 0.6 % -- Siamese nets [ reference ] 97.3 % 98.4 % 88.2 % 97.0 % matching nets [ reference ] 98.1 % 98.9 % 93.8 % 98.5 % neural statistician [ reference ] 98.1 % 99.5 % 93.2 % 98.1 % memory mod .;Classification;15
77056;3112d2d95d66b3d54a72c55072647aab937e410e;Material;na;;;NBA basketball games;The dataset consists of two sources of articles summarizing NBA basketball games , paired with their corresponding box - and line - score tables .;The dataset is intended to be comparable to WEATHERGOV in terms of token count , but to have significantly longer target texts , a larger vocabulary space , and to require more difficult content selection .;The data statistics of these two sources , RO - TOWIRE and SBNATION , are also shown in Ta An;Data - to - Text Datasets;3
103234;43428880d75b3a14257c3ee9bda054e61eb869c0;Material;1;;;WMT'14 English - French task;"Next , we evaluate the inference speed of our architecture on the development set of the WMT'14 English - French task which is the concatenation of newstest2012 and newstest2013 ; it comprises 6003 sentences .";;We measure generation speed both on GPU and CPU hardware .;Generation Speed;20
40704;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;na;;;in - domain and out - domain data;First we train the model on the combination of in - domain and out - domain data for four data passes without any learning rate adjustment .;Here we also used a two - phase training schedule to train the LSTM LMs .;We then start from the resulting model and train on in - domain data until convergence .;LSTM - LM setup;11
54182;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;JFLEG dataset;Among our systems , the system with fluency boost learning and inference outperforms human ’s performance on both CoNLL and JFLEG dataset , while the system with only fluency boost learning achieves higher scores on CoNLL dataset .;For CoNLL - 10 , we follow the evaluation setting in and to fairly compare systems ’ performance to human ’s , which is marked with ( SvH ) in Table [ reference ] .;We further study the effectiveness of fluency boost learning and inference for different error types .;Experimental results;14
83997;3600aac8edc5bc015e69f2ffa893c21b6d4e1057;Material;na;;;NIVIDIA TITAN Xp;The computational cost is tested on a NIVIDIA TITAN Xp .;As shown in Table [ reference ] , our FAN detector can not only obtain the state - of - art results but also possess efficient computational speed .;The min size means the shortest side of the images which are resized to by keeping the aspect ratio .;Inference Time;17
36868;151313065d71b49dbf07289c002c887d7b5a0a6b;Material;1;;;Company dataset;Overall , our proposed DeepFM model beats the competitors by more than 0.37 % and 0.42 % in terms of AUC and Logloss on Company dataset , respectively .;Compared to these two models , DeepFM achieves more than 0.48 % and 0.33 % in terms of AUC ( 0.61 % and 0.66 % in terms of Logloss ) on Company and Criteo datasets .;In fact , a small improvement in offline AUC evaluation is likely to lead to a significant increase in online CTR .;Effectiveness Comparison;15
27336;0ee850dd6640a96531ac5ad21da5438db04d8b3c;Material;1;;;TREC collections;In our experiments , we used two standard TREC collections : The first collection ( called Robust04 ) consists of over 500k news articles from different news agencies , that is available in TREC Disks 4 and 5 ( excluding Congressional Records ) .;Collections .;This collection , which was used in TREC Robust Track 2004 , is considered as a homogeneous collection , because of the nature and the quality of documents .;Data;9
40641;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;CTS evaluation transcripts;It gives a perplexity of 69 on the 1997 CTS evaluation transcripts .;The first - pass LM has approximately 15.9 million bigrams , trigrams , and 4grams , and a vocabulary of 30 , 500 words .;The initial decoding produces a lattice with the pronunciation variants marked , from which 500 - best lists are generated for rescoring purposes .;LM Rescoring and System Combination;9
36039;14908a18ff831005b6b4fc953ce61e1b4e7b54ee;Material;0;;;SemEval leaderboard;Similarly , the second place winner of the SemEval leaderboard trained a word - level bidirectional LSTM with attention , as well as including non - deep learning features into their ensemble .;The winner of the Task1:E - c challenge trained a bidirectional LSTM with an 800 , 000 word embedding vocabulary derived from training word vectors on a dataset of 550 million tweets .;Both submissions used training data across SemEval tasks , as well as additional training data outside of the training set .;SemEval Tweets;18
51989;2019ede61cc0be14859908312e18458a7c79908f;Material;1;;;Wikipedia infobox;To tackle this problem we introduce a statistical generation model conditioned on a Wikipedia infobox .;Furthermore , these datasets have a limited vocabulary of only about 350 words each , compared to over 400k words in our dataset .;We focus on the generation of the first sentence of a biography which requires the model to select among a large number of possible fields to generate an adequate output .;Introduction;2
39426;15e81c8d1c21f9e928c72721ac46d458f3341454;Material;0;Alle Sprachen als NA kennzeichnen?;;English input;both “ Danke schön . ” and “ Vielen dank . ” are consistent with the English input “;In many cases , there are multiple correct translations consistent with a single sequence of fertilities — for instance ,;Thank you . ” and the fertility sequence , because “ you ” is not directly translated in either German sentence .;Sequence - Level Knowledge Distillation;24
67347;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;;video noise;It can be seen that FFDNet can handle various kinds of noises , such as JPEG lossy compression noise ( see image “ Audrey Hepburn ” ) , and video noise ( see image “ Movie ” ) .;[ reference ] further shows more visual results of FFDNet on the other nine images from RNI15 .;Fig .;Experiments on Real Noisy Images;18
63838;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;1;?;;DUC validation set;In our work , we simply run the models trained on Gigaword corpus as they are , without tuning them on the DUC validation set .;In these experiments , we use the same metric to evaluate our models too , but we omit reporting numbers from other systems in the interest of space .;The only change we made to the decoder is to suppress the model from emitting the end - of - summary tag , and force it to emit exactly 30 words for every summary , since the official evaluation on this corpus is based on limited - length Rouge recall .;DUC Corpus;10
22258;0c36c988acc9ec239953ff1b3931799af388ef70;Material;1;;;WIDER FACE validation dataset;Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .;Several techniques were employed including multi - scale training , multi - scale testing , light - designed RCNN , some tricks for inference and a vote - based ensemble method .;;Abstract;1
91342;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;hand - curated KBs;The problem is that KBs constructed by analysing Wikipedia and similar are patchy and inconsistent at best , and hand - curated KBs are inevitably very topic specific .;The quality of the information in the KB is one of the primary issues in this approach to VQA .;Using visually - sourced information is a promising approach to solve this problem , but has a way to go before it might be usefully applied within our approach .;Visual Question Answering;5
57522;2393447b8b0b79046afea1c88a8ed3949338949e;Material;1;;;WFA dev;Table [ reference ] shows the results of the ablation test for our model ( controlled with the NLG style ) on the well - formed answers of the WFA dev .;;set .;Does our multi - style learning improve NLG performance ?;37
56600;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;LSUN Bedrooms;In experiments we have compared GANs trained with TTUR to conventional GAN training with a one time - scale update rule on CelebA , CIFAR - 10 , SVHN , LSUN Bedrooms , and the One Billion Word Benchmark .;Finally , to evaluate GANs , we introduced the ‘ Fréchet Inception Distance ’ ’ ( FID ) which captures the similarity of generated images to real ones better than the Inception Score .;TTUR outperforms conventional GAN training consistently in all experiments .;Conclusion;14
34885;139768cf7714beb9309efba734460f8562c60c78;Material;1;;;FCE training set;We trained our error generation models on the public FCE training set [ reference ] and used them to generate additional artificial training data .;;Grammatically correct text is needed as the starting point for inserting artificial errors , and we used two different sources : 1 ) the corrected version of the same FCE training set on which the system is trained ( 450 K tokens ) , and 2 ) example sentences extracted from the English Vocabulary Profile ( 270 K tokens ) .;Evaluation;11
44549;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;CzEng 1.6pre;For English Czech , we trained our baseline model on the complete WMT16 parallel training set ( including CzEng 1.6pre ) , until we observed convergence on our heldout set ( newstest2014 ) .;;This took approximately 1 M minibatches , or 3 weeks .;English Czech;10
28411;0f2f4edb7599de34c97f680cf356943e57088345;Material;1;;;FLIC;Our results on FLIC are very competitive reaching 99 % PCK@0.2 accuracy on the elbow , and 97 % on the wrist .;Results can be seen in Figure [ reference ] and Table [ reference ] .;It is important to note that these results are observer - centric , which is consistent with how others have evaluated their output on FLIC .;Evaluation;9
54935;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;1;;;MNIST ↔ SVHN;MNIST ↔ SVHN .;In contrast , SA does not result in any significant improvement in the classification accuracy , thus highlighting that the adaptation task is even more challenging than in the case of the MNIST experiment .;In this experiment , we further increase the gap between distributions , and test on MNIST and SVHN , which are significantly different in appearance .;Results;8
91790;3b1d8eb163ffff598c2faa0d9d7cf933857a359f;Material;na;?;;nips;Sentence - encoding - based models use Siamese architecture DBLP : conf / nips / BromleyGLSS93 .;These models mainly fall into two types of approaches : sentence - encoding - based models and models using also inter - sentence attention .;The parameter - tied neural networks are applied to encode both the premise and the hypothesis .;Related Work;2
100895;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;0;;;MultiBox;As a result , our output layer has parameters ( for VGG - 16 ) , two orders of magnitude fewer than MultiBox ’s output layer that has parameters ( for GoogleNet in MultiBox ) .;MultiBox has a - dimensional fully - connected output layer , whereas our method has a - dimensional convolutional output layer in the case of anchors .;If considering the feature projection layers , our proposal layers still have an order of magnitude fewer parameters than MultiBox .;Anchors;5
50942;1e7678467b1807777dcd9be557b79328ce9419a8;Material;0;;;Imagenet;This can be explained by their specificity to Imagenet classification .;Note , the AutoAugment data augmentation does not transfer well to the retrieval tasks .;This shows the limitation of a particular choice of data - augmentation if a single embedding for classification and retrieval datasets is desired .;Additional results and ablation study for Multigrain in retrieval;39
91781;3b1d8eb163ffff598c2faa0d9d7cf933857a359f;Material;1;;;MultiNLI datasets;We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .;In this paper we enrich neural - network - based NLI models with external knowledge in co - attention , local inference collection , and inference composition components .;The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .;Introduction;1
44591;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;English data;We apply the Latin BPE operations to the English data ( training data and input ) , and both the Cyrillic and Latin BPE operations to the Russian data .;We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the BPE operations on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .;Translation results are shown in Table [ reference ] .;English Russian;12
50670;1e7678467b1807777dcd9be557b79328ce9419a8;Material;0;;;YFCC100 M;The PCA whitening transformations are computed from the features of 20 K images from YFCC100 M , distinct from the C10k distractors .;We call the combination C10k .;;Datasets .;27
106415;454dd76eb0a82286c054a6dd9d9413e09ad66801;Material;1;;;abstract scenes ” dataset;We run optimization until convergence ( typically 20 epochs on the “ abstract scenes ” , 100 epochs on the “ balanced ” dataset ) and report performance on the test set from the epoch with the highest performance on the validation set ( measured by VQA score on the “ abstract scenes ” dataset , and accuracy over pairs on the “ balanced ” dataset ) .;Weights are optimized with Adadelta with mini - batches of 128 questions .;The edges between word nodes in the input question graph are labeled with the dependency labels identified by the Stanford parser .;Implementation;20
33653;128c727ac06fcc50f1735cb222a441eee6adcab6;Material;1;;;FB15k - 237;For WN18 and WN18RR , which both contain a significantly smaller number of relations relative to the number of entities as well as a small number of relations compared to FB15k and FB15k - 237 , we set and .;For FB15k and FB15k - 237 , we set both entity and relation embedding dimensions to .;We use both batch normalization and dropout to control overfitting and improve predictions .;Implementation and Experiments;16
70990;2d5dba33c706d907733f15e7b57fde9909894e29;Material;1;;;ICDAR 2015 Incidental Text;paragraph : ICDAR 2015 Incidental Text ( IC15 );We only use the dataset for pretraining our network .;is the Challenge 4 of the ICDAR 2015 Robust Reading Competition .;ICDAR 2015 Incidental Text ( IC15 );21
61492;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;?;;Large - Scale Cross - Age Face Dataset;section : Large - Scale Cross - Age Face Dataset ( CAF );Finally , The age component can be easily generalized to any other common component such as pose , illumination , emotion , etc .;In order to further motivate the development of AIFR and enrich the capability of the current model , a dataset with a large age gap is urgently needed .;Large - Scale Cross - Age Face Dataset ( CAF );6
10032;060ff1aad5619a7d6d6cdfaf8be5da29bff3808c;Material;1;;;CoNLL - 2012 datasets;In experiments on the CoNLL - 2005 and CoNLL - 2012 datasets we show that our linguistically - informed models out - perform the syntax - free state - of - the - art .;The model is trained such that , as syntactic parsing models improve , providing high - quality parses at test time will improve its performance , allowing the model to leverage updated parsing models without requiring re - training .;On CoNLL - 2005 with predicted predicates and standard word embeddings , our single model out - performs the previous state - of - the - art model on the WSJ test set by 2.5 F1 points absolute .;Introduction;1
9981;060ff1aad5619a7d6d6cdfaf8be5da29bff3808c;Material;1;;;CoNLL - 2005 SRL;In experiments on CoNLL - 2005 SRL , LISA achieves new state - of - the - art performance for a model using predicted predicates and standard word embeddings , attaining 2.5 F1 absolute higher than the previous state - of - the - art on newswire and more than 3.5 F1 on out - of - domain data , nearly 10 % reduction in error .;Moreover , if a high - quality syntactic parse is already available , it can be beneficially injected at test time without re - training our SRL model .;On ConLL - 2012 English SRL we also show an improvement of more than 2.5 F1 .;Linguistically - Informed Self - Attention for Semantic Role Labeling;0
27785;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;na;;;art images;This dataset contains classes and to images in each class with approximately half photo and half art images .;We evaluate the classification performance on the Photo - Art - 50 dataset .;The authors also provide train - test splits for comparative evaluation .;Dataset;8
9559;05ee231749c9ce97f036c71c1d2d599d660a8c81;Material;1;;;IJB - B face recognition dataset;Given this analysis , we train a network that far exceeds the state - of - the - art on the IJB - B face recognition dataset .;Third , we explore how input feature dimension , number of clusters and different training techniques affect the recognition performance .;This is currently one of the most challenging public benchmarks , and we surpass the state - of - the - art on both the identification and verification protocols .;;1
2656;01959ef569f74c286956024866c1d107099199f7;Material;1;;;abstract scene dataset;We present a large dataset that contains 204 , 721 images from the MS COCO dataset [ reference ] and a newly created abstract scene dataset [ reference ] , [ reference ] that contains 50 , 000 scenes .;[ reference ] . Unlike the open - ended task that requires a free - form response , the multiple - choice task only requires an algorithm to pick from a predefined list of possible answers .;The MS COCO dataset has images depicting diverse and complex scenes that are effective at eliciting compelling and diverse questions .;INTRODUCTION;2
51985;2019ede61cc0be14859908312e18458a7c79908f;Material;0;;;ROBOCUP dataset;Previous work experimented with datasets that contain only a few tens of thousands of records such as WEATHERGOV or the ROBOCUP dataset , while our dataset contains over 700k biographies from;An infobox is a fact table describing a person , similar to a person subgraph in a knowledge base [ reference ][ reference ] . Similar generation applications include the generation of product descriptions based on a catalog of millions of items with dozens of attributes each .;* Rémi performed this work while interning at Facebook .;Introduction;2
67072;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;?;;latent image;Here , is obtained by adding AWGN to latent image , and is the noise level map .;To train the FFDNet model , we need to prepare a training dataset of input - output pairs .;The reason to use AWGN to generate the training dataset is two - fold .;Dataset Generation and Network Training;14
13048;072fd0b8d471f183da0ca9880379b3bb29031b6a;Material;1;;;Cityscapes labels→photo;Cityscapes labels→photo 2975 training images from the Cityscapes training set , trained for 200 epochs , with random jitter and mirroring .;Weights were initialized from a Gaussian distribution with mean 0 and standard deviation 0.02 .;We used the Cityscapes validation set for testing .;Training details;24
4857;02a5b7a41ffa8518eb3b7cae9914a2bd2bbc886b;Material;1;;;VOT - 2018 benchmark;In Table 2 we compare the two variants of SiamMask against seven recently published state - of - the - art trackers on the VOT - 2018 benchmark .;Results on VOT - 2018 .;Both achieve outstanding performance and run in real - time .;Evaluation for visual object tracking;11
79068;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;0;mit Section Name 1?;;German datasets;The scaling of the supervised cost was low ( ) for the Adult , Health and German datasets due to the correlation of with .;Furthermore , the extra strength of the MMD , , was tuned according to a validation set .;On the Amazon reviews and Extended Yale B datasets however the scaling of the supervised cost was higher : for the Amazon reviews dataset ( empirically determined after observing the classification loss on the first few iterations on the first source - target pair ) and for the Extended Yale B dataset .;Experimental Setup;10
82859;3580d8a5e7584e98d547ebfed900749d347f6714;Material;0;;;ROBOCUP;"In contrast to previous work experimented on small datasets which contain only a few tens of thousands of records such as WEATHERGOV [ reference ] and ROBOCUP [ reference ] , we focus on a more challenging task to generate biographies The Wikipedia infobox of Charles Winstead , the corresponding introduction on his wiki page reads "" Charles Winstead [ reference ] was an FBI agent in the 1930s - 40s , famous for being one of the agents who shot and killed John Dillinger . "" .";To this end , we focus on table - to - text generation which involves comprehensive representation for the complex structure of a table rather than pre - defined schemas .;based on the Wikipedia infoboxes .;Introduction;2
54270;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;JFLEG;Table [ reference ] shows the results of GEC systems on CoNLL and JFLEG dataset .;SMT - NMT hybrid : the state - of - the - art GEC system grundkiewicz2018near that is based on an SMT - NMT hybrid approach .;Our base convolutional seq2seq model outperforms most of previous GEC systems owing to the larger size of training data we use .;Experimental results;14
99383;41232a69c0f8d4b993e6c6e00b16c223442c962f;Material;1;;;Gigaword corpus;The statistics of the Gigaword corpus is presented in Table [ reference ] .;Following , we remove pairs with empty titles , leading to slightly different accuracy compared with .;;Datasets;9
44855;1a67622ca58aa851afe36ad6c6e78f9fb9d691d2;Material;0;;;Flickr;Flickr [ reference ] is a network of the contacts between users of the photo sharing website .;•;The labels represent the interest groups of the users such as ' black and white photos ' .;Datasets;21
41482;1713d05f9d5861cac4d5ec73151667cb03a42bfc;Material;1;;;English texts;The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea kytea .;We only use the first 150 M pairs for training the models .;The vocabulary size for each language is reduced to 40 K using byte - pair encoding .;Machine Translation;8
35845;143a3186c368544ded00a444be33153420baa254;Material;na;;;Omniglot Classification;Omniglot Classification;5 - way;1 - shot 5 - shot context vector 94.9 ± 0.9 % 97.7 ± 0.3 % MAML 98.7 ± 0.4 % 99.9 ± 0.1 % age inputs;C.2 . Context vector adaptation;24
61519;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;MORPH;Since our testing dataset contains MORPH , CACD - VS , FG - NET , and LFW , we have excluded these data from the training set .;The same identities appeared in different datasets are carefully merged together .;Finally , our training set contains 1 , 765 , 828 images with 19 , 976 identities in total , which includes 313 , 986 cross - age face images with 4 , 668 identities and 1 , 451 , 842 general face images with 17609 identities respectively .;Implementation Details;10
76555;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;;heldout - 40;For test , we use another three shards from the heldout set , namely [ heldout - 20 , heldout - 30 , heldout - 40 ] .;For validation , we use two shards from the heldout set , namely [ heldout - 00 , heldout - 10 ] .;The hyper - parameters are listed below .;1B Word Dataset;30
107941;462d4e265c9cbe9ad5feeb9a7736184a90b36fed;Material;1;;;GeoNames;The PERSON gazetteer is collected from U.S. census 2000 , U.S. census 2010 and DBpedia whereas GeoNames is the main source for LOCATION , taking in both official and alternative names .;For the gazetteer feature , we focus on PERSON and LOCATION and compile a list for each .;All the tokens on both lists are then filtered to exclude frequently occurring common words .;Hand - crafted Features;8
64944;28eceb438da0b841bbd3d02684dbfa263838ed60;Material;1;;;NYU datasets;Results of pairwise comparisons of images synthesized by models trained on the Cityscapes and NYU datasets .;In the second , each pair is shown for a randomly chosen duration between Table 1 .;Each column compares our approach with one of the baselines .;Experimental procedure;14
25781;0dcde9f2c5149f0e4c806db7b4cc4915bed077da;Material;1;;;Google Street View House Number;The network ’s performance on the Google Street View House Number ( SVHN ) ( 4 % error ) database is also competitive with state - of - the art methods .;Our results match existing state - of - the - art results on the MNIST ( 0.37 % error ) and NORB - small ( 2.2 % error ) image classification databases , but with very fast training times compared to standard deep network approaches .;;Enhanced Image Classification With a Fast - Learning Shallow Convolutional Neural Network;0
73121;2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79;Material;1;;;Pubmed datasets;In this comparison , our LoNGAE model achieves competitive performance when compared against the GCN model on the Cora dataset , but outperforms GCN and all other baseline methods on the Citeseer and Pubmed datasets .;The other baseline methods are taken from Yang et al . .;Multi - task Learning Lastly , we report LPNC results obtained by our LoNGAE model in the MTL setting over 10 runs with random weight initializations .;Results and Analysis;9
26590;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;Dataset oder nicht?;;raw sensory data;Similar ideas have been developed around the theme of lifelong learning : learning a reusable , high - level understanding of the world from raw sensory data thrun95lifelong , pierce_kuipers_97 , stober08pixels , sutton11horde .;To this end , different theoretical frameworks have been proposed to formalize the notion of “ big ” artificial intelligence e.g. , ¿Russell97rationalityand , Hutter:04uaibook , legg08machine .;The growing interest in competitions such as the General Game Playing competition , Reinforcement Learning competition , and the International Planning competition coles_12 also suggests the artificial intelligence community;Introduction;1
63898;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;1;;;CNN / Daily Mail data;On CNN / Daily Mail data , although our models are able to produce good quality multi - sentence summaries , we notice that the same sentence or phrase often gets repeated in the summary .;We intend to carry out more experiments with this model in the future .;We believe models that incorporate intra - attention such as lstmn can fix this problem by encouraging the model to ‘ remember ’ the words it has already produced in the past .;Qualitative Analysis;12
57176;2329a46590b2036d508097143e65c1b77e571e8c;Material;1;;;FSH;We evaluate our system trained on only the 300 hour Switchboard conversational telephone speech dataset and trained on both Switchboard ( SWB ) and Fisher ( FSH ) , a 2000 hour corpus collected in a similar manner as Switchboard .;We use the full set , which is the most challenging case and report the overall word error rate .;Many researchers evaluate models trained only with 300 hours from Switchboard conversational telephone speech when testing on Hub5’00 .;Conversational speech : Switchboard Hub5’00 ( full );13
46243;1b9472907f5b7a1815c98b4562dce6c46dd2cf34;Material;1;;;AFAD;While UTKFace and CACD also contain some lower - quality images , a possible reason why the methods perform worse on UTKFace compared to AFAD is that UTKFace is about ten times smaller than AFAD .;For instance , we found that AFAD includes some images of particularly low resolution ( e.g. , 20x20 ) .;While CACD has approximately the same size as AFAD , the lower performance can be explained by the wider age range that needs to be considered ( 14 - 62 in CACD compared to 15 - 40 in AFAD ) .;Estimating the Apparent Age from Face Images;25
84271;360cfa09b2f7c8e10b1831d899c5a51aefa1883e;Material;0;;;WSJ - 5k corpus;Training was based on the original WSJ - 5k corpus ( consisting of 7138 sentences uttered by 83 speakers ) that was contaminated with a set of impulse responses measured in a real apartment .;The reference context was a domestic environment characterized by the presence of non - stationary noise ( with an average SNR of about 10dB ) and acoustic reverberation ( with an average reverberation time of about 0.7 seconds ) .;The test phase was carried out with both real and simulated datasets , each consisting of 409 WSJ sentences uttered by six native American speakers .;Corpora and tasks;9
69218;2c761495cf3dd320e229586f80f868be12360d4e;Material;1;;;Pascal VOC 2007 test set;, 201 out of 4952 images in Pascal VOC 2007 test set , and 84 out of 1449 images in Pascal VOC 2012 validation set that have near duplicates in JFT - 300M.;We find there are 5536 out of 50 K images in ImageNet validation set , 1648 out of 8 K images in COCO minival ∗;We rerun several experiments by removing near - duplicate images from validation sets and then comparing performance between baselines and learned models .;De - duplication Experiments;26
76608;303fef411f235e6d1125a40af1e93224f498a4d5;Material;0;;;1B;bs ’ ’ setting measures the computational cost introduced by MoS given enough memory , which is 1.9x , 2.5x , and 3.8x slowdown on PTB , WT2 , and 1B respectively .;‘;The ‘ ‘ best - 1 ’ ’ setting is usually slower compared to ‘ ‘ bs ’ ’ , because a single batch does not fit into the memory of a single GPU using MoS , in which case we have to split one batch into multiple small ones , resulting in further slowdown .;MoS Computational Time;34
75889;302207c149bdf7beb6e46e4d4afbd2fa9ac02c64;Material;1;;;RaFD;subsection : Experimental Results on RaFD;This is because unlike the other methods , StarGAN can handle image translation involving multiple attribute changes by randomly generating a target domain label in the training phase .;We next train our model on the RaFD dataset to learn the task of synthesizing facial expressions .;Experimental Results on RaFD;12
68489;2c03df8b48bf3fa39054345bafabfeff15bfd11d;Material;0;;;ImageNet cases;Also similar to the ImageNet cases;6 ( middle ) shows the behaviors of ResNets .;( Fig . 4 , right );CIFAR - 10 and Analysis;12
91787;3b1d8eb163ffff598c2faa0d9d7cf933857a359f;Material;0;;;WilliamsNB17;More recently the availability of much larger annotated data , e.g. , SNLI DBLP : conf / emnlp / BowmanAPM15 and MultiNLI DBLP : journals / corr / WilliamsNB17 , has made it possible to train more complex models .;Early research on natural language inference and recognizing textual entailment has been performed on relatively small datasets ( refer to MacCartneyThesis for a good literature survey ) , which includes a large bulk of contributions made under the name of RTE , such as Dagan2005ThePR , Iftene : W07 - 1421 , among many others .;These models mainly fall into two types of approaches : sentence - encoding - based models and models using also inter - sentence attention .;Related Work;2
106530;45b559e6271570598602fcf9777ed6f2f2d133e6;Material;0;;;Switchboard - 51h dataset;In contrast to our work , the architectures investigated in are quite different and the paper only provides results from training on a non - standard Switchboard - 51h dataset , with WER not close to state of the art performance on Hub5’00 .;Most closely related to this is , which also uses VGG Net - inspired CNNs for LVCSR .;In the context of low - resource language tasks , it can be crucial to leverage training data in languages other than the target language .;INTRODUCTION;1
2491;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;;full ImageNet - 22 K set;Our 5 K dataset is a subset of the full ImageNet - 22 K set [ reference ] .;Next we evaluate our models on a larger ImageNet subset that has 5000 categories .;The 5000 categories consist of the original ImageNet - 1 K categories and additional 4000 categories that have the largest number of images in the full ImageNet set .;Experiments on ImageNet - 5 K;13
53734;2116b2eaaece4af9c28c32af2728f3d49b792cf9;Material;0;1 wenn mit Section Name;;RCV1 - v2;Reuters Corpus Volume I ( RCV1 - v2 ) is an archive of 804 , 414 newswire stories that have been manually categorized into 103 topics .;;The corpus covers four major groups : corporate / industrial , economics , government / social , and markets .;Experiments on Reuters;9
37121;1518039b5001f1836565215eb047526b3ac7f462;Material;1;;;Russian training text;To increase the consistency between English and Russian segmentation despite the differing alphabets , we transliterate the Russian vocabulary into Latin characters with ISO - 9 to learn the joint BPE encoding , then transliterate the BPE merge operations back into Cyrillic to apply them to the Russian training text .;If we apply BPE independently , the same name may be segmented differently in the two languages , which makes it harder for the neural models to learn a mapping between the subword units .;section : Evaluation;Byte Pair Encoding ( BPE );6
92814;3c78c6df5eb1695b6a399e346dde880af27d1016;Material;0;;;WikiReading;Additional datasets including SQuAD , WikiReading , MS Marco and TriviaQA provided more realistic questions .;The first large scale datasets for training neural reading comprehension models used a Cloze - style task , where systems must predict a held out word from a piece of text .;Another dataset of trivia questions , Quasar - T , was introduced recently that uses ClueWeb09 as its source for documents .;Related Work;21
7928;051b3763c2ad4e4271db712b0e9a4cfe298d05db;Material;0;;;Sintel dataset;Figure [ reference ] shows some examples of flow fields on Sintel dataset .;Despite DC Flow ( a hybrid method consists of CNN and post - processing ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .;LiteFlowNet - ft and FlowNet2 - ft;Results;7
101206;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;na;;;validation set;We experiment with the 80k images on the training set , 40k images on the validation set , and 20k images on the test - dev set .;This dataset involves 80 object categories .;We evaluate the mAP averaged for IoU ( COCO ’s standard metric , simply denoted as mAP@ [ .5 , .95 ] ) and mAP@0.5 ( PASCAL VOC ’s metric ) .;Experiments on MS COCO;12
62360;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;7 - scenes training set;[ reference ] uses fragments fused from multiple depth frames , we fine - tune our 3DMatch ConvNet on correspondences over a set of fragments constructed in the same way using the 7 - scenes training set .;Since the benchmark from Choi et al .;We then run pairwise geometric registration with 3DMatch + RANSAC on every pair of fragments from the benchmarks .;Matching Local Geometry in Scenes;14
26685;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;;game background;Each game background is precomputed offline , using 18 , 000 observations collected from sample trajectories .;The Basic method first removes the image background by storing the frequency of colours at each pixel location within a histogram .;The sample trajectories are generated by following a human - provided trajectory for a random number of steps and subsequently selecting actions uniformly at random .;Basic .;9
26632;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;;;Pitfall !;The console ’s joystick , as well as some of the original games such as Adventure and Pitfall ! , are iconic symbols of early video games .;"Over 500 original games were released for the console ; “ homebrew ” games continue to be developed today , over thirty years later .";Nearly all arcade games of the time – Pac - Man and;The Atari 2600;3
79941;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;bioinformatic datasets;It again show a consistent performance gain of accuracy ( highest being on PTC dataset ) on many bioinformatic datasets when compared against with strong graph kernels .;Our GCAPS - CNN is also very competitive with state - of - art graph kernel methods .;While other considered deep learning methods are not even close enough to beat graph kernels on many of these datasets .;Experiment and Results;12
29671;0fbd17a4f791e04bbf8f240f7c48c178900e30a6;Material;1;;;COCO keypoints dataset;We trained our keypoint and person detection models on COCO keypoints dataset ( without using any external / extra data ) in our experiments .;;We used COCO for evaluating the keypoint and person detection , however , we used PASCAL VOC 2012 for evaluating person segmentation due to the lack of semantic segmentation annotations in COCO .;Datasets;19
97490;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;1;;;AFLW - LFPA;Comparison on the AFLW2000 - 3D and AFLW - LFPA datasets shows that our method achieves excellent performance on both tasks of 3D face reconstruction and dense face alignment .;•;;Introduction;5
89259;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;VOC 2012 validation set;After removing the overlapping images between VOC 2012 validation data and this training dataset , we were left with 346 images from the original VOC 2012 validation set to validate our models on .;In the first experiment , following [ reference ][ reference ][ reference ] , we used a training set consisted of VOC 2012 training data ( 1464 images ) , and training and validation data of [ reference ] , which amounts to a total of 11 , 685 images .;We call this set the reduced validation set in the sequel .;Pascal VOC Datasets;18
26973;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;;games;This method is motivated by three observations on the Atari 2600 hardware and games : While the Atari 2600 hardware supports a screen resolution of , game objects are usually larger than a few pixels .;The idea behind BASS is to directly encode colours present on the screen .;Overall , important game events happen at a much lower resolution .;Basic Abstraction of the ScreenShots ( BASS );38
65097;29c19276b8fff231717c3e342cb24144d2b77726;Material;1;;;FTB;If there is more than one treebank per language , we use the treebank that has the canonical language name ( e.g. , Finnish instead of Finnish - FTB ) .;For languages with token segmentation ambiguity we use the provided gold segmentation .;We consider all languages that have at least 60k tokens and are distributed with word forms , resulting in 22 languages .;Datasets;6
26938;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;na;;;Games;"Games can easily last 10 , 000 time steps ( compared to 200–1000 in other domains ) ; observations are composed of 7 - bit images ( compared to black and white images in the work of stober08pixels , or 5 - 6 input features elsewhere ) ; observations are also more complex , containing the two players ’ score and side walls .";The Atari 2600 Pong , however , is significantly more complex than Pong domains developed for research .;In sheer size , the Atari 2600 Pong is thus a larger domain .;Final Remarks;35
57576;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;Chinese multi - document RC dataset;DuReader HeLLLZXLWWSLWW18 , a Chinese multi - document RC dataset , provides the top - 10 ranked entire documents from Baidu Search and Zhidao .;To the best of our knowledge , there are no datasets for providing answers in natural language with multiple styles except MS MARCO 2.1 , although there are some datasets that provide abstractive answers .;Many of the answers are long and relatively far from the source documents compared with those from MS MARCO .;RC with NLG .;45
27347;0ee850dd6640a96531ac5ad21da5438db04d8b3c;Material;1;;;TREC topics;We use the following query sets for evaluation that contain human - labeled judgements : a set of 250 queries ( TREC topics 301–450 and 601–700 ) for the Robust04 collection that were previously used in TREC Robust Track 2004 .;Evaluation query sets .;A set of 200 queries ( topics 1 - 200 ) were used for the experiments on the ClueWeb collection .;Data;9
99914;41b38da2f4137c957537908f9cb70cbd2fac8bc1;Material;1;;;CK + dataset;The CK + dataset contains images of faces with seven different facial expressions .;We conducted our experiments on the extended Cohn - Kanade dataset .;These expressions are labeled as anger , contempt , disgust , fear , happiness , sadness and surprise .;Experimental Results;6
36372;14ad9d060c1e8f0449e697ee189ac346353fbfbc;Material;1;;;BC5CDR - chem;We reported the performance on BC5CDR - chem and BC5CDR - disease .;The BC5CDR dataset has the sub - datasets BC5CDR - chem , BC5CDR - disease and BC5CDR - both , and they contain chemical entity types , disease entity types , and both entity types , respectively .;We have a total of six datasets : BC2GM , BC4CHEMD , BC5CDR - chem , BC5CDR - disease , JNLPBA , and NCBI .;Datasets;11
101205;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;Microsoft COCO object detection dataset;We present more results on the Microsoft COCO object detection dataset .;;This dataset involves 80 object categories .;Experiments on MS COCO;12
67131;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;Kodak dataset;The Kodak24 dataset consists of 24 center - cropped images of size 500 500 from the original Kodak dataset .;BSD68 dataset .;The McMaster dataset is a widely - used dataset for color demosaicing , which contains 18 cropped images of size 500 500 .;Dataset Generation and Network Training;14
106424;454dd76eb0a82286c054a6dd9d9413e09ad66801;Material;na;;;Abstract scenes;Abstract scenes also enabled removing confounding factors ( visual recognition ) .;The balanced dataset of abstract scenes was the only one allowing evaluation free from dataset biases .;It is not unreasonable to view the scene descriptions ( provided with abstract scenes ) as the output of a “ perfect ” vision system .;Additional details;21
44629;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;EN RU;Our models are also used in QT21 - HimL - SysComb , ranked 1–2 for EN RO , and in AMU - UEDIN , ranked 2–3 for EN RU , and 1–2 for RU EN .;They are also the ( tied ) best constrained system for EN RU and RO EN , or 7 out of 8 translation directions in total .;;Shared Task Results;13
79869;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;NCI1;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;To evaluate our GCAPS - CNN model , we perform graph classification tasks on variety of benchmark datasets .;In second round , we used social network datasets namely : COLLAB , IMDB - BINARY , IMDB - MULTI , REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset contains enzymes and non - enzymes proteins structures .;Experiment and Results;12
34228;12e20e4ea572dbe476fd894c5c9a9930cf250dd2;Material;1;;;TriviaQA dataset;The experimental evaluation shows that our model achieves the state - of - the - art result on TriviaQA dataset and competitive result in SQuAD .;We propose the multi - layer embedding to encode the document and the memory network of full - orientation matching to obtain the interaction of the context and query .;Moreover , the ablations and hops analysis demonstrate the importance of every part of the hierarchical attention vectors and the benefit of multi - hops in memory network .;Conclusion;19
59780;2451db113552afb6d9ad15ef4009ec4133d28f74;Material;1;;;Cars;For fine - grained categorization , we use three popular fine - grained benchmarks , i.e. , CUB - 200 - 2011 ( Birds ) , FGVC - aircraft ( Aircrafts ) and Stanford cars ( Cars ) .;As in , we report the results on the validation set .;The Birds dataset contains 11 , 788 images from 200 species , with large intra - class variation but small inter - class variation .;Datasets and Our Meta - layer Implementation;13
105028;44da806ae67ae9885592492202b3dc5f50182cc8;Material;1;;;SCUT - CTW1500;Then , we evaluate the proposed PSENet on three recent challenging public benchmarks : ICDAR 2015 , ICDAR 2017 MLT and SCUT - CTW1500 and compare PSENet with many state - of - the - art methods .;In this section , we first conduct ablation studies for PSENet .;;Experiment;9
47243;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;1;;;PASCAL data;In section 4 we show that , when fine - tuned on the PASCAL VOC training set , this architecture achieves state - of - the - art weakly supervised object detection on the PASCAL data , achieving superior results to the current state - of - the - art [ reference ] but using only CNN machinery .;Once the modifications have been applied , the network is ready to be fine - tuned on a target dataset , using only image - level labels , region proposals and back - propagation .;Since the system can be trained end - to - end using standard CNN packages , it is also as efficient as the recent fully - supervised Fast R - CNN detector of Girshick et al .;Introduction;2
73;000f90380d768a85e2316225854fc377c079b5c4;Material;0;;;Virtual KITTI;As a large amount of annotated data is crucial in order to train such deep networks , multiple new datasets have been released to encourage further research in this area , including Synthia [ reference ] , Virtual KITTI [ reference ] , and Cityscapes [ reference ] .;The dramatic performance improvements from using CNNs for semantic segmentation have brought about an increasing demand for such algorithms in the context of autonomous driving scenarios .;In this work , we focus on Cityscapes , a recent large - scale dataset consisting of real - world imagery with well - curated annotations .;Related Work;3
67089;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;;clean patches;The noisy patches are obtained by adding AWGN of noise level to the clean patches .;The patch size should be larger than the receptive field of FFDNet , and we set it to 70 70 and 50 50 for grayscale images and color images , respectively .;For each noisy patch , the noise level map is uniform .;Dataset Generation and Network Training;14
80780;34cf90fcbf83025666c5c86ec30ac58b632b27b0;Material;0;;;ReID datasets;Although these DCNN models have obtained impressive results on existing ReID datasets , there are still two problems .;Existing DCNN models for person ReID typically learn a global full - body representation for input person image ( Full body in Figure [ reference ] ) , or learn a part - based representation for predefined rigid parts ( Rigid body parts in Figure [ reference ] ) or learn a feature embedding for both of them .;First , for feature learning , current popular DCNN models typically stack single - scale convolution and max pooling layers to generate deep networks .;Introduction;1
104757;44c5dec4d1295d34f052d3243d8e08f14a3c0990;Material;1;;;WikiText - 103;We apply Transformer - XL to a variety of datasets on both word - level and character - level language modeling to have a comparison with state - of - the - art systems , including WikiText - 103 merity2016pointer , enwiki8 mahoney2011large , text8 mahoney2011large , One Billion Word chelba2013one , and Penn Treebank mikolov2012context .;;WikiText - 103 is the largest available word - level language modeling benchmark with long - term dependency .;Main Results;8
105373;4508f81033c9a7cec785ce4d16f1193920c1b341;Material;1;;;NewsTest 2013;We use NewsTest 2013 for validation and NewsTest 2014 and 2015 for testing .;We evaluate the full ByteNet on the WMT English to German translation task .;"The English and German strings are encoded as sequences of characters ; no explicit segmentation into words or morphemes is applied to the strings .";Character - Level Machine Translation;15
52973;20926884a62778a2bf3f9f3c56f30976749ad763;Material;1;;;IHDP;We investigate the effects of increasing imbalance between the original treatment groups by constructing biased subsamples of the IHDP dataset .;We average over 1000 realizations of the outcomes with 63 / 27 / 10 train / validation / test splits .;A logistic - regression propensity score model is fit to form estimates of the conditional treatment probability .;Simulated outcome : IHDP;18
75602;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;0;;;IJB - A Template IDs;Between the templates are the IJB - A Template IDs for probe and gallery as well as the best mated and best non - mated scores .;Up to eight individual media are shown with the last space showing a mosaic of the remaining media in the template .;Figure [ reference ] ( far left ) shows the highest mated similarities .;Error Analysis;11
34081;12e20e4ea572dbe476fd894c5c9a9930cf250dd2;Material;na;;;POS;We also use skip - gram model to train the embeddings of part - of - speech ( POS ) tags and named - entity;This layer maps each token to a high dimensional vector space and is proved to be helpful in handling out - of - vocab ( OOV ) words .;recognition ( NER ) tags .;Encoding of Context and Query;3
100462;420c46d7cafcb841309f02ad04cf51cb1f190a48;Material;0;;;VOC - 2012 validation set;Images from the VOC - 2012 validation set were not used for training .;Fine - tuning was performed for 50 K iterations with a learning rate of .;The front - end module trained by this procedure achieves;Experiments;5
13728;07cca2bdd0dc2fee02889e17789748eba9d06ffa;Material;1;weil Title..;;sparse GPS data;Inferring hybrid transportation modes from sparse GPS data using a moving window SVM classification 0198 - 9715 / $ see front matter 2012;;Elsevier Ltd. A http: // dx.doi.org / 10.1016 / j.compenvurbsys.2012.06.00 ⇑ Corresponding author .;Title;0
107454;46018a894d533813d67322827ca51f78aed6d59e;Material;0;;;BRATS 2014 challenge;Recently , preliminary investigations have shown that the use of deep CNNs for brain tumor segmentation makes for a very promising approach ( see the BRATS 2014 challenge workshop papers of ) .;Ideally , one would like to have features that are composed and refined into higher - level , task - adapted representations .;All three methods divide the 3D MR images into 2D or 3D patches and train a CNN to predict its center pixel class . as well as implemented a fairly common CNN , consisting of a series of convolutional layers , a non - linear activation function between each layer and a softmax output layer .;Related work;2
57545;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;ANS dev;Table [ reference ] shows the passage re - ranking performance for the ten given passages on the ANS dev .;;set .;Does our joint learning improve the passage re - ranking performance ?;40
54915;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;1;;;BSDS500;In order to obtain the target domain ( MNIST - M ) we blend digits from the original set over patches randomly extracted from color photos from BSDS500 .;Our first experiment deals with the MNIST dataset ( source ) .;This operation is formally defined for two images as , where are the coordinates of a pixel and is a channel index .;Results;8
51507;1ea6b2f67a3a7f044209aae0d0fd1cb14a1e9e06;Material;0;;;ImageNet experiments;"For the CIFAR - 10 and ImageNet experiments , these layers have 1024 feature maps ; for the MNIST experiment , the layers have 32 feature maps .";The top feature map is then passed through a couple of layers consisting of a Rectified Linear Unit ( ReLU ) and a convolution .;Residual and layer - to - output connections are used across the layers of all three networks .;Specifications of Models;12
81163;34cf90fcbf83025666c5c86ec30ac58b632b27b0;Material;0;;;MARS;MARS :;This demonstrates the effectiveness of the proposed model .;This dataset is the largest sequence - based person ReID dataset .;Comparison with State - of - the - art Methods;11
40469;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;conversational corpora;The Switchboard and later Fisher data collections of the 1990s and early 2000s provide what is to date the largest and best studied of the conversational corpora .;One of last big initiatives in this area was in conversational telephone speech ( CTS ) , which is especially difficult due to the spontaneous ( neither read nor planned ) nature of the speech , its informality , and the self - corrections , hesitations and other disfluencies that are pervasive .;The history of work in this area includes key contributions by institutions such as IBM , BBN , SRI , AT & T , LIMSI , Cambridge University , Microsoft and numerous others .;Introduction;1
91094;3a8d537bcec370d37990d39eab01c729496ad057;Material;0;;;ShapeNet dataset;In addition , the VSL is trained from scratch while the 3D - R2N2 is pre - trained using the ShapeNet dataset .;Unlike the NRSfM , the VSL does not require any segmentation , pose information , or keypoints .;However , the jointly - trained VSL did not outperform the 3D - R2N2 , which is also jointly - trained .;Single Image 3D Model Retrieval;14
98240;40b0fced8bc45f548ca7f79922e62478d2043220;Material;1;;;ILSVRC 2012 challenge dataset;We perform experiments using a network architecture almost identical to that popularized by Krizhevsky et al . and trained for classification using the 1.2 million images of the ILSVRC 2012 challenge dataset .;;All experiments are implemented using caffe , and our network is the publicly available caffe reference model .;Preliminaries;6
54170;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;CoNLL - 10;Adding fluency boost inference improves recall ( from 36.30 to 40.18 on CoNLL - 2014 and from 50.31 to 53.15 on CoNLL - 10 ) at the expense of a drop of precision ( from 74.12 to 68.45 on CoNLL - 2014 and from 88.56 to 84.71 on CoNLL - 10 ) .;Fluency boost learning improves the base convolutional seq2seq model in terms of all aspects ( i.e. , precision , recall , and GLEU ) , demonstrating fluency boost learning is actually helpful for training a seq2seq model for GEC .;Since weighs precision twice as recall , adding fluency boost inference leads to a drop of on the CoNLL dataset .;Experimental results;14
67410;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;synthetic images;The results on synthetic images with AWGN demonstrated that FFDNet can not only produce state - of - the - art results when input noise level matches ground - truth noise level , but also have the ability to robustly control the trade - off between noise reduction and detail preservation .;To achieve this goal , several techniques were utilized in network design and training , such as the use of noise level map as input and denoising in downsampled sub - images space .;The results on images with spatially variant AWGN validated the flexibility of FFDNet for handing inhomogeneous noise .;Conclusion;20
32071;10a36dea0167511b66deca65fdca978aa9afdb11;Material;1;;;COCO test2015;We submit the prediction result given by the final model on the testing set ( COCO test2015 ) to the evaluation server , to get the final accuracy on the test - dev and test - standard set .;After we find the best model parameters , we combine the whole train2014 and val2014 to train the final model .;For Open - Ended Question track , we take the top - 1 predicted answer from the softmax output .;Experiments;3
91579;3b1b94441010615195a5c404409ce2416860508c;Material;1;;;MS COCO Captioning Challenge 2015;Following the human evaluation protocol of the MS COCO Captioning Challenge 2015 , two evaluation metrics are applied .;We randomly sample 1000 results from the COCO validation dataset , generated by our proposed model Att - RegionCNN + LSTM and the baseline model VggNet + LSTM .;M1 is the percentage of captions that are evaluated as better or equal to human caption and M2 is the percentage of captions that pass the Turing Test .;Evaluation;16
44563;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;news2015 monolingual English;For Czech English the training process was similar to the above , except that we created the synthetic training data ( back - translated from samples of news2015 monolingual English ) in batches of 2.5 M , and so were able to observe the effect of increasing the amount of synthetic data .;Bleu on the ensemble of 4 , and 4.3–4.9 on the baseline .;After training a baseline model on all the WMT16 parallel set , we continued training with a parallel corpus consisting of 2 copies of the 2.5 M sentences of back - translated data , 5 copies of news - commentary v11 , and a matching quantity of data sampled from Czeng 1.6pre .;English Czech;10
15192;0899bb0f3d5425c88b358638bb8556729720c8db;Material;0;;;T - LESS dataset;This is especially difficult in the T - LESS dataset since objects are very similar .;We follow these strict evaluation guidelines , but treat the harder problem of 6D detection where it is unknown which of the considered objects are present in the scene .;justification = centering ,;Test Conditions;24
77162;3112d2d95d66b3d54a72c55072647aab937e410e;Material;1;;;SBNATION data;This poor performance is presumably attributable to the noisy quality of the SBNATION data , and the fact that many documents in the dataset focus on information not in the box - and line - scores .;We found that all models performed quite poorly on the SBNATION data , with the best model achieving a validation perplexity of 33.34 and a BLEU score of 1.78 .;Accordingly , we focus on ROTOWIRE in what follows .;Results;10
43294;19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7;Material;na;;;supervised signal;Deep learning methods have recently achieved state - of - the - art results in vision and speech domains , mainly due to their ability to automatically learn high - level features from a supervised signal .;;Recent advances in reinforcement learning ( RL ) have successfully combined deep learning with value function approximation , by using a deep convolutional neural network to represent the action - value ( Q ) function .;Introduction;1
91299;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;binary questions;Geman et al . proposed an automatic ‘ query generator ’ that is trained on annotated images and produces a sequence of binary questions from any given test image .;built a query answering system based on a joint parse graph from text and videos .;Each of these approaches places significant limitations on the form of question that can be answered .;Visual Question Answering;5
2076;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;;COCO Image Captions;We conduct the test with text generated by the models trained on WMT News and COCO Image Captions .;In the questionnaire , each ( machine generated or real ) sentence gets + 1 score when it is regarded as a real one , and 0 score otherwise .;The average score for each algorithm is calculated .;Turing Test and Generated Samples;16
91467;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;database of common sense information;The KB + SPARQL combination is very general , however , and could be applied problem specific KBs , or a database of common sense information , and can even perform basic inference over RDF .;We therefore retrieve the comment text for each query term .;Figure [ reference ] shows an example of the query language and returned text .;Relating to the Knowledge Base;10
95254;3e7f54801c886ea2061650fd24fc481e39be152f;Material;0;;;ITOP;[ b ] 0.3 [ b ] 0.3 [ b ] 0.3 Invariant - Top View Dataset ( ITOP ) .;This is performed three times such that each person is the test set once .;Existing depth datasets for pose estimation are often small in size , both in the number of people and number of frames per person .;Datasets;7
51906;1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f;Material;0;;;CIFAR;The bottleneck Residual Units ( for ResNet - 164 / 1001 on CIFAR ) are constructed following .;[ reference ] .;For example , a unit in ResNet - 110 is replaced with a unit in ResNet - 164 , both of which have roughly the same number of parameters .;Appendix : Implementation Details;12
72447;2dc32f9e0a7870b272a2a51082202a9fa52fb854;Material;1;;;Manga109;In Figure [ reference ] , we show visual comparisons on Urban100 , BSDS100 and Manga109 with the a scale factor of .;Nevertheless , our model provides a competitive performance to the state - of - the - art methods in and SR .;Our method accurately reconstructs parallel straight lines and grid patterns such as windows and the stripes on tigers .;Comparisons with the state - of - the - arts;14
96801;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;na;;;parallel corpora;Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .;[ ] McCann2017LearnedIT or an unsupervised language model Peters2017SemisupervisedST .;In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences Chelba2014OneBW .;Related work;2
28188;0f0cab9235bbf185acdd4f9713fd111ca50effca;Material;1;;;RAF datasets;We were able to obtain state - of - the - art results on image - based facial expression recognition problems on the SFEW 2.0 and RAF datasets .;Similarly , covariance matrix computed from image feature vectors were used as input to SPDNet for video - based facial expression recognition problem .;In video - based facial expression recognition , training SPDNet on image - based features was able to obtain results comparable to state - of - the - art results .;Conclusion;25
82807;357776cd7ee889af954f0dfdbaee71477c09ac18;Material;1;;;Toronto Face datasets;In Section [ reference ] , we will show that the test - likelihood achieved by the joint training scheme of adversarial autoencoders outperforms the test - likelihood of GMMN and GMMN + AE on MNIST and Toronto Face datasets .;"However , the main difference of our work with GMMN + AE is that the adversarial training procedure of our method acts as a regularizer that shapes the code distribution while training the autoencoder from scratch ; whereas , the GMMN + AE model first trains a standard dropout autoencoder and then fits a distribution in the code space of the pre - trained network .";;Relationship to GANs and GMMNs;5
91514;3b1b94441010615195a5c404409ce2416860508c;Material;0;?;;Flickr30k;Because most of previous works in image captioning are not evaluated on the official split for Flickr30k and MS COCO , for fair comparison , we report results with the widely used publicly available splits in the work of .We further tested on the actually MS COCO test set consisting of 40775 images ( human captions for this split are not available publicly ) , and evaluated them on the COCO evaluation server .;In our reported results , we use pre - defined splits for Flickr8k .;;Dataset;15
77208;3112d2d95d66b3d54a72c55072647aab937e410e;Material;na;;;test data;To accomplish this , raters were presented with a particular NBA game 's box score and line score , as well as with ( randomly selected ) sentences from summaries generated by our different models for Table 2 : Performance of induced metrics on gold and system outputs of RotoWire development and test data .;The first study attempted to determine whether generations considered to be more precise by our metrics were also considered more precise by human raters .;Columns indicate Record Generation ( RG ) precision and count , Content Selection ( CS ) precision and recall , Count Ordering ( CO ) in normalized Damerau - Levenshtein distance , perplexity , and BLEU .;Human Evaluation;12
9773;05ee231749c9ce97f036c71c1d2d599d660a8c81;Material;0;;;IJB - B dataset;The IJB - B dataset is an extension of IJB - A with a total of 11 , 754 images and 7 , 011 videos from 1 , 845 subjects , as well as 10 , 044 non - face images .;"The IJB - A dataset contains 5 , 712 images and 2 , 085 videos , covering 500 subjects ; thus the average number of images and videos per subject are 11.4 and 4.2 videos , respectively .";There is no overlap between subjects in VGGFace2 , which we use for training , and the test datasets .;Benchmark datasets and evaluation protocol;10
75567;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;0;;;CASIA;One hypothesis is that this imagery exhibits an unmodeled dataset bias for IJB - A faces , or that CASIA is image only , while IJB - A is imagery and videos .;Figure [ reference ] ( bottom ) shows that this results in slightly reduced verification performance .;;Negative Set Study;8
50667;1e7678467b1807777dcd9be557b79328ce9419a8;Material;1;;;INRIA Copydays dataset;We also report the performance of our network in a copy detection setting , indicating the mean average precision on the “ strong ” subset of the INRIA Copydays dataset .;"We also report the accuracy on the UKB object recognition benchmark , which contains instances of objects under varying viewpoints each ; each image is used as a query to find its 4 closest neighbors in embedding space ; the number of correct neighbors is averaged across all images , yielding a maximum score of .";We add 10 K distractor images randomly sampled from the YFCC100 M large - scale collection of unlabelled images .;Datasets .;27
64957;28eceb438da0b841bbd3d02684dbfa263838ed60;Material;1;;;Real;After just 1 8 of a second , the Pix2pix images are clearly rated less realistic than the real Cityscapes images or the CRN images ( 72.5 % Real > Pix2pix , 73.4 % CRN > Pix2pix ) .;[ reference ] ( referred to as ' Pix2pix ' following the public implementation ) .;On the other hand , the CRN images are on par with real images at that time , as seen both in the Real > CRN rate ( 52.6 % ) and in the nearly identical Real > Pix2pix and CRN > Pix2pix rates .;Results;15
47734;1c7e078611c9df412e6eb3a356f31a0da0c1f99c;Material;0;;;YCB - Video Dataset;section : The YCB - Video Dataset;In this way , the SLoss will not penalize rotations that are equivalent with respect to the 3D shape symmetry of the object .;Object - centric datasets providing ground - truth annotations for object poses and / or segmentations are limited in size by the fact that the annotations are typically provided manually .;The YCB - Video Dataset;8
72704;2e10643c3759f97b673ff8c297778c0b6c20032b;Material;1;;;Amazon review dataset;We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34 , 686 , 770 reviews from 6 , 643 , 669 users on 2 , 441 , 053 products .;Amazon reviews .;Similarly to the Yelp review dataset , we also constructed 2 datasets – one full score prediction and another polarity prediction .;Large - scale Datasets and Results;11
16335;09da677bdbba113374d8fe4bb15ecfbdb4c8fe40;Material;1;;;ImageNet - 1k object classification dataset;Key properties of the proposed DPNs are studied on the ImageNet - 1k object classification dataset and further verified on the Places365 - Standard scene understanding dataset .;Specifically , we evaluate the proposed architecture on three tasks : image classification , object detection and semantic segmentation , using three standard benchmark datasets : the ImageNet - 1k dataset , Places365 - Standard dataset and the PASCAL VOC datasets .;To verify whether the proposed DPNs can benefit other tasks besides image classification , we further conduct experiments on the PASCAL VOC dataset to evaluate its performance in object detection and semantic segmentation .;Experiments;9
72692;2e10643c3759f97b673ff8c297778c0b6c20032b;Material;1;;;SogouCS news corpora;This dataset is a combination of the SogouCA and SogouCS news corpora , containing in total 2 , 909 , 551 news articles in various topic channels .;Sogou news corpus .;We then labeled each piece of news using its URL , by manually classifying the their domain names .;Large - scale Datasets and Results;11
31132;10203151008a20b32ce089f7f9d580005c2426cf;Material;1;;;Oxford;The proposed methods outperform the state of the art on Paris and Oxford datasets , with and without distractors with all 16D , 32D , 128D , 256D , and 512D descriptors .;The results for MAC and R - MAC with the fine - tuned networks are summarized together with previously published results in Table [ reference ] .;On Holidays dataset , the Neural codes win the extreme short code category , while off - the - shelf NetVlad performs the best on 256D and higher .;Comparison with the state of the art .;23
104955;44da806ae67ae9885592492202b3dc5f50182cc8;Material;1;;;ICDAR 2015;Furthermore , it also achieves competitive results on the regular quadrangular text benchmarks : ICDAR 2015 and ICDAR 2017 MLT .;Our proposed PSENet significantly surpasses the state - of - the - art methods on the curve text detection dataset SCUT - CTW1500 .;;Introduction;1
91506;3b1b94441010615195a5c404409ce2416860508c;Material;na;;;image - QA pairs;We use Stochastic gradient Descent ( SGD ) with mini - batches of 100 image - QA pairs .;Note that is a regularization term , where .;The attributes , internal textual representation , external knowledge embedding size , word embedding size and hidden state size are all 256 in all experiments .;An Answer Generation Model with Multiple Inputs;12
7886;051b3763c2ad4e4271db712b0e9a4cfe298d05db;Material;1;;;Sintel clean;We compare several variants of LiteFlowNet to state - of - the - art methods on public benchmarks including FlyingChairs ( Chairs ) , Sintel clean and final , KITTI12 , KITTI15 , and Middlebury .;;Conventional Hybrid Heavyweight CNN Lightweight CNN FlyingChairs .;Results;7
79935;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;COLLAB dataset;Here , we were able to achieve upto accuracy gain on COLLAB dataset and rest were around gain with consistency when compared against other deep learning approaches .;Again , this trend is continued to be the same on social network datasets as shown in Table [ reference ] .;Our GCAPS - CNN is also very competitive with state - of - art graph kernel methods .;Experiment and Results;12
103593;434bf475addfb580707208618f99c8be0c55cf95;Material;1;;;Extended CohnâKanade database;We would like to thank the Affect Analysis Group of the University of Pittsburgh for providing the Extended CohnâKanade database , and Prof . Pantic and Dr. Valstar for the MMI data - base .;;;Acknowledgments;23
78101;31e5dab321066712cdc8b30943f7950066840ee1;Material;1;;;LDC2015E86;We report results on the AMR dataset LDC2015E86 and LDC2017T10 .;We use both BLEU and Meteor as evaluation metrics .;All systems are implemented in PyTorch using the framework;Experiments;13
102906;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;;OMNIGLOT data omniglot;Next we work with the OMNIGLOT data omniglot .;;This contains 1628 classes of handwritten characters but with just 20 examples per class .;Omniglot;18
45464;1abf6491d1b0f6e8af137869a01843931996a562;Material;1;;;PASCAL - Context dataset;Experiment results on VOC2012 and PASCAL - Context dataset also verify our assumption .;For example , Fig [ reference ] has misclassified a large portion of the image as bird since it only used local information , however , adding contextual information in the loop , which might contain strong signal of cat , corrects the mistake .;Compared with , the improvement is similar as of using CRF to post - process the output of FCN .;Global Context;4
53376;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;1;;;VGG - Net16;We first compare Pairwise Comparison with with Label - Smoothing Regularization ( LSR ) on all six FGVC datasets for VGG - Net16 , ResNet - 50 and DenseNet - 161 .;We additionally compare the performance of our optimization technique with other regularization methods as well .;These results are summarized in Table S7 .;S4 Comparison with Regularization;23
101240;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;COCO training set;Using the COCO training set to train , Faster R - CNN has 42.1 % mAP@0.5 and 21.5 % mAP@ [ .5 , .95 ] on the COCO test - dev set .;Next we evaluate our Faster R - CNN system .;This is 2.8 % higher for mAP@0.5 and 2.2 % higher for mAP@ [ .5 , .95 ] than the Fast R - CNN counterpart under the same protocol ( Table [ reference ] ) .;Experiments on MS COCO;12
103986;435259c5f3cffd75ef837a8e638cc8f6244e25c4;Material;1;;;iSEG MICCAI Grand Challenge;The proposed approach was evaluated on the iSEG MICCAI Grand Challenge , and compared to 21 other competing teams .;We showed that confidence values can identify potential errors in the automated segmentation , which might require corrections .;Our approach achieved a state - of - the - art performance , obtaining the highest score in most cases .;Discussion;19
58727;23dcfda130aada27c158c0b5f394cac489c9c795;Material;na;;;synthetically expanded data;We follow the path of which used synthetically expanded data to train their landmark detection model .;;One of the datasets they train on is the 300W - LP dataset which is a collection of popular in - the - wild 2D landmark datasets which have been grouped and re - annotated .;Training on a Synthetically Expanded Dataset;7
63484;27c761258329eddb90b64d52679ff190cb4527b5;Material;1;;;CHASE_DB;The processing times during the testing phase for the STARE , CHASE_DB , and DRIVE datasets were 6.42 , 8.66 , and 2.84 seconds per sample respectively .;The network architectures with different numbers of network parameters with respect to the different time - step are shown in Table IV .;In addition , skin cancer segmentation and lung segmentation take 0.22 and 1.145 seconds per sample respectively .;D. Evaluation;15
107411;46018a894d533813d67322827ca51f78aed6d59e;Material;1;;;BRATS;We focus our experimental analysis on the fully - annotated MICCAI brain tumor segmentation ( BRATS ) challenge 2013 dataset using the well defined training and testing splits , thereby allowing us to compare directly and quantitatively to a wide variety of other methods .;Since convolutions are efficient operations , this approach can be significantly faster than implementing a CRF .;"Our contributions in this work are four fold : We propose a fully automatic method with results currently ranked second on the BRATS 2013 scoreboard ; To segment a brain , our method takes between 25 seconds and 3 minutes , which is one order of magnitude faster than most state - of - the - art methods .";Introduction;1
67080;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;na;;;quantized noisy image;, we use the matlab function imnoise to generate the quantized noisy image from a clean one .;[ reference ];We collected a large dataset of source images , including 400 BSD images , 400 images selected from the validation set of ImageNet , and the 4 , 744 images from the Waterloo Exploration Database .;Dataset Generation and Network Training;14
54926;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;na;;;Synthetic numbers;Synthetic numbers → SVHN .;At the same time , the improvement over source - only model achieved by subspace alignment ( SA ) is quite modest , thus highlighting the difficulty of the adaptation task .;To address a common scenario of training on synthetic data and testing on real data , we use Street - View House Number dataset SVHN as the target domain and synthetic digits as the source .;Results;8
23271;0ca2bd0e40a8f0a57665535ae1c31561370ad183;Material;1;;;Hutter Prize Wikipedia;We evaluate our model on three benchmark text corpora : ( 1 ) Penn Treebank , ( 2 ) Text8 and ( 3 ) Hutter Prize Wikipedia .;t.;We use the bits - per - character ( BPC ) , E [ − log 2 p ( x t + 1 | x ≤t ) ] , as the evaluation metric .;CHARACTER - LEVEL LANGUAGE MODELLING;8
847;007ab5528b3bd310a80d553cccad4b78dc496b02;Material;1;;;SQuAD;The results of these variations on the dev data of SQuAD are shown in Table [ reference ] .;Since the output dimension of changes , the input dimension of the first LSTM of the modeling layer will change as well .;It is important to note that there are non - trivial gaps between our definition of and other definitions employed by previous work .;Eqn . [ reference ] : MLP after concatenation .;33
51290;1e7a36c4d4f96b29e3edf51b6eb61f8e16217704;Material;1;;;Hutter Prize;subsection : Hutter Prize dataset;Previous versions of the paper also report a character level result on Penn Treebank dataset of 1.35 bits / char with an unregularised mLSTM , however we do not include this experiment in this version as we have no results with our updated training and regularisation methodology .;We performed experiments using the Hutter Prize dataset , originally used for the Hutter Prize compression benchmark .;Hutter Prize dataset;10
100669;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;ILSVRC and COCO 2015 competitions;In ILSVRC and COCO 2015 competitions , Faster R - CNN and RPN are the foundations of the 1st - place winning entries in several tracks .;For the very deep VGG - 16 model , our detection system has a frame rate of 5fps ( including all steps ) on a GPU , while achieving state - of - the - art object detection accuracy on PASCAL VOC 2007 , 2012 , and MS COCO datasets with only 300 proposals per image .;Code has been made publicly available .;Faster R - CNN : Towards Real - Time Object Detection with Region Proposal Networks;0
13293;074b6fe0cc6848fb86a6703d1c52074494177c79;Material;1;;;SYNTHIA dataset;For our first evaluation , we consider the SYNTHIA dataset ros_cvpr16 , which contains synthetic renderings of urban scenes .;We did not have the required memory for this at the time of submission , but leave it to future work to deploy model parallelism or experiment with larger GPU memory .;We use the SYNTHIA video sequences , which are rendered across a variety of environments , weather conditions , and lighting conditions .;Semantic Segmentation Adaptation;6
102881;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;;synthetic - D datasets;In our first experiment we wanted to know if the neural statistician will learn to cluster synthetic - D datasets by distribution family .;;We generated a collection of synthetic - D datasets each containing samples .;Simple 1 - D Distributions;16
79507;33998aff64ce51df8dee45989cdca4b6b1329ec4;Material;1;;;PPI dataset;It is worth noting the improvements achieved on the PPI dataset : Our GAT model improves by 20.5 % w.r.t .;More specifically , we are able to improve upon GCNs by a margin of 1.5 % and 1.6 % on Cora and Citeseer , respectively , suggesting that assigning different weights to nodes of a same neighborhood may be beneficial .;the best GraphSAGE result we were able to obtain , demonstrating that our model has the potential to be applied in inductive settings , and that larger predictive power can be leveraged by observing the entire neighborhood .;Method PPI;12
99862;41b38da2f4137c957537908f9cb70cbd2fac8bc1;Material;1;;;CK +;The resulting subset of features is used for classification in the extended Cohn - Kanade dataset ( CK + ) .;Forward sequential feature selection reduces the number of features to 7 .;By using the selected spatial features , 88.7 % recognition accuracy is obtained .;Introduction;1
75213;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;0;;;Labeled Faces in the Wild dataset;Face recognition performance evaluation has traditionally focused on one - to - one verification , popularized by the Labeled Faces in the Wild dataset for imagery and the YouTubeFaces dataset for videos .;;In contrast , the newly released IJB - A face recognition dataset unifies evaluation of one - to - many face identification with one - to - one face verification over templates , or sets of imagery and videos for a subject .;Template Adaptation for Face Verification and Identification;0
102897;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;;spatial MNIST;We created a dataset called spatial MNIST .;Building on the previous experiments we investigate 2 - D datasets that have complex structure , but the datapoints contain little information by themselves , making it a good test of the statistic network .;In spatial MNIST each image from MNIST mnist is turned into a dataset by interpreting the normalized pixel intensities as a probability density and sampling coordinate values .;Spatial MNIST;17
74824;2f92b10acf7c405e55c74c1043dabd9ded1b1800;Material;1;;;Wiktionary;We use ConceptNethttp: // conceptnet.io / Speer2012 , a freely - available , multi - lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources , such as Wiktionary , Open Multilingual WordNet , OpenCyc and DBpedia .;;It presents information in the form of relational triples .;Supplementary Knowledge Sources;10
105705;450e9676991b91e6b5eba3f77ac95dd0d3d6b655;Material;1;;;ImageNet - 1k datasets;In tests using CIFAR - 10 , CIFAR - 100 , and ImageNet - 1k datasets , our PyramidNets outperform all previous state - of - the - art deep network architectures .;"We also developed a novel residual unit , which includes a new building block for a residual unit with a zero - padded shortcut ; this design leads to significantly improved generalization ability .";Furthermore , the insights in this paper could be utilized by any network architecture , to improve their capacity for better performance .;Conclusion;15
2897;01959ef569f74c286956024866c1d107099199f7;Material;1;;;multiple - choice - real;Screenshots of leaderboards for open - ended - real and multiple - choice - real are shown in Fig . 13 .;| mc - abstract ] . 2 ) Compare their test - standard accuracies with those on the corresponding test2015 leaderboards [ oe - real - leaderboard | oe - abstract - leaderboard | mc - real - leaderboard | mcabstract - leaderboard ] . For more details , please see the challenge page [ reference ] .;We also compare the test - standard accuracies of our best model ( deeper LSTM Q + norm I ) for both open - ended and multiple - choice tasks ( real images ) with other entries ( as of October 28 , 2016 ) on the corresponding leaderboards in Table 5 .;VQA CHALLENGE AND WORKSHOP;17
53270;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;1;;;CUB - 200 - 2011 dataset;We conduct this experiment for four different models : GoogLeNet [ reference ] , ResNet - 50 [ reference ] and VGGNet - 16 [ reference ] and Bilinear - CNN [ reference ] on the CUB - 200 - 2011 dataset .;Choice of Hyperparameter λ : Since our formulation requires the selection of a hyperparameter λ , it is important to study the sensitivity of classification performance to the choice of λ .;PC 's performance is not very sensitive to the choice of λ ( Figure 2 and Supplementary Tables S1 - S5 ) .;Fine - Grained Visual Classification;13
86172;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;1;;;ITOP datasets;Note that our model outperforms previous works by a large margin without epoch ensemble on the ICVL , NYU , MSRA , and ITOP datasets while running in real - time using a single GPU .;The next step is network forwarding , which takes 5 ms and takes 0.5 ms to extract 3D coordinates from the 3D heatmap .;;Computational complexity;17
71033;2d5dba33c706d907733f15e7b57fde9909894e29;Material;1;;;IC15;We mix the training set of TD500 with the training set of IC15 , in the way that every batch has half of its images coming from each dataset .;The training set of TD500 only has 300 images , which are not enough for finetuning our model .;The pretrained model is finetuned for k iterations .;Detecting Multi - Lingual Text in Long Lines;26
54074;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;Cambridge Learner Corpus;As previous studies ji2017nested , we use the public Lang - 8 Corpus mizumoto2011mining , tajiri2012tense , Cambridge Learner Corpus ( CLC ) nicholls2003cambridge and NUS Corpus of Learner English ( NUCLE ) dahlmeier2013building as our original error - corrected training data .;;Table [ reference ] shows the stats of the datasets .;Dataset and evaluation;12
28082;0f0cab9235bbf185acdd4f9713fd111ca50effca;Material;1;;;RAF ) dataset;For comparing our deep learning architectures for image - based facial expression recognition against standard results , we use Static Facial Expressions in the Wild ( SFEW ) 2.0 dataset and Real - world Affective Faces ( RAF ) dataset .;;SFEW 2.0 contains 1394 images , of which 958 are to be used for training and 436 for validation .;Image - based Facial Expression Recognition;16
27534;0ee850dd6640a96531ac5ad21da5438db04d8b3c;Material;1;;;Robust04;We used over six million queries to train our models and evaluated them on Robust04 and ClueWeb;We examine various neural ranking models with different ranking architectures and objectives , and different input representations .;09 - Category B collections , in an ad - hoc retrieval setting .;Conclusions;13
74117;2ebfc12285f5d426e0d0e8d2befa1af27f99a56e;Material;0;;;PASCAL - VOC - 2011 segmentation dataset;SYM - PASCAL is derived from the PASCAL - VOC - 2011 segmentation dataset and targets object symmetry detection in the wild .;It is split into 228 train images and 100 test images .;It consists of 648 train images and 787 test images .;Dataset and evaluation protocol;9
14821;0891ed6ed64fb461bc03557b28c686f87d880c9a;Material;1;;;CoNLL - 2003 datasets;’s ability to generalize to different languages , we present results on the CoNLL - 2002 and CoNLL - 2003 datasets that contain independent named entity labels for English , Spanish , German and Dutch .;To demonstrate our model;All datasets contain four different types of named entities : locations , persons , organizations , and miscellaneous entities that do not belong in any of the three previous categories .;Data Sets;16
26515;0e8753f550350e53824358ca3f0f8cfd2f2dc2f7;Material;1;;1;MovieLens 10 M dataset;We work with the widely used MovieLens 10 M dataset , containing ratings ( ‘ stars ’ ) from to ( increments of ) given by 71 , 567 users for 10 , 677 movies .;In this section , we report experiments on real data , which appear consistent with the results on the aforementioned artificial data .;The density of the observations is .;Movielens dataset;9
88212;380b2c78d21ae6c43d418b6f0cb0222d5293d345;Material;1;;1;CoNLL 2009 shared task;In order to be able to compare with similar greedy parsers we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task .;Flattening the sampling distribution ( ) is especially beneficial when training with pretrained word embeddings .;Since some of the treebanks contain nonprojective sentences and arc - hybrid does not allow nonprojective trees , we use the pseudo - projective approach .;Experiments;7
34201;12e20e4ea572dbe476fd894c5c9a9930cf250dd2;Material;0;;0;Stanford Question Answering dataset;Recently , released the Stanford Question Answering dataset ( SQuAD ) , which is almost two orders of magnitude larger than all previous hand - annotated datasets .;However , these datasets are either not large enough to support deep neural network models or too easy to challenge natural language .;Moreover , this dataset consists 100 , 000 + questions posed by crowdworkers on a set of Wikipedia articles , where the answer to each question is a segment of text from the corresponding passage , rather than a limited set of multiple choices or entities .;Machine Reading Comprehension Dataset .;17
94393;3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a;Material;1;;0;MPII Human Pose Multi - Person;Extensive experiments on benchmarks MPII Human Pose Multi - Person , extended PASCAL - Person - Part , and WAF , show the efficiency of GPN with new state - of - the - art performance .;GPN is implemented with the Hourglass architecture as the backbone network to simultaneously learn joint detector and dense regressor .;;Generative Partition Networks for Multi - Person Pose Estimation;0
67266;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;kein dataset;na;Real Noisy Images;subsection : Experiments on Real Noisy Images;When the ground - truth noise level is unknown , it is more preferable to set a larger input noise level than a lower one to remove noise with better perceptual quality .;In this subsection , real noisy images are used to further assess the practicability of FFDNet .;Experiments on Real Noisy Images;18
85387;36911f5fc4f4eb1221f832114946de4773cf78e6;Material;0;kein dataset;1;English Wikipedia paragraphs;The corpus consists of all of the English Wikipedia paragraphs , except the abstracts .;The relevant passages are the paragraphs within that section .;The released dataset has five predefined folds , and we use the first four as a training set ( approximately 3 M queries ) , and the remaining as a validation set ( approximately 700k queries ) .;TREC - CAR;8
91451;3b1b94441010615195a5c404409ce2416860508c;Material;0;kein dataset;na;scale knowledge base;The third input source is the textual knowledge which is mined from a large - scale knowledge base , the DBpedia .;Average - pooling is applied over the 5 hidden - state vectors , to obtain a vector representation for the image .;More details are shown in the following section .;A VQA Model with External Knowledge;9
58647;23dcfda130aada27c158c0b5f394cac489c9c795;Material;0;;0;AFLW dataset;presents an in - depth study of relatively shallow networks trained using a regression loss on the AFLW dataset .;Also recently , work has developed on estimating head pose using neural networks .;In KEPLER the authors present a modified GoogleNet architecture which predicts facial keypoints and pose jointly .;RELATED WORK;2
33212;1109b663453e78a59e4f66446d71720ac58cec25;Material;1;;1;ImageNet 2012 training set;We train the network on the ImageNet 2012 training set ( 1.2 million images and classes ) .;;Our model uses the same fixed input size approach proposed by Krizhevsky et al .;Model Design and Training;4
97666;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;0;kein dataset;na;"in - the - wild "" images";"This is mainly because 2DASL involves a number of the "" in - the - wild "" images for training , enabling the model to perform well in cases even unseen in the 3D annotated training data .";Although all the state - of - the - art methods of dense face alignment conduct evaluation on AFLW2000 - 3D , the ground truth of AFLW2000 - 3D is controversial [ reference ][ reference ] , since its annotation pipeline is based on the Landmarks Marching method in [ reference ] . As can be seen , our results are more accurate than the ground truth in some cases .;For fair comparison , we adopt the normalized mean error ( NME ) [ reference ] as the metric to evaluate the alignment performance .;Dense face alignment;15
98443;40b4596a0ae4f4ff065f3f13f36db39543e50068;Material;1;;1;Cityscapes datset;We conduct extensive experiments by using the GTAV dataset and the Cityscapes datset .;The two modules above can be easily integrated with existing state - of - the - art semantic segmentation networks to boost their generalizability when adapting from synthetic to real urban scenes .;Our proposed method achieves a new state - of - the - art of mean IoU.;Introduction;1
101913;42764b57d0794b63487a295ce8c07eeb6961477e;Material;1;?;1;2012 trainval;Finally , we train the MNC model on the union set of 2007 trainval + test and 2012 trainval .;Using these box layers ’ outputs ( box coordinates and scores ) in place of the mask - level outputs , we obtain an mAP of 73.5 % ( Table [ reference ] ) .;As the 2007 set has no mask - level annotation , when a sample image from the 2007 set is used , its mask regression loss is ignored ( but the mask is generated for the later stages ) and its mask - level IoU measure for determining positive / negative samples is ignored .;Experiments on PASCAL VOC 2012;11
101906;42764b57d0794b63487a295ce8c07eeb6961477e;Material;0;;0;2007 set;We note that our result is obtained with fewer training images ( without the 2007 set ) , but with mask - level annotations .;Table [ reference ] shows that our result ( 70.9 % ) compares favorably to the recent Fast / Faster R - CNN systems .;This experiment shows the effectiveness of our algorithm for detecting both box - and mask - level instances .;Experiments on PASCAL VOC 2012;11
75840;302207c149bdf7beb6e46e4d4afbd2fa9ac02c64;Material;1;;0;Radboud Faces Database;The Radboud Faces Database ( RaFD ) consists of 4 , 824 images collected from 67 participants .;RaFD .;Each participant makes eight facial expressions in three different gaze directions , which are captured from three different angles .;Datasets;9
1745;01125e3c68edb420b8d884ff53fb38d9fbe4f2b8;Material;0;;1;Florence;To measure the effect of facial expressions on performance , we rendered frontal images in difference expressions from BU - 4DFE ( since Florence only exhibits a neutral expression ) and measured the performance for each expression .;Certain expressions are usually considered harder to accurately reproduce in 3D face reconstruction .;This kind of extreme acted facial expressions generally do not occur in the training set , yet as shown in Fig .;Ablation studies;11
53827;2116b2eaaece4af9c28c32af2728f3d49b792cf9;Material;1;;1;CIFAR - 10 without dropout;Our model for CIFAR - 10 without dropout is a CNN with three convolutional layers .;;Pooling layers follow all three .;Models for CIFAR - 10;20
65163;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;;0;Portuguese;"They use a convolutional neural network ( CNN ; or convnet ) and evaluated their model on English ( PTB ) and Portuguese , showing that the model achieves state - of - the - art performance close to taggers using carefully designed feature templates .";For POS tagging , santos : zadrozny:2014 were the first to propose character - based models .;ling :;Related Work;11
12281;06c5b86b638b2f3572b9cdd9ef0be4740b16781b;Material;1;;1;SST datasets;Our model has robust superiority over competitors and sets state - of - the - art on MR and SST datasets .;We summarize the experimental results in Table red [ reference ] .;First , our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge ( e.g. , RNTN , LSTM , BiLSTM , CNN and ID - LSTM ) on both datasets .;Experiment Results;10
2046;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;1;COCO Dataset;Note that the COCO Dataset is not a long text dataset , in which most sentences are of about 10 words .;We take the image captions as the text to generate .;Thus we apply some preprocessing on the dataset .;Middle Text Generation : COCO Image Captions;13
6538;03a5b2aac53443e6078f0f63b35d4f95d6d54c5d;Material;1;;1;BSD datasets;’s run time on Set14It should be noted our results outperform all other algorithms in accuracy on the larger BSD datasets .;In this section , we evaluated our best model;However , the use of Set14 on a single CPU core is selected here in order to allow a straight - forward comparison with results from previous published results [ ] .;Run time evaluations;14
33375;1109b663453e78a59e4f66446d71720ac58cec25;Material;1;;1;ImageNet 2012;These were never addressed in and thus we are the first to explain how this can be done in the context of ImageNet 2012 .;A second important contribution of our paper is explaining how ConvNets can be effectively used for detection and localization tasks .;The scheme we propose involves substantial modifications to networks designed for classification , but clearly demonstrate that ConvNets are capable of these more challenging tasks .;Discussion;15
91785;3b1d8eb163ffff598c2faa0d9d7cf933857a359f;Material;0;;0;conf / emnlp / BowmanAPM15;More recently the availability of much larger annotated data , e.g. , SNLI DBLP : conf / emnlp / BowmanAPM15 and MultiNLI DBLP : journals / corr / WilliamsNB17 , has made it possible to train more complex models .;Early research on natural language inference and recognizing textual entailment has been performed on relatively small datasets ( refer to MacCartneyThesis for a good literature survey ) , which includes a large bulk of contributions made under the name of RTE , such as Dagan2005ThePR , Iftene : W07 - 1421 , among many others .;These models mainly fall into two types of approaches : sentence - encoding - based models and models using also inter - sentence attention .;Related Work;2
97737;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;0;kein dataset;na;RGB face images;"In this section , we perform ablation study on AFLW2000 - 3D by evaluating several variants of our model : ( 1 ) 2DASL ( base ) , which only takes the RGB images as input without self - supervision and self - critic supervision ; ( 2 ) 2DASL ( cyc ) , which takes as input the combination of RGB face images and the corresponding 2D FLMs with self - supervison , however without self - critic supervision ; ( 3 ) 2DASL ( sc ) , which takes as input the RGB face images only using self - critic learning .";;( 4 ) 2DASL ( cyc + sc ) , which contains both self - supervision and self - critic supervision .;Ablation study;17
81381;34f63959ea4a13a05948274a1558c6854a051150;Material;1;;0;SST - 2;paragraph : SST - 2;It uses Matthews correlation coefficient as the evaluation metric .;The Stanford Sentiment Treebank is to determine the sentiment of sentences .;SST - 2;18
83390;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;0;kein dataset;na;sentence - aligned subtitles of TED;The corpus consists of sentence - aligned subtitles of TED and TEDx talks .;We use data from the German - English machine translation track of the IWSLT 2014 evaluation campaign cettolo2014 .;We pre - process the training data using the tokenizer of the Moses toolkit koehn2007 and remove sentences longer than words as well as casing .;Machine Translation;13
92532;3c1d781f2dab8da12e3cb0e4d7abfb440a340a09;Material;1;;0;multinli;This design is inspired by the reported annotation tags in multinli .;To qualitatively evaluate the performance of our models , we design a set of annotation tags that can be extracted automatically .;The specifications of our annotation tags are as follows :;Analysis;15
8745;052443e1709c0f7d3432cca7c451534eea76b7ca;Material;1;;1;B100;We validate our seven techniques on standard SR benchmarks ( i.e. Set5 , Set14 , B100 ) and methods ( i.e. A + , SR - CNN , ANR , Zeyde , Yang ) and achieve substantial improvements .;: 1 ) augmentation of data , 2 ) use of large dictionaries with efficient search structures , 3 ) cascading , 4 ) image self - similarities , 5 ) back projection refinement , 6 ) enhanced prediction by consistency check , and 7 ) context reasoning .;The techniques are widely applicable and require no changes or only minor adjustments of the SR methods .;Abstract;1
107752;46018a894d533813d67322827ca51f78aed6d59e;Material;1;;1;BRATS 2015 challenge;Using our best performing method , we took part in the BRATS 2015 challenge .;However , their method is very similar to the LocalPathCNN which , according to our experiments , has worse performance .;The BRATS 2015 training dataset comprises of 220 subjects with high grade and 54 subjects with low grade gliomas .;Cascaded architectures;15
5346;02e85d62fbd8249a046d00ac10e39546511b2a51;Material;1;;1;BRATS );For brain tumors , we evaluate our system on the data from the 2015 Brain Tumor Segmentation Challenge ( BRATS ) ( ) .;;The training set consists of 220 cases with high grade ( HG ) and 54 cases with low grade ( LG ) glioma for which corresponding reference segmentations are provided .;Material and Pre - Processing;22
99476;41232a69c0f8d4b993e6c6e00b16c223442c962f;Material;1;;1;Gigaword benchmark;Experiments on the Gigaword benchmark demonstrate that our model greatly reduce fake summaries by 80 % .;Then , we propose the dual - attention s2s framework to force the generation conditioned on both source sentence and the fact descriptions .;In addition , since the fact descriptions often condense the meaning of the sentence , the import of them also brings significant improvement on informativeness .;Conclusion and Future Work;17
5378;02e85d62fbd8249a046d00ac10e39546511b2a51;Material;1;;1;BRATS test data;Table [ reference ] shows the results of our method on the BRATS test data .;DeepMedic behaves very well in preserving the hierarchical structure of the tumor , which we account to the large context processed by our multi - scale network .;Results of other submissions are not accessible .;Results;24
81359;34f63959ea4a13a05948274a1558c6854a051150;Material;1;;1;SciTail;We evaluate the proposed MT - DNN on three popular NLU benchmarks : GLUE , Stanford Natural Language Inference ( SNLI ) , and SciTail .;;We compare MT - DNN with existing state - of - the - art models including BERT and demonstrate the effectiveness of MTL for model fine - tuning using GLUE and domain adaptation using SNLI and SciTail .;Experiments;15
96453;3f45d73a7b8d10a59a68688c11950e003f4852fc;Material;1;;1;Retinex images;With the Retinex images , we apply the HSV color histogram to extract color features .;This makes person re - identification easier than using the original images .;In addition to color description , we also apply the Scale Invariant Local Ternary Pattern ( SILTP ) descriptor for illumination invariant texture description .;Dealing with Illumination Variations;4
79874;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;1;IMDB - MULTI;In second round , we used social network datasets namely : COLLAB , IMDB - BINARY , IMDB - MULTI , REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset contains enzymes and non - enzymes proteins structures .;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;For other datasets details can be found in .;Experiment and Results;12
11408;063ad0349f05c8aacbbb653ffcf01047a293fa30;Material;1;;1;SentiHood dataset;As a testbed for this task , we introduce the SentiHood dataset , extracted from a question answering ( QA ) platform where urban neighbourhoods are discussed by users .;In particular , we identify the sentiment towards each aspect of one or more entities .;In this context units of text often mention several aspects of one or more neighbourhoods .;SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods;0
73205;2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79;Material;0;kein dataset;na;Conflict;The datasets are summarized in Table [ reference ] and include networks for Protein interactions , Metabolic pathways , military Conflict between countries , the U.S. PowerGrid , collaboration between users on the BlogCatalog social website , and publication citations from the Cora , Citeseer , Pubmed , Arxiv - GRQC databases .;We evaluate our proposed autoencoder models on nine graph - structured datasets , spanning multiple application domains , from which previous graph embedding methods have achieved strong results for LPNC .;{ Protein , Metabolic , Conflict , PowerGrid } are reported in .;Datasets and Baselines;7
86582;36c3972569a6949ecca90bfa6f8e99883e092845;Material;0;;0;VQA 2.0 test - std split;Their performance reached 70.34 % on VQA 2.0 test - std split with an ensemble of 30 models trained with different seeds .;Multi - modal fusion is done through a simple Hadamard product followed by a multi - label classifier using a sigmoid activation function to predict the answer scores .;"For presentation clarity , we present our proposed changes ( and the respective improvements ) in a sequence ; however , we also found them to be independently useful .";Bottom - Up and Top - Down Attention;2
91600;3b1b94441010615195a5c404409ce2416860508c;Material;1;;1;VQA test split;The human ground truth answers for the actual VQA test split are not available publicly and only can be evaluated via the VQA evaluation server .;We randomly choose 5000 images from the validation set as our val set , with the remainder testing .;Hence , we also apply our final model on a test split and report the overall accuracy .;Evaluation on Visual Question Answering;17
40862;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;kein dataset;na;vocalized - noise;The phonetic inventory includes special models for noise , vocalized - noise , laughter and silence .;Additionally , we have trained several models with 27k tied states .;We use a 30k - vocabulary derived from the most common words in the Switchboard and Fisher corpora .;Acoustic Model Details;21
2882;01959ef569f74c286956024866c1d107099199f7;Material;1;;1;VQA train + val dataset;": We created a filtered version of the VQA train + val dataset in which we only keep the answers with subject confidence "" yes "" .";Filtered Dataset;"Also , we keep only those questions for which at least 50 % ( 5 out of 10 ) answers are annotated with subject confidence "" yes "" .";4 ) K = 2000 :;16
96952;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;0;kein dataset;na;domain specific data;In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .;Once pretrained , the biLM can compute representations for any task .;This can be seen as a type of domain transfer for the biLM .;Pre - trained bidirectional language model architecture;7
71225;2d83dbf4c8eabc6bdef3326c4a30d5f33ffc944e;Material;0;;0;MS - COCO dataset;The images come from the MS - COCO dataset , 123 , 287 of them for training and validation , and 81 , 434 for test .;Table [ reference ] shows that Other type has the most benefit from Multiple - Choice .;The images are carefully collected to contain multiple objects and natural situations , which is also valid for visual question - answering tasks .;Visual QA Dataset;9
77656;31ae4873da19b1e28eca8787a17f49bba08627e5;Material;1;;1;trainval35k ’ set;For the COCO dataset , we use the ‘ trainval35k ’ set for training and the ‘ minival ’ set for testing .;MS COCO .;During training the Fast - RCNN , we apply SGD with 320 K iterations .;Experimental settings;11
2314;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;1;ImageNet classification dataset;Our neural networks , named ResNeXt ( suggesting the next dimension ) , outperform ResNet - 101 / 152 [ reference ] , ResNet - 200 [ reference ] , Inception - v3 [ reference ] , and Inception - ResNet - v2 [ reference ] on the ImageNet classification dataset .;Experiments demonstrate that increasing cardinality is a more effective way of gaining accuracy than going deeper or wider , especially when depth and width starts to give diminishing returns for existing models .;In particular , a 101 - layer ResNeXt is able to achieve better accuracy than ResNet - 200 [ reference ] but has only 50 % complexity .;Introduction;2
96334;3f45d73a7b8d10a59a68688c11950e003f4852fc;Material;1;;1;CUHK Campus;Experiments on four challenging person re - identification databases , VIPeR , QMUL GRID , CUHK Campus , and CUHK03 , show that the proposed method improves the state - of - the - art rank - 1 identification rates by 2.2 % , 4.88 % , 28.91 % , and 31.55 % on the four databases , respectively .;We also present a practical computation method for XQDA , as well as its regularization .;;Person Re - identification by Local Maximal Occurrence Representation and Metric Learning;0
41752;1751668492bac56f0ae2b6410417515ab3215945;Material;1;;1;PTB - WSJ dataset;paragraph : PTB - WSJ dataset .;subsection : Results;Table [ reference ] shows the POS tagging results .;PTB - WSJ dataset .;20
84542;360cfa09b2f7c8e10b1831d899c5a51aefa1883e;Material;0;;0;CHiME;More information on CHiME data can be found in .;The experiments reported in this paper are based on the single channel setting , in which the test phase is carried out with a single microphone ( randomly selected from the considered microphone setup ) .;To evaluate the proposed model on a larger scale ASR task , some additional experiments were performed with the TED - talk dataset , that was released in the context of the IWSLT evaluation campaigns .;Corpora and tasks;9
102241;42e80c73867bff9eaff6beceb8730fc1276283b9;Material;1;;1;WMT 2014;Our experiments show the effectiveness of our approach , as we improve the previous state - of - the - art in unsupervised machine translation by 5 - 7 BLEU points in French - English and German - English WMT 2014 and 2016 .;In addition to that , we use our improved SMT approach to initialize a dual NMT model that is further improved through on - the - fly back - translation .;In the future , we would like to explore learnable similarity functions like the one proposed by mccallum2005conditional to compute the character - level scores in our initial phrase - table .;Conclusions and future work;13
40860;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;kein dataset;na;phonetic inventory;The phonetic inventory includes special models for noise , vocalized - noise , laughter and silence .;Additionally , we have trained several models with 27k tied states .;We use a 30k - vocabulary derived from the most common words in the Switchboard and Fisher corpora .;Acoustic Model Details;21
102179;42e80c73867bff9eaff6beceb8730fc1276283b9;Material;1;;1;German - English;In order to make our experiments comparable to previous work , we use the French - English and German - English datasets from the WMT 2014 shared task .;SMT + NMT;More concretely , our training data consists of the concatenation of all News Crawl monolingual corpora from 2007 to 2013 , which make a total of 749 million tokens in French , 1 , 606 millions in German , and 2 , 109 millions in English , from which we take a random subset of 2 , 000 sentences for tuning ( Section [ reference ] ) .;Experiments and results;9
33279;1109b663453e78a59e4f66446d71720ac58cec25;Material;1;;;ImageNet Fall11 dataset;Pre - training with extra data from the ImageNet Fall11 dataset improved this number to 11.2 % .;The best accuracy using only ILSVRC13 data was 11.7 % .;In post - competition work , we improve the OverFeat results down to 13.6 % error by using bigger models ( more features and more layers ) .;Results;7
97933;4087ebc37a1650dbb5d8205af0850bee74f3784b;Material;1;;;WikiTest 2;2 shows the training and testing perplexity of the L2 model on PTB and WikiTest 2 as trained via the baseline schedule along with our best CBS schedule ( from Table 1 ) .;Fig .;Notice the cyclical spikes in training and testing perplexity .;Language Results;6
68643;2c03df8b48bf3fa39054345bafabfeff15bfd11d;Material;1;;;PAS - CAL VOC sets;With the single model on the COCO dataset ( 55.7 % mAP@.5 in Table 9 ) , we fine - tune this model on the PAS - CAL VOC sets .;We revisit the PASCAL VOC dataset based on the above model .;The improvements of box refinement , context , and multi - scale testing are also adopted .;PASCAL VOC;18
63329;27c761258329eddb90b64d52679ff190cb4527b5;Material;0;;;CHASH_DB1 dataset;The CHASH_DB1 dataset contains 28 color retina images and the size of each image is 999×960 pixels [ reference ] .;"In this implementation , we used the "" leaveone - out "" approach for STARE dataset .";The images in this dataset were collected from both left and right eyes of 14 school children .;A. Database Summary 1 ) Blood Vessel Segmentation;6
26869;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;kein dataset;;displayed image;wintermute2010 proposed a method that also extracts objects from the displayed image and embeds them into a logic - based architecture , SOAR .;Their results show that DOORMAX can discover the optimal behaviour for this OO - MDP within one episode .;Their method uses a forward model of the scene to improve the performance of the Q - Learning algorithm .;Atari Games;33
67127;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;CBSD68 dataset;The CBSD68 dataset is the corresponding color version of the grayscale;“ RNI15 ” .;BSD68 dataset .;Dataset Generation and Network Training;14
91241;3b1b94441010615195a5c404409ce2416860508c;Material;0;kein dataset;;image descriptions;composed image descriptions given computer vision based inputs such as detected objects , modifiers and locations using web - scale - grams .;Li et al .;Zhu et al .;Image Captioning;4
47737;1c7e078611c9df412e6eb3a356f31a0da0c1f99c;Material;0;;;LINEMOD dataset;For example , the popular LINEMOD dataset provides manual annotations for around 1 , 000 images for each of the 15 objects in the dataset .;Object - centric datasets providing ground - truth annotations for object poses and / or segmentations are limited in size by the fact that the annotations are typically provided manually .;While such a dataset is useful for evaluation of model - based pose estimation techniques , it is orders of magnitude smaller than a typical dataset for training state - of - the - art deep neural networks .;The YCB - Video Dataset;8
53253;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;1;;;NABirds;On NABirds , we obtain improvements of 4.60 % and 3.42 % over baseline ResNet - 50 and DenseNet - 161 architectures .;For instance , a baseline DenseNet - 161 architecture obtains an average accuracy of 84.21 % , but PC - DenseNet - 161 obtains an accuracy of 86.87 % , an improvement of 2.66 % .;2 .;Fine - Grained Visual Classification;13
44592;1a5ea605111eb3403868d4b679315e944beee8c6;Material;0;kein dataset;;Cyrillic;We apply the Latin BPE operations to the English data ( training data and input ) , and both the Cyrillic and Latin BPE operations to the Russian data .;We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the BPE operations on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .;Translation results are shown in Table [ reference ] .;English Russian;12
52485;207e0ac5301a3c79af862951b70632ed650f74f7;Material;1;;;PRID2011 datasets;For semi - supervised setting , we use the VIPeR and PRID2011 datasets .;;The same data splits are used as in the fully - supervised setting .;Semi - supervised Learning Results;13
47426;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;1;;;PASCAL VOC test set;This baseline obtains 21.6 % mAP detection score on the PASCAL VOC test set , which is well below the state - of - the - art ( 31.6 % in [ reference ] ) .;The loss function is once more a sum of C binary hinge - losses , one for each class .;Pre - trained CNN architectures .;Detection results;12
64581;289e91654f6da968d625481ef21f52892052d4fc;Material;0;;;KanShan - Cup;Considering the user privacy and data security , KanShan - Cup does not provide the original texts of the questions and topics , but uses numbered codes and numbered segmented words to represent text messages .;Zhihu dataset .;Therefore , it is inconvenient for researchers to perform analyses like visualization and case study .;Datasets;20
70091;2d294bde112b892068636f3a48300b3c033d98da;Material;0;kein dataset;;COFW faces;The training set includes LFPW faces and COFW faces , and the testing set includes remaining COFW faces .;It contains images annotated with landmarks .;IBUG [ ] contains testing images which present large variations in pose , expression , illumination , and occlusion .;Datasets;14
32058;10a36dea0167511b66deca65fdca978aa9afdb11;Material;1;;;COCO VQA dataset;In the COCO VQA dataset , there are 3 questions annotated by Amazon Mechanical Turk ( AMT ) workers for each image in the COCO dataset .;Here we train and evaluate the iBOWIMG model on the Full release of COCO VQA dataset , the largest VQA dataset so far .;For each question , 10 answers are annotated by another batch of AMT workers .;Experiments;3
56631;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;ImageNet images;The larger is , the larger is the disturbance of the CelebA dataset by contaminating it by ImageNet images .;means all images are from CelebA , means that 75 % of the images are from CelebA and 25 % from ImageNet etc .;The larger the disturbance level is , the more the dataset deviates from the reference real world dataset .;Fréchet Inception Distance ( FID );18
67120;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;quantized “ Clip300 ” dataset;In particular , to evaluate FFDNet - Clip , we use the quantized “ Clip300 ” dataset which comprises the 100 images of test set from the BSD300 dataset and 200 images from PASCALVOC 2012 dataset .;The RNI6 dataset contains 6 real noisy images without ground - truth .;Note that all the testing images are not included in the training dataset .;Dataset Generation and Network Training;14
76165;303065c44cf847849d04da16b8b1d9a120cef73a;Material;1;;;KF - ITW Dataset;section : KF - ITW Dataset;By keeping the number of vertexes small ( ) , we manage to greatly speed - up the fitting process without any accuracy penalty .;For the evaluation of the 3DMM , we have constructed KF - ITW , the first dataset of 3D faces captured under relatively unconstrained conditions .;KF - ITW Dataset;11
88223;380b2c78d21ae6c43d418b6f0cb0222d5293d345;Material;1;;;WMT 2015 dataset;"( 2015 ) ; for English and Chinese we used the same pretrained word embeddings as in Table [ reference ] , for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3 .";We also include results with pretrained word embeddings for English , Chinese , German , and Spanish following the same training setup as Dyer et al .;See Table [ reference ] .;Experiments;7
77207;3112d2d95d66b3d54a72c55072647aab937e410e;Material;0;;;RotoWire development;To accomplish this , raters were presented with a particular NBA game 's box score and line score , as well as with ( randomly selected ) sentences from summaries generated by our different models for Table 2 : Performance of induced metrics on gold and system outputs of RotoWire development and test data .;The first study attempted to determine whether generations considered to be more precise by our metrics were also considered more precise by human raters .;Columns indicate Record Generation ( RG ) precision and count , Content Selection ( CS ) precision and recall , Count Ordering ( CO ) in normalized Damerau - Levenshtein distance , perplexity , and BLEU .;Human Evaluation;12
26684;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;kein dataset;;Atari 2600 screen;The Basic method , derived from naddaf2010 ’s BASS naddaf2010 , encodes the presence of colours on the Atari 2600 screen .;;The Basic method first removes the image background by storing the frequency of colours at each pixel location within a histogram .;Basic .;9
77259;3112d2d95d66b3d54a72c55072647aab937e410e;Material;0;kein dataset;;NBA games;"The ROTOWIRE data covers NBA games played between 1 / 1 / 2014 and 3 / 29 / 2017 ; some games have multiple summaries .";;The summaries have been randomly split into training , validation , and test sets consisting of 3398 , 727 , and 728 summaries , respectively .;A. Additional Dataset Details;16
45134;1a6b67622d04df8e245575bf8fb2066fb6729720;Material;1;;;PTB;subsection : Model and training for PTB and WikiText - 2;For the remaining ones , we perform light hyperparameter search in the vicinity of those reported for AWD - LSTM in and for AWD - LSTM - MoS in .;For the single softmax model ( AWD - LSTM + PDR ) , for both PTB and WT2 , we use a 3 - layered LSTM with 1150 , 1150 and 400 hidden dimensions .;Model and training for PTB and WikiText - 2;4
88908;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;Pascal VOC 2012 segmentation benchmark;We apply the proposed method to the problem of semantic image segmentation , obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark .;Importantly , our system fully integrates CRF modelling with CNNs , making it possible to train the whole deep network end - to - end with the usual back - propagation algorithm , avoiding offline post - processing methods for object delineation .;;Abstract;1
31833;1023b20d226bd0af9fdf0fd1847accefbfa5ec84;Material;0;;;Children ’s Book Test;The third dataset , the Children ’s Book Test ( CBT ) , is built from books that are freely available thanks to Project Gutenberg .;;Each context document is formed by consecutive sentences taken from a children ’s book story .;Children ’s Book Test;6
9881;05ee231749c9ce97f036c71c1d2d599d660a8c81;Material;1;;;IJB - B benchmark;For the same measures on the more challenging IJB - B benchmark , our network achieves the best TAR at FAR=1E − 5 and FAR=1E − 4 , and is only lower than a concurrent work [ reference ] at FAR=1E − 3 and FAR=1E − 2 .;"The only points for which the GhostVLAD network does n't beat the state - of - the - art , though it is on par with it , is in TPIR at Rank - 1 to Rank - 10 for identification on IJB - A ; but this is because IJB - A is not challenging enough and the TPIR values have saturated to a 99 % mark .";Furthermore , our networks produce much smaller template descriptors than the previous stateof - the - art networks ( 128 - D vs 2048 - D ) , making them more useful in real - world [ reference ] ( priv1 ) and [ reference ] (;Comparison with state - of - the - art;13
64731;28eceb438da0b841bbd3d02684dbfa263838ed60;Material;1;;;Cityscapes dataset of urban scenes;"( a ) Semantic layouts from the Cityscapes dataset of urban scenes ; semantic classes are coded by color .";Given a pixelwise semantic layout , the presented model synthesizes an image that conforms to this layout .;( b ) Images synthesized by our model for these layouts .;;3
101298;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;VOC dataset;Then we fine - tune the COCO detection model on the VOC dataset .;This result is better than that trained on VOC07 + 12 ( 73.2 % ) by a good margin , even though the PASCAL VOC data are not exploited .;In this experiment , the COCO model is in place of the ImageNet - pre - trained model ( that is used to initialize the network weights ) , and the Faster R - CNN system is fine - tuned as described in Section [ reference ] .;From MS COCO to PASCAL VOC;13
81370;34f63959ea4a13a05948274a1558c6854a051150;Material;0;;;SciTail datasets;This section briefly describes the GLUE , SNLI , and SciTail datasets , as summarized in Table [ reference ] .;;"The GLUE benchmark is a collection of nine NLU tasks , including question answering , sentiment analysis , and textual entailment ; it is considered well - designed for evaluating the generalization and robustness of NLU models .";Datasets;16
69107;2c761495cf3dd320e229586f80f868be12360d4e;Material;1;;;PASCAL VOC 2007 Test;For PASCAL VOC , we use the 16551 ‘ trainval ’ images from PASCAL VOC 2007 and 2012 for training , and report performance on the PASCAL VOC 2007 Test , which has 4952 images using mAP@.5 metric .;‘ test - dev ’ split ( evaluated by the official result server ) .;We use the TensorFlow Faster RCNN implementation and adopt their default training hyperparameters except for learning rate schedules .;Experimental Setup;11
47430;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;1;;;VOC 2007 dataset;Table 1 shows that WSDDN with individual models S and M are already on par with the state - ofthe - art method [ reference ] and the ensemble outperforms the best previous score in the VOC 2007 dataset .;We evaluate our method with the models S , M and L and also report the results for the ensemble of these models by simply averaging their scores .;Differently from supervised detection methods ( e.g. [ reference ] ) , detection performance of WSDDN does not improve with use of wider or deeper networks .;Detection results;12
20168;0b544dfe355a5070b60986319a3f51fb45d1348e;Material;0;kein dataset;;English to French;The proposed RNN Encoder – Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French .;Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .;We train the model to learn the translation probability of an English phrase to a corresponding French phrase .;Introduction;1
50967;1e7a36c4d4f96b29e3edf51b6eb61f8e16217704;Material;1;;;WikiText - 2 dataset;We also apply a purely byte - level mLSTM on the WikiText - 2 dataset to achieve a character level entropy of 1.26 bits / char , corresponding to a word level perplexity of 88.8 , which is comparable to word level LSTMs regularised in similar ways on the same task .;In this version of the paper , we regularise mLSTM to achieve 1.27 bits / char on text8 and 1.24 bits / char on Hutter Prize .;;Multiplicative LSTM for sequence modelling;0
12236;06c5b86b638b2f3572b9cdd9ef0be4740b16781b;Material;0;kein dataset;;blue;Movie Review ( MR ) blue and Stanford Sentiment Treebank;;( SST ) blue are used to evaluate our model .;Datasets and Sentiment Resources;7
45772;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;1;;;MovieLens dataset;Our set of movies were also matched to the MovieLens dataset .;;We built a KB using OMDb and MovieLens metadata with entries for each movie and nine different relation types : director , writer , actor , release year , language , genre , tags , IMDb rating and IMDb votes , with 10k related actors , 6k directors and 43k entities in total .;KB;14
47277;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;0;;;images of PASCAL VOC;[ 23 ] employ a pre - trained CNN to compute a mid - level image representation for images of PASCAL VOC .;Oquab et al .;In their follow - up work , Oquab et al .;Related Work;3
64591;289e91654f6da968d625481ef21f52892052d4fc;Material;0;kein dataset;;TextCNN;For the word - based models , we reported the results from TextCNN , TextRNN and FastText .;Word - based Model .;The three models got the best performance in the KanShan - Cup competition , so we applied them as the word - based baselines .;Baselines;21
64577;289e91654f6da968d625481ef21f52892052d4fc;Material;0;;;Chinese community question answering platform;This dataset is released by a competition of tagging topics for questions ( multi - label classification ) posted in the largest Chinese community question answering platform , Zhihu .;KanShan - Cup dataset .;The dataset contains 3 , 000 , 000 questions and 1 , 999 topics ( classes ) , where one question may belong to one to five topics .;Datasets;20
40527;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;kein dataset;;Linguistic Data Consortium;( We have submitted the human transcripts thus produced to the Linguistic Data Consortium for publication , so as to facilitate research by other groups . );These numbers should be taken as an indication of the “ error rate ” of a trained professional working in industry - standard speech transcript production .;Past work reports inter - transcriber error rates for data taken from the later RT03 test set ( which contains Switchboard and Fisher , but no CallHome data ) .;Human Performance;2
6263;0373b97580cdfd0b69f165e1a946bae62da95dce;Material;1;;;CIFAR - 100 dataset;[ reference ] shows that for CIFAR - 100 dataset as well , our model learns faster than the original ResNet model .;Fig .;The original model yields a test error of 27.23 % , which is already state - of - the - art in CIFAR - 100 with standard data augmentation .;CIFAR - 100 Analysis;14
65123;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;kein dataset;;Farsi;OOV Acc columns ) , especially for languages like Arabic , Farsi , Hebrew , Finnish .;Table [ reference ];We examined simple RNNs and confirm the finding of ling : ea:2015 that they performed worse than their LSTM counterparts .;Results;7
48729;1d696a1beb42515ab16f3a9f6f72584a41492a03;Material;1;;;YouTube Faces;By increasing the dimension of hidden representations and adding supervision to early convolutional layers , DeepID2 + achieves new state - of - the - art on LFW and YouTube Faces benchmarks .;It is learned with the identification - verification supervisory signal .;Through empirical studies , we have discovered three properties of its deep neural activations critical for the high performance : sparsity , selectiveness and robustness .;Deeply learned face representations are sparse , selective , and robust;0
70996;2d5dba33c706d907733f15e7b57fde9909894e29;Material;0;;;ICDAR 2013;paragraph : ICDAR 2013 ( IC13 );Different from IC15 , TD500 is annotated at the level of text lines .;contains mostly horizontal text , with some text slightly oriented .;ICDAR 2013 ( IC13 );23
88435;3861ae2a6bdd2a759c2d901a6583e63a216bc2fc;Material;1;;;WMT 2014 English - to - German data set;The WMT 2014 English - to - German data set contains M sentence pairs .;We benchmark our proposed architecture on the WMT 2014 English - to - German and English - to - French tasks .;The English - to - French contains M sentence pairs .;Results on Benchmark Data Sets;7
95610;3e95925d2bca43223453010ff8516a492287ce19;Material;0;;;Paraphrase Database;Moreover , to outperform previous methods , the NBT uses pretrained embeddings tailored to retain semantic relationships by injecting semantic similarity constraints from the Paraphrase Database .;The NBT employs convolutional filters over word embeddings in lieu of previously - used hand - engineered features .;On the one hand , these specialized embeddings are more difficult to obtain than word embeddings from language modeling .;Related Work;13
51519;1ea6b2f67a3a7f044209aae0d0fd1cb14a1e9e06;Material;1;;;ImageNet datasets;Then we give results on the relative effectiveness of architectural components and our best results on the MNIST , CIFAR - 10 and ImageNet datasets .;In Section [ reference ] we give details about the training .;;Experiments;13
51664;1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f;Material;1;;;CIFAR - 100;We report improved results using a 1001 - layer ResNet on CIFAR - 10 ( 4.62 % error ) and CIFAR - 100 , and a 200 - layer ResNet on ImageNet .;This motivates us to propose a new residual unit , which makes training easier and improves generalization .;Code is available at : .;Identity Mappings in Deep Residual Networks;0
93284;3d18ce183b5a5b4dcaa1216e30b774ef49eaa46f;Material;1;;;300W.;In this paper , we test the performance of 3DDFA on three different tasks , including the large - pose face alignment on AFLW , 3D face alignment on AFLW2000 - 3D and medium - pose face alignment on 300W.;;;Comparison Experiments;20
102876;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;;omniglot;In all cases experiments were terminated after a given number of epochs when training appeared to have sufficiently converged ( epochs for omniglot , youtube and spatial MNIST examples , and epochs for the synthetic experiment ) .;We primarily use the Theano theano framework with the Lasagne lasagne library , but the final experiments with face data were done using Tensorflow tensorflow .;;Experimental Results;15
85369;36911f5fc4f4eb1221f832114946de4773cf78e6;Material;0;kein dataset;;passage text;We also truncate the passage text such that the concatenation of query , passage , and separator tokens have the maximum length of 512 tokens .;B. We truncate the query to have at most 64 tokens .;We use a model as a binary classification model , that is , we use the vector as input to a single layer neural network to obtain the probability of the passage being relevant .;Method;4
91658;3b1b94441010615195a5c404409ce2416860508c;Material;1;;;MS COCO captions;This is not surprising because the Toronto COCO - QA questions were generated automatically from the MS COCO captions , and thus the fact that they can be answered by training on the captions is to be expected .;Cap + Know - LSTM also performs better than Att + Know - LSTM .;This generation process also leads to questions which require little external information to answer .;Results on Toronto COCO - QA;19
76452;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;;WT2;Compared to the vanilla AWD - LSTM , though being more expressive , MoC performs only better on PTB , but worse on WT2 .;The results are shown in Table [ reference ] .;It suggests that simply adding another hidden layer or employing a mixture structure in the feature space does not guarantee a better performance .;Ablation Study;15
54573;220a0b46840a2a1421c62d3d343397ab087a3f17;Material;1;;;Middlebury;Both learned methods are less accurate than Classic + NL on Middlebury but both are also significantly faster .;SPyNet is significantly more accurate on Middlebury , where FlowNet has trouble with the small motions .;;KITTI and Middlebury .;10
26941;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;;;Atari 2600 Pong;In sheer size , the Atari 2600 Pong is thus a larger domain .;"Games can easily last 10 , 000 time steps ( compared to 200–1000 in other domains ) ; observations are composed of 7 - bit images ( compared to black and white images in the work of stober08pixels , or 5 - 6 input features elsewhere ) ; observations are also more complex , containing the two players ’ score and side walls .";Its dynamics are also more complicated .;Final Remarks;35
34602;1329206dbdb0a2b9e23102e1340c17bd2b2adcf5;Material;1;;;Caltech - UCSD bird dataset;As we show later , average recall of parts using selective search proposals is 95 % on the Caltech - UCSD bird dataset .;We conjecture that progress made on bottom - up region proposal methods , like selective search , could benefit localization of smaller parts in addition to whole objects .;In this paper , we propose a part localization model which overcomes the limitations of previous fine - grained recognition systems by leveraging deep convolutional features computed on bottom - up region proposals .;Introduction;1
102945;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;;;Youtube Faces;subsection : Youtube Faces;This means that it can exaggerate differences between similar classes , which are more likely to appear in a 20 - way problem than a 5 - way problem .;Finally , we provide a proof of concept for generating faces of a particular person .;Youtube Faces;19
36794;151313065d71b49dbf07289c002c887d7b5a0a6b;Material;0;;;Criteo Dataset;Criteo Dataset :;1 );Criteo dataset includes 45 million users ’ click records .;Datasets;9
13315;074b6fe0cc6848fb86a6703d1c52074494177c79;Material;0;kein dataset;;winter domain image;In Figure [ reference ] we show the result of pixel only adaptation as we generate a winter domain image ( b ) from a fall domain image ( a ) , and visa versa ( c - d ) .;Therefore , we use this setting as an example where we may directly visualize the shift from fall to winter and inspect the intermediate pixel level adaptation result from our algorithm .;We may clearly see the changes of adding or removing snow .;Cross - season adaptation;7
68574;2c03df8b48bf3fa39054345bafabfeff15bfd11d;Material;1;;;VOC 2012;"For the PASCAL VOC 2012 test set , we use the 10k trainval + test images in VOC 2007 and 16k trainval images in VOC 2012 for training ( "" 07 ++ 12 "" ) .";"Following [ reference ][ reference ] , for the PASCAL VOC 2007 test set , we use the 5k trainval images in VOC 2007 and 16k trainval images in VOC 2012 for training ( "" 07 + 12 "" ) .";The hyper - parameters for training Faster R - CNN are the same as in [ reference ] . Table 7 shows the results .;PASCAL VOC;15
78757;32a93598e8a338496f04a0ace81b0768c2ef059d;Material;1;;;IWSLT 2015;The Thai - English data comes from IWSLT 2015 .;The teacher model is a LSTM ( as in Luong2015 ) and we train two student models : and .;There are k sentences in the training set and we take 2010 / 2011 / 2012 data as the dev set and 2012 / 2013 as the test set , with a vocabulary size is;Experimental Setup;9
103384;43428880d75b3a14257c3ee9bda054e61eb869c0;Material;1;;;WMT'14 English - German;C. Attention Visualization Figure 3 shows attention scores for a generated sentence from the WMT'14 English - German task .;Both terms of this inequality have rotational symmetry w.r.t 0 , and thus;The model used for this plot has 8 decoder layers and a 80 K BPE vocabulary .;B. Upper Bound on Squared Sigmoid;31
3718;020a9aba95bce75dca08e3c499efc9e100f1cbb6;Material;1;;;Atari domain;To evaluate the speed of the learning process , we propose the AUC - 100 benchmark to evaluate learning progress on the Atari domain .;Aside from achieving a high final score , our method also achieves substantially faster learning .;;Introduction;1
2846;01959ef569f74c286956024866c1d107099199f7;Material;1;;;VQA validation set;9 , we show the average accuracy of the model on questions with 50 most frequent ground truth answers on the VQA validation set ( plot is sorted by accuracy , not frequency ) .;In Fig .;"We can see that the model performs well for answers that are common visual objects such as "" wii "" , "" tennis "" , "" bathroom "" while the performance is somewhat underwhelming for counts ( e.g. , "" 2 "" , "" 1 "" , "" 3 "" ) , and particularly poor for higher counts ( e.g. , "" 5 "" , "" 6 "" , "" 10 "" , "" 8 "" , "" 7 "" ) .";Results;15
47349;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;1;;;2010 datasets;We evaluate our method on the PASCAL VOC 2007 and 2010 datasets [ reference ] , as they are the most widely - used benchmark in weakly supervised object detection .;;While the VOC 2007 dataset consists of 2501 training , 2510 validation , and 5011 test images containing bounding box annotations for 20 object categories , VOC 2010 dataset contains 4998 training , 5105 validation , and 9637 test images for the same number of categories .;Benchmark data .;10
80689;34a6762ed8e92612ba4fdf02ee95d2ee0d587908;Material;1;;;Market1501 dataset;We initially tried with Adam , that gave an accuracy of 74 % on Rank 1 on Market1501 dataset , then we introduced weight decay that helped us achieve higher accuracy .;We tried many optimizers .;We then tried SGD with Cyclical Learning Rate scheduler , we were able to achieve much higher accuracy .;Optimizer;6
26967;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;kein dataset;;Natural Science and Engineering Research Council;The work presented here was supported by the Alberta Innovates Technology Futures , the Alberta Innovates Centre for Machine Learning at the University of Alberta , and the Natural Science and Engineering Research Council of Canada .;We would also like to thank our reviewers for their helpful feedback and enthusiasm about the Atari 2600 as a research platform .;Invaluable computational resources were provided by Compute / Calcul Canada .;Conclusion;36
52778;20926884a62778a2bf3f9f3c56f30976749ad763;Material;0;;;Infant Health and Development Program;compiled a dataset for causal effect estimation based on the Infant Health and Development Program ( IHDP ) , in which the covariates come from a randomized experiment studying the effects of specialist home visits on future cognitive test scores .;;The treatment groups have been made imbalanced by removing a biased subset of the treated population .;Simulated outcome : IHDP;18
69777;2cf6a8389135f682b0cb727a07f4e77c097d5434;Material;1;;;Letter;[ reference ] , where there are three sub - figures showing the speed of four methods on the benchmark datasets of Letter , MNIST and Waveform respectively .;The results are illustrated in Fig .;As we shall see , both selection time of TED and RRSS increases dramatically as increases .;Speed vs. increasing;22
3970;0217fb2a54a4f324ddf82babc6ec6692a3f6194f;Material;0;;;MNIST dataset;Another intriguing line of work consists of the ladder network [ reference ] , which has achieved spectacular results on a semi - supervised variant of the MNIST dataset .;A lot of promising recent work originates from the Skip - gram model [ reference ] , which inspired the skip - thought vectors [ reference ] and several techniques for unsupervised feature learning of images [ reference ] .;More recently , a model based on the VAE has achieved even better semi - supervised results on MNIST [ reference ] . GANs [ reference ] have been used by Radford et al .;Related Work;3
73322;2e57bccb74bcb46cbc5b4225b62679023ed1f9da;Material;0;;;machine comprehension datasets;Finally , by accounting for a dynamic termination state during inference and applying proposed deep reinforcement learning optimization method , ReasoNets achieve the state - of - the - art results in machine comprehension datasets , including unstructured CNN and Daily Mail datasets , and the proposed structured Graph Reachability dataset , when the paper is rst publicly available on arXiv .;Motivated by [ reference ][ reference ] , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .;[ reference ];INTRODUCTION;2
53395;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;1;;;CUB - 200 - 2011;For example , on CUB - 200 - 2011 with ResNet - 50 , the average false positive error is increased by 0.06 % , but Table S6 .;We also found that using PC slightly increased false positive errors while obtaining a larger reduction in false negative errors .;Pairwise Confusion ( PC ) obtains state - of - the - art performance on six widelyused fine - grained visual classification datasets ( A - F ) .;S5 Changes to Class - wise Prediction Accuracy;24
72808;2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79;Material;0;;;Wikidata;You May Know ” ) , terrorist networks , communication networks , cybersecurity , recommender systems , and knowledge bases such as DBpedia and Wikidata .;“ People;"There are a number of technical challenges associated with learning to make meaningful predictions on complex graphs : Extreme class imbalance : in link prediction , the number of known present ( positive ) edges is often significantly less than the number of known absent ( negative ) edges , making it difficult to reliably learn from rare examples ;";Introduction;1
40530;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;Switchboard and Fisher;Past work reports inter - transcriber error rates for data taken from the later RT03 test set ( which contains Switchboard and Fisher , but no CallHome data ) .;( We have submitted the human transcripts thus produced to the Linguistic Data Consortium for publication , so as to facilitate research by other groups . );Error rates of 4.1 to 4.5 % are reported for extremely careful multiple transcriptions , and 9.6 % for “ quick transcriptions .;Human Performance;2
103165;43428880d75b3a14257c3ee9bda054e61eb869c0;Material;1;;;WMT'14 EnglishFrench;[ reference ] . All models are implemented in Torch [ reference ] and trained on a single Nvidia M40 GPU except for WMT'14 EnglishFrench for which we use a multi - GPU setup on a single machine .;Besides dropout on the embeddings and the decoder output , we also apply dropout to the input of the convolutional blocks;"We train on up to eight GPUs synchronously by maintaining copies of the model on each card and split the batch so that each worker computes 1 / 8 - th of the gradients ; at the end we sum the gradients via Nvidia NCCL .";Model Parameters and Optimization;14
28413;0f2f4edb7599de34c97f680cf356943e57088345;Material;0;kein dataset;;elbow;Our results on FLIC are very competitive reaching 99 % PCK@0.2 accuracy on the elbow , and 97 % on the wrist .;Results can be seen in Figure [ reference ] and Table [ reference ] .;It is important to note that these results are observer - centric , which is consistent with how others have evaluated their output on FLIC .;Evaluation;9
68303;2c03df8b48bf3fa39054345bafabfeff15bfd11d;Material;1;;;ImageNet test set;Our 152 - layer residual net is the deepest network ever presented on ImageNet , while still having lower complexity than VGG nets [ reference ] . Our ensemble has 3.57 % top - 5 error on the ImageNet test set , and won the 1st place in the ILSVRC 2015 classification competition .;On the ImageNet classification dataset [ reference ] , we obtain excellent results by extremely deep residual nets .;The extremely deep representations also have excellent generalization performance on other recognition tasks , and lead us to further win the 1st places on : ImageNet detection , ImageNet localization , COCO detection , and COCO segmentation in ILSVRC & COCO 2015 competitions .;Introduction;2
67040;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;kein dataset;;Noisy Images;subsection : Un - clipping vs. Clipping of Noisy Images for Training;For simplicity , we do not use residual learning for network design .;In the AWGN denoising literature , there exist two widely - used settings , i.e. , un - clipping and clipping , of synthetic noisy image to evaluate the performance of denoising methods .;Un - clipping vs. Clipping of Noisy Images for Training;12
77058;3112d2d95d66b3d54a72c55072647aab937e410e;Material;1;;;ROTOWIRE dataset;example data - record and document pair from the ROTOWIRE dataset .;The data statistics of these two sources , RO - TOWIRE and SBNATION , are also shown in Ta An;We show a subset of the game 's records ( there are 628 in total ) , and a selection from the gold document .;Data - to - Text Datasets;3
74823;2f92b10acf7c405e55c74c1043dabd9ded1b1800;Material;0;;;Open Mind Common Sense project;We use ConceptNethttp: // conceptnet.io / Speer2012 , a freely - available , multi - lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources , such as Wiktionary , Open Multilingual WordNet , OpenCyc and DBpedia .;;It presents information in the form of relational triples .;Supplementary Knowledge Sources;10
83408;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;1;;;Imagenet dataset;"The context is represented by 1024 features extracted by a Convolutional Neural Network ( CNN ) trained on the Imagenet dataset imagenet_cvpr09 ; we do not back - propagate through these features .";At training time we sample one of these captions , while at test time we report the maximum BLEU score across the five captions .;We use a similar experimental set up as described in sbengio - nips2015 .;Image Captioning;14
79873;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;IMDB - BINARY;In second round , we used social network datasets namely : COLLAB , IMDB - BINARY , IMDB - MULTI , REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset contains enzymes and non - enzymes proteins structures .;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;For other datasets details can be found in .;Experiment and Results;12
21028;0b5aef2894d3248fb5ecc955d50501f0aa276036;Material;1;;;IEMOCAP;We think that this is due to the audio and video modality of IEMOCAP being richer than the same of CMU - MOSI .;One key observation for IEMOCAP dataset is that its A + V modality combination performs significantly better than the same of CMU - MOSI dataset .;The performance difference with another strong baseline is even more ranging from 2.1 % to 3 % on CMU - MOSI dataset and 2.2 % to 5 % on IEMOCAP dataset .;IEMOCAP;38
65655;2a69ddbafb23c63e5e22401664bea229daaeb7d6;Material;1;;;HKU - IS;Following , we train those two models using the MSRA - B dataset , and evaluate results on ECSSD , PASCAL - S , HKU - IS , and DUT - OMRON datasets .;For fair comparison , we only replace the backbone with ResNet - 50 and our proposed Res2Net - 50 , while keeping other configurations unchanged .;The F - measure and Mean Absolute Error ( MAE ) are used for evaluation .;Salient Object Detection;26
4673;02a5b7a41ffa8518eb3b7cae9914a2bd2bbc886b;Material;0;;;VOT - 2018;Despite its simplicity and fast speed , SiamMask establishes a new state - of - the - art on VOT - 2018 for the problem of real - time object tracking .;Once trained , SiamMask solely relies on a single bounding box initialisation , operates online without updates and produces object segmentation masks and rotated bounding boxes at 35 frames per second .;Moreover , the same method is also very competitive against recent semi - supervised VOS approaches on DAVIS - 2016 and DAVIS - 2017 , while being the fastest by a large margin .;Init;3
86624;36c3972569a6949ecca90bfa6f8e99883e092845;Material;1;;;Visual Genome and Visual Dialog;We added additional training data from Visual Genome and Visual Dialog ( VisDial v0.9 ) datasets .;;For VisDial , we converted the 10 turns in a dialog to 10 independent question - answer pairs .;Data Augmentation;6
79868;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;PROTEINS;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;To evaluate our GCAPS - CNN model , we perform graph classification tasks on variety of benchmark datasets .;In second round , we used social network datasets namely : COLLAB , IMDB - BINARY , IMDB - MULTI , REDDIT - BINARY and REDDIT - MULTI - 5K. D & D dataset contains enzymes and non - enzymes proteins structures .;Experiment and Results;12
85021;3652c2d20f198dc39ad159eba55d08341c56d628;Material;1;;;STL - 10 datasets;We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well , e.g. , digit recognition with the MNIST dataset , and the more challenging CIFAR - 10 and STL - 10 datasets , where our accuracy is competitive with the state of the art . positioning , decorations.pathreplacing;Second , we bridge a gap between the neural network literature and kernels , which are natural tools to model invariance .;;Convolutional Kernel Networks;0
40847;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;Switchboard ( SWB;Evaluation is carried out on the NIST 2000 CTS test set , which comprises both Switchboard ( SWB ) and CallHome ( CH ) subsets .;We train with the commonly used English CTS ( Switchboard and Fisher ) corpora .;The waveforms were segmented according to the NIST partitioned evaluation map ( PEM ) file , with 150ms of dithered silence padding added in the case of the CallHome conversations .;Speech corpora;20
62476;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;0;kein dataset;;NVIDIA Tesla K40c;5.1 on an NVIDIA Tesla K40c .;We observe an average conversion time of 8.9 milliseconds per patch from the keypoint matching benchmark in Sec .;3DMatch Descriptor .;A.3 . Run - time Information;24
94442;3e58fbb8cb96880e018ca18a60e2d86e3cb0c10a;Material;0;;;WAF;Extensive experiments on MPII Human Pose Multi - Person , extended PASCAL - Person - Part and WAF benchmarks evidently show the efficiency and effectiveness of the proposed GPN .;We implement GPN based on the Hourglass network for learning joint detector and dense regressor , simultaneously .;Moreover , GPN achieves new state - of - the - art on all these benchmarks .;Introduction;1
98992;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;0;;;vanilla RGB - D image patch;While the model in the first row directly estimates box location and parameters from vanilla RGB - D image patch , the other one ( second row ) uses a FCN trained from the COCO dataset for 2D mask estimation ( as that in Mask - RCNN ) and only uses features from the masked region for prediction .;The baseline methods trained VGG models on ground truth boxes of RGB - D images and adopt the same box parameter and loss functions as our main method .;The depth values are also translated by subtracting the median depth within the 2D mask .;Comparing with alternative approaches for 3D detection .;21
81655;3526555fa0178c101ee9896252c818f9e03532a5;Material;0;;;Tobacco - 3482 datasets;[ reference ] shows the sample images both real and document images from the ImageNet and Tobacco - 3482 datasets respectively .;Fig .;While the images are visually very different , the visual queues are generic and thus , transfer learning helps to boost the performance of the document image classification .;Introduction;1
106623;45b559e6271570598602fcf9777ed6f2f2d133e6;Material;1;;;Babel languages;We trained the multilingual deep CNN architecture on 6 Babel languages using alignments from 6 baseline speaker independent HMM / DNN systems using PLP features , with 1000 context dependent states .;We report results after cross - entropy training with adadelta ( , ) , and varying from 0 to 1 .;The context dependent states are specific to each language .;Babel;8
86152;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;1;;;ITOP top - view datasets;The qualitative results of the V2V - PoseNet on the ICVL , NYU , MSRA , HANDS 2017 , ITOP front - view , and ITOP top - view datasets are shown in Figure 9 , 10 , 11 , 12 , 13 , and 14 , respectively .;As shown in Table 5 , the proposed system outperforms all the existing methods by a large margin in both of views , which indicates that our model can be applied to not only 3D hand pose estimation , but also other challenging problems such as 3D human pose estimation from the front - and top - views .;;Comparison with state - of - the - art methods;16
51184;1e7a36c4d4f96b29e3edf51b6eb61f8e16217704;Material;0;;;Text8;Text8 contains 100 million characters of English text taken from Wikipedia in 2006 , consisting of just the 26 characters of the English alphabet plus spaces .;;This dataset can be found at .;Text8 dataset;11
107361;46018a894d533813d67322827ca51f78aed6d59e;Material;0;kein dataset;;CT;Furthermore , unlike images derived from X - ray computed tomography ( CT ) scans , the scale of voxel values in MR images is not standardized .;Another fundamental difficulty with segmenting brain tumors is that they can appear anywhere in the brain , in almost any shape and size .;Depending on the type of MR machine used ( 1.5 , 3 or 7 tesla ) and the acquisition protocol ( field of view value , voxel resolution , gradient strength , b0 value , etc . ) , the same tumorous cells may end up having drastically different grayscale values when pictured in different hospitals .;Introduction;1
106601;45b559e6271570598602fcf9777ed6f2f2d133e6;Material;0;;;Babel training data;"Firstly , in our experience the optimization problem converges much faster than with SGD ; for the Babel experiments we typically see convergence after about 40 million frames using the 18 hours of Babel training data ( after silence removal about 5.8 million frames ) .";Using Adadelta has two main advantages .;Secondly , the optimal working point of Adadelta ’s hyperparameters and was stable across architectures , always giving optimal performance .;Training;6
97647;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;1;;;UMDFaces dataset;"The "" in - the - wild "" face images are all from the UMDFaces dataset [ reference ]";This dataset contains more than 60 K face images with annotated 3DMM coefficients .;that contains 367 , 888 still face images for 8 , 277 subjects .;Training details and datasets;14
38838;15e07c1344e97e46ade2ee0a57017371fa05fe12;Material;1;;;BUCC;We next present our results on the BUCC mining task , UN corpus reconstruction and downstream machine translation over the filtered ParaCrawl corpus .;;Unless otherwise indicated , all experiments use an English / French / Spanish / German multilingual encoder trained on Europarl v7 for 10 epochs as described in Section [ reference ] .;Experiments and results;7
86571;36c3972569a6949ecca90bfa6f8e99883e092845;Material;0;;;Visual Genome dataset;The key idea in up - down is the use of an object detector – Faster RCNN pre - trained on the Visual Genome dataset – to extract image features with bottom - up attention , , visual feed - forward attention .;We perform ablations and augmentations over the baseline system of the up - down model , which was the basis of the winning entry to the 2017 VQA challenge .;Specifically , a ResNet - 101 was chosen as the backbone network , and its entire Res - 5 block was used as the second - stage region classifier for detection .;Bottom - Up and Top - Down Attention;2
48900;1d696a1beb42515ab16f3a9f6f72584a41492a03;Material;1;;;LFW test set;[ reference ] compares the face verification accuracies of DeepID2 + and LBP features on LFW test set with varying degrees of partial occlusion .;Fig .;The DeepID2 + features are taken from the FC - to FC - layers with increasing depth in a single DeepID2 + net taking the entire face region as input .;Robustness of DeepID2 + features;10
100913;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;0;kein dataset;;scale image;Because of this multi - scale design based on anchors , we can simply use the convolutional features computed on a single - scale image , as is also done by the Fast R - CNN detector .;We show by experiments the effects of this scheme for addressing multiple scales and sizes ( Table [ reference ] ) .;The design of multi - scale anchors is a key component for sharing features without extra cost for addressing scales .;Anchors;5
65656;2a69ddbafb23c63e5e22401664bea229daaeb7d6;Material;1;;;DUT - OMRON datasets;Following , we train those two models using the MSRA - B dataset , and evaluate results on ECSSD , PASCAL - S , HKU - IS , and DUT - OMRON datasets .;For fair comparison , we only replace the backbone with ResNet - 50 and our proposed Res2Net - 50 , while keeping other configurations unchanged .;The F - measure and Mean Absolute Error ( MAE ) are used for evaluation .;Salient Object Detection;26
95836;3f3a483402a3a2b800cf2c86506a37f6ef1a5332;Material;1;;;Leeds Sports Poses;“ Leeds Sports Poses ” ( LSP ) ( person - centric ( PC ) ) ,;We train and evaluate on three public benchmarks :;“ LSP Extended ” ( LSPET ) , and “ MPII Human Pose ”;Evaluation of Part Detectors;11
103186;43428880d75b3a14257c3ee9bda054e61eb869c0;Material;1;;;WMT'16 winning entry;Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a BPE encoding and by 1.3 BLEU with a word factored vocabulary .;We test both word - based and BPE vocabularies ( § 4 ) .;This instance of our architecture has 20 layes in the encoder and 20 layers in the decoder , both using kernels of width 3 and hidden size 512 throughout .;Recurrent vs. Convolutional Models;16
81591;34f63959ea4a13a05948274a1558c6854a051150;Material;0;;;QNLI;Two such examples use the SAN answer module for the pairwise text classification output module , and the pairwise ranking loss for the QNLI task which by design is a binary classification problem in GLUE .;The gain of MT - DNN is also attributed to its flexible modeling framework which allows us to incorporate the task - specific model structures and training methods which have been developed in the single - task setting , effectively leveraging the existing body of research .;To investigate the relative contributions of the above two modeling design choices , we implement different versions of MT - DNNs and compare their performance on the development sets .;GLUE Results;29
42080;1768909f779869c0e83d53f6c91764f41c338ab5;Material;1;;;CompCars ”;To facilitate future car - related research , in this paper we present our on - going effort in collecting a large - scale dataset , “ CompCars ” , that covers not only different car views , but also their different internal and external parts , and rich attributes .;We show that there are still many interesting car - related problems and applications , which are not yet well explored and researched .;Importantly , the dataset is constructed with a cross - modality nature , containing a surveillance - nature set and a web - nature set .;A Large - Scale Car Dataset for Fine - Grained Categorization and Verification;0
101238;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;test - dev set;Our Fast R - CNN baseline has 39.3 % mAP@0.5 on the test - dev set , higher than that reported in .;In Table [ reference ] we first report the results of the Fast R - CNN system using the implementation in this paper .;We conjecture that the reason for this gap is mainly due to the definition of the negative samples and also the changes of the mini - batch sizes .;Experiments on MS COCO;12
60880;2594a77a3f0dd5073f79ba620e2f287804cec630;Material;1;;;Shoulder - Pain dataset;We test our network on the Shoulder - Pain dataset that contains 200 videos of 25 subjects and is widely used for benchmarking the pain intensity estimation .;;The dataset comes with four types of labels .;Dataset and Training Details;8
55824;22aab110058ebbd198edb1f1e7b4f69fb13c0613;Material;1;;;ImageNet training;As a simple test for D ’s memorization ( related to ) , we evaluate uncollapsed discriminators on the ImageNet training and validation sets , and measure what percentage of samples are classified as real or generated .;One possible explanation for this behavior is that D is overfitting to the training set , memorizing training examples rather than learning some meaningful boundary between real and generated images .;While the training accuracy is consistently above 98 % , the validation accuracy falls in the range of 50 - 55 % , no better than random guessing ( regardless of regularization strategy ) .;Characterizing Instability : The Discriminator;8
65144;29c19276b8fff231717c3e342cb24144d2b77726;Material;0;;;non - Indoeuropean languages;For non - Indoeuropean languages it is on par and above the other taggers with even less data ( 100 sentences ) .;The bi - LSTM model performs already surprisingly well after only 500 training sentences .;This shows that the bi - LSTMs often needs more data than the generative markovian model , but this is definitely less than what we expected .;Data set size;9
65007;28eceb438da0b841bbd3d02684dbfa263838ed60;Material;0;kein dataset;;Pix2pix;Pix2pix rates finally diverge , although both are extremely high ( 95.1 % and 87.4 % , respectively ) , and the Real > CRN rate rises to 64.2 % .;At 500 milliseconds , the Real > Pix2pix and CRN >;Over time , the CRN >;Results;15
36371;14ad9d060c1e8f0449e697ee189ac346353fbfbc;Material;1;Zumindest Teile davon used;;BC5CDR;The BC5CDR dataset has the sub - datasets BC5CDR - chem , BC5CDR - disease and BC5CDR - both , and they contain chemical entity types , disease entity types , and both entity types , respectively .;So we preprocessed the original dataset by Kim et al . with a more accurate sentence separation .;We reported the performance on BC5CDR - chem and BC5CDR - disease .;Datasets;11
67418;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;kein dataset;;handing inhomogeneous noise;The results on images with spatially variant AWGN validated the flexibility of FFDNet for handing inhomogeneous noise .;The results on synthetic images with AWGN demonstrated that FFDNet can not only produce state - of - the - art results when input noise level matches ground - truth noise level , but also have the ability to robustly control the trade - off between noise reduction and detail preservation .;The results on real noisy images further demonstrated that FFDNet can deliver perceptually appealing denoising results .;Conclusion;20
15747;0985497d1de3ffd11713e75289cc2ad55836623d;Material;1;;;MS - MARCO;To get better insight into our system , we conduct in - depth ablation study on the development set of MS - MARCO , which is shown in Table [ reference ] .;;Following snet , we mainly focus on the ROUGE - L score that is averaged case by case .;Ablation Study;16
62391;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;da figure;;SUN3D datasets;5 shows challenging cases of loop closure from the testing split of the SUN3D datasets that are difficult for color - based descriptors to find correspondences due to drastic viewpoint differences .;Fig .;Our 3DMatch - based registration algorithm is capable of matching the local geometry to find correspondences and bring the scans into alignment .;Integrate 3DMatch in Reconstruction Pipeline .;16
68682;2c03df8b48bf3fa39054345bafabfeff15bfd11d;Material;1;;;1000 - class ImageNet training set;We train networks on the provided 1000 - class ImageNet training set .;We pre - train the networks for ImageNet classification and then fine - tune them for localization .;Our localization algorithm is based on the RPN framework of [ reference ] with a few modifications .;C. ImageNet Localization;20
2055;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;;Chinese poems;To evaluate the performance of LeakGAN in short text generation , we pick the dataset of Chinese poems which is proposed by [ reference ] and most related work such as [ reference ][ reference ][ reference ] ) .;;The dataset consists of 4 - line 5 - character poems .;Short Text Generation : Chinese Poems;14
6668;03a5b2aac53443e6078f0f63b35d4f95d6d54c5d;Material;1;;;Set14;However , the use of Set14 on a single CPU core is selected here in order to allow a straight - forward comparison with results from previous published results [ ] .;’s run time on Set14It should be noted our results outperform all other algorithms in accuracy on the larger BSD datasets .;with an upscale factor of 3 .;Run time evaluations;14
107938;462d4e265c9cbe9ad5feeb9a7736184a90b36fed;Material;1;;;U.S. census 2000;The PERSON gazetteer is collected from U.S. census 2000 , U.S. census 2010 and DBpedia whereas GeoNames is the main source for LOCATION , taking in both official and alternative names .;For the gazetteer feature , we focus on PERSON and LOCATION and compile a list for each .;All the tokens on both lists are then filtered to exclude frequently occurring common words .;Hand - crafted Features;8
2966;01959ef569f74c286956024866c1d107099199f7;Material;0;kein dataset;;multiple - choice questions;27 , Fig . 28 , and Fig . 29 , we show a random selection of the VQA dataset for the MS COCO [ reference ] images , abstract scenes , and multiple - choice questions , respectively .;In Fig .;;APPENDIX IX : ADDITIONAL EXAMPLES;29
99086;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;0;kein dataset;;LiDAR point clouds;The object detection benchmark in KITTI provides synchronized RGB images and LiDAR point clouds with ground truth amodal 2D and 3D box annotations for vehicles , pedestrians and cyclists .;;The training set contains 7 , 481 frames and an undisclosed test set contains 7 , 581 frames .;KITTI Training;32
60068;2451db113552afb6d9ad15ef4009ec4133d28f74;Material;1;;;large - scale ImageNet dataset;We validated our method on both large - scale ImageNet dataset and challenging fine - grained benchmarks .;Compared to existing works depending heavily on GPU unfriendly EIG or SVD , our method , based on coupled Newton - Schulz iteration , runs much faster as it involves only matrix multiplications , suitable for parallel implementation on GPU .;Given efficiency and promising performance of our iSQRT - COV , we hope global covariance pooling will be a promising alternative to global average pooling in other deep network architectures , e.g. , ResNeXt , Inception and DenseNet .;Conclusion;18
54116;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;Common Crawl;The language model is the 5 - gram language model trained on Common Crawl released by , which is also used for computing fluency score in Eq ( [ reference ] ) .;For single - round inference , we follow to generate 12 - best predictions and choose the best sentence after re - ranking with edit operation and language model scores .;As most of the systems sakaguchi2017grammatical , chollampatt2018 , grundkiewicz2018near evaluated on JFLEG that use an additional spell checker to resolve spelling errors , we use a public spell checker to resolve spelling errors in JFLEG as preprocessing .;Experimental setting;13
57354;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;MS MARCO 2.1 dataset;On the MS MARCO 2.1 dataset , Masque achieves state - of - the - art performance on the dataset ’s two tasks , Q & A and NLG , with different answer styles .;In this study , we propose a generative model , called Masque , for multi - passage RC .;The main contributions of this study are that our model enables the following two abilities .;Introduction;1
56892;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;large scale image database;The Large - scale CelebFaces Attributes ( CelebA ) dataset , aligned and cropped , the training dataset of the bedrooms category of the large scale image database ( LSUN ) , the CIFAR - 10 training dataset , the Street View House Numbers training dataset ( SVHN ) , and the One Billion Word Benchmark .;We used the following datasets to evaluate GANs :;All experiments rely on the respective reference implementations for the corresponding GAN model .;Used Software , Datasets , Pretrained Models , and Implementations;67
35815;143a3186c368544ded00a444be33153420baa254;Material;1;da figure;;10 - shot learning;In Figure 6 , we show the full quantitative results of the MAML model trained on 10 - shot learning and evaluated on 5 - shot , 10 - shot , and 20 - shot .;;In Figure 7 , we show the qualitative performance of MAML and the pretrained baseline on randomly sampled sinusoids .;B. Additional Sinusoid Results;21
91331;3b1b94441010615195a5c404409ce2416860508c;Material;0;kein dataset;;large - scale KB;Instead of building a problem - specific KB , we use a pre - built large - scale KB ( DBpedia ) from which we extract information using a standard RDF query language .;used a hand - crafted KB primarily containing image - related information such as category labels , attribute labels and affordance labels , but also some quantities relating to their specific question format such as GPS coordinates and similar .;DBpedia has been created by extracting structured information from Wikipedia , and is thus significantly larger and more general than a hand - crafted KB .;Visual Question Answering;5
106617;45b559e6271570598602fcf9777ed6f2f2d133e6;Material;0;?;;CEB;The languages used for training are languages from the second Option Period of the Babel program , i.e. Kurmanji ( KUR ) , Tok Pisin ( TOK ) , Cebuano ( CEB ) , Kazakh ( KAZ ) , Telugu ( TEL ) , and Lithuanian ( LIT ) .;As training data we use a combination of 6 languages , with 3 hours of training data per language .;The features used in these experiments are standard log - Mel features , standardized with a global mean and variance shared across the speakers and langauges .;Babel;8
56529;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;Image Data;paragraph : WGAN - GP on Image Data .;TTUR constantly outperforms standard training and is more stable .;We used the WGAN - GP image model to test TTUR with the CIFAR - 10 and LSUN Bedrooms datasets .;WGAN - GP on Image Data .;12
4454;027f9695189355d18ec6be8e48f3d23ea25db35d;Material;0;;;SNLI ) dataset;In the Stanford Natural Language Inference ( SNLI ) dataset [ reference ] ) , which we use for NLI experiments , a relationship is either contradiction , entailment , or neutral .;Natural language inference ( NLI ) is a task of predicting the relationship between two sentences ( hypothesis and premise ) .;For a model to correctly predict the relationship between two sentences , it should encode semantics of sentences accurately , thus the task has been used as one of standard tasks for evaluating the quality of sentence representations .;Natural Language Inference;10
52469;207e0ac5301a3c79af862951b70632ed650f74f7;Material;0;;;VIPeR;Compared with VIPeR and PRID2011 , these two datasets are much bigger with thousands of training samples .;Results on CUHK01 & CUHK03;However , the sample size is still much smaller than the feature dimension , i.e. the SSS problem still exists .;Fully Supervised Learning Results;12
50230;1e5b9e512c01e244287fe7afb05e03c96d5c1cd0;Material;0;kein dataset;;Japanese;The only cases for which this is not true are again languages that require significant segmentation efforts ( i.e. , Hebrew , Chinese , Vietnamese and Japanese ) or when the task was trivial .;Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task ( i.e. , 1.8 % absolute improvement on average , corresponding to a RRIE of 21.20 % ) .;Given the fact that [ reference ] obtained the best results in part - of - speech tagging by a significant margin in the CoNLL 2017 Shared Task , it would be expected that their model would also perform significantly well in morphological tagging since the tasks are very similar .;Morphological Tagging Results;15
23291;0ca2bd0e40a8f0a57665535ae1c31561370ad183;Material;1;;;Text8 test set;MI - RNN 1.52 Skipping - RNN [ reference ] Table 2 : BPC on the Text8 test set .;Model BPC td - LSTM 1.63 HF - MRNN 1.54;Penn Treebank;Text8;9
73421;2e57bccb74bcb46cbc5b4225b62679023ed1f9da;Material;1;;;Daily Mail dataset;For training our ReasoNet , we keep the most frequent |V | = 101k words ( not including 584 entities and 1 placeholder marker ) in the CNN dataset , and |V | = 151k words ( not including 530 entities and 1 placeholder marker ) in the Daily Mail dataset .;Vocab Size :;Embedding Layer :;CNN and Daily Mail Datasets;7
100816;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;0;;;image crops;The MultiBox proposal network is applied on a single image crop or multiple large image crops ( , 224 224 ) , in contrast to our fully convolutional scheme .;These class - agnostic boxes are used as proposals for R - CNN .;MultiBox does not share features between the proposal and detection networks .;Related Work;2
86834;3729a9a140aa13b3b26210d333fd19659fc21471;Material;1;;;SICK dataset;Textual entailment : For textual entailment , we also used the SICK dataset and exactly the same data split as the semantic relatedness dataset .;The evaluation metric is the Mean Squared Error ( MSE ) between the gold and predicted scores .;The evaluation metric is the accuracy .;Datasets;18
37010;1518039b5001f1836565215eb047526b3ac7f462;Material;1;;;WMT 15 translation tasks English→German;We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .;This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .;;Abstract;1
44565;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;WMT16 parallel set;After training a baseline model on all the WMT16 parallel set , we continued training with a parallel corpus consisting of 2 copies of the 2.5 M sentences of back - translated data , 5 copies of news - commentary v11 , and a matching quantity of data sampled from Czeng 1.6pre .;For Czech English the training process was similar to the above , except that we created the synthetic training data ( back - translated from samples of news2015 monolingual English ) in batches of 2.5 M , and so were able to observe the effect of increasing the amount of synthetic data .;After training this to convergence , we restarted training from the baseline model using 5 M sentences of back - translated data , 5 copies of news - commentary v11 , and a matching quantity of data sampled from Czeng 1.6pre .;English Czech;10
99128;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;1;;;CLS - LOC validation dataset;The resulting base network architecture has about 66.7 % top - 1 classification accuracy on the CLS - LOC validation dataset and only needs about 1.2ms to process a image on a NVIDIA GTX 1080 .;Then we fine - tune it on ImageNet CLS - LOC dataset for 400k iterations with batch size of 260 on 10 GPUs .;We then add the feature pyramid layers from conv3_3 , conv4_3 , conv5_3 , and fc7 , which are used to predict region proposals with scales of 16 , 32 , 64 , 128 respectively .;Details on RGB Detector ( Sec 4.1 );34
60791;2594a77a3f0dd5073f79ba620e2f287804cec630;Material;0;;;Shoulder - Pain;Particularly , Shoulder - Pain is the only dataset available for visual analysis with per - frame labels .;However , the ability to train deep CNNs for pain assessment is limited by small datasets with labels of patient - reported pain intensities , i.e. , annotated datasets such as EmoPain , Shoulder - Pain , BioVid Heat Pain .;It contains only 200 videos of 25 patients who suffer from shoulder pain and repeatedly raise their arms and then put them down ( onset - apex - offset ) .;Introduction;1
67117;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;RNI6 dataset;The RNI6 dataset contains 6 real noisy images without ground - truth .;The Set12 dataset is a collection of widely - used testing images .;In particular , to evaluate FFDNet - Clip , we use the quantized “ Clip300 ” dataset which comprises the 100 images of test set from the BSD300 dataset and 200 images from PASCALVOC 2012 dataset .;Dataset Generation and Network Training;14
26715;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;kein dataset;;Wikipedia page;Of these games , 123 games have their own Wikipedia page , have a single player mode , are not adult - themed or prototypes , and can be emulated in ALE .;Our testing set was constructed by choosing semi - randomly from the 381 games listed on Wikipedia at the time of writing .;From this list , 50 games were chosen at random to form the test set .;Evaluation Methodology;14
76416;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;;1B Word dataset;To investigate whether the effectiveness of MoS can be extended to even larger datasets , we conduct an additional language modeling experiment on the 1B Word dataset chelba2013one .;We heuristically and manually search hyper - parameters for MoS based on the validation performance while limiting the model size ( see Appendix [ reference ] for our hyper - parameters ) .;Specifically , we lower - case the text and choose the top 100 K tokens as the vocabulary .;Main Results;14
62231;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;RGB - D scanning data;In contrast , our descriptor focuses on learning geometric features for real - world RGB - D scanning data at a local level , to provide more robustness when dealing with partial data suffering from various occlusion patterns and viewpoint differences .;While these works are inspiring , their focus is centered on extracting features from complete 3D object models at a global level .;Learned 3D Local Descriptors .;Related Work;3
13301;074b6fe0cc6848fb86a6703d1c52074494177c79;Material;1;;;GTA5 to the real - world Cityscapes dataset;We consider adaptation from GTA5 to the real - world Cityscapes dataset cordts_cvpr16 , from which we used 19998 images without annotation for training and 500 images for validation .;For our synthetic source domain , we use the GTA5 dataset richter_eccv16 extracted from the game Grand Theft Auto V , which contains 24966 images .;Both of these datasets are evaluated on the same set of 19 classes , allowing for straightforward adaptation between the two domains .;Semantic Segmentation Adaptation;6
27024;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;kein dataset;;game screen;Unlike the previous three methods , which generate feature vectors based on the game screen , the RAM - based feature generation method relies on the contents of the console memory .;;The Atari 2600 has only bits of random access memory , which must hold the complete internal state of a game : location of game entities , timers , health indicators , etc .;RAM - based Feature Generation;42
97024;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;0;;;newswire;CoNLL2003NER consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .;The CoNLL 2003 NER task;Following recent state - of - the - art systems lample -;Evaluation;8
20272;0b544dfe355a5070b60986319a3f51fb45d1348e;Material;0;kein dataset;;French words;All the word counts refer to French words after tokenization .;To train the French language model , about 712 M words of crawled newspaper material is available in addition to the target side of the bitexts .;It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .;Data and Baseline System;10
26616;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;kein dataset;;unseen testing games;With the large number of available games in ALE , we propose that a similar methodology can be used to the same effect : an approach ’s domain representation and parametrization should be first tuned on a small number of training games , before testing the approach on unseen testing games .;The typical practice is instead to train on a training set then evaluate on a disjoint test set .;Ideally , agents designed in this fashion are evaluated on the testing games only once , with no possibility for subsequent modifications to the algorithm .;Introduction;1
40734;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;BBN Switchboard - 2 transcripts;The 4 - gram language model for decoding was trained on the available CTS transcripts from the DARPA EARS program : Switchboard ( 3 M words ) , BBN Switchboard - 2 transcripts ( 850k ) , Fisher ( 21 M ) , English CallHome ( 200k ) , and the University of Washington conversational Web corpus ( 191 M ) .;;A separate N - gram model was trained from each source and interpolated with weights optimized on RT - 03 transcripts .;Training data;12
91010;3a8d537bcec370d37990d39eab01c729496ad057;Material;0;;;ModelNet 40;There are two variants of the ModelNet dataset , ModelNet10 and ModelNet 40 , introduced in , with 10 and 40 target classes respectively .;ModelNet;ModelNet10 has 3D shapes which are pre - aligned with the same pose across all categories .;Datasets;10
3772;020a9aba95bce75dca08e3c499efc9e100f1cbb6;Material;0;;;Atari games benchmark;The most direct way of learning a dynamics model is to directly predict the state at the next time step , which in the Atari games benchmark corresponds to the next frame ’s pixel intensity values .;;However , directly predicting these pixel intensity values is unsatisfactory , since we do not expect pixel intensity to capture the salient features of the environment in a robust way .;Autoencoders;5
89542;39dba6f22d72853561a4ed684be265e179a39e4f;Material;1;;;WMT’14 English to French dataset;We used the WMT’14 English to French dataset .;;We trained our models on a subset of 12 M sentences consisting of 348 M French words and 304 M English words , which is a clean “ selected ” subset from .;Dataset details;4
102914;432d8cba544bf7b09b0455561fea098177a85db1;Material;1;section name;;OMNIGLOT data;We also randomly applied the dilation operator from computer vision as further data augmentation since we observed that the stroke widths are quite uniform in the OMNIGLOT data , whereas there is substantial variation in MNIST , this augmentation improved the visual quality of the few - shot MNIST samples considerably and increased the few - shot classification accuracy by about percent .;We sampled a binarization of each image for each epoch .;Finally we used ‘ sample dropout ’ whereby a random subset of each dataset was removed from the pooling in the statistic network , and then included the number of samples remaining as an extra feature .;Omniglot;18
39103;15e1af79939dbf90790b03d8aa02477783fb1d0f;Material;1;;;DukeMTMC - reID;We insert a dropout layer before the final convolutional layer and set the dropout rate to 0.5 for CUHK03 and 0.75 for Market - 1501 and DukeMTMC - reID , respectively .;All the images are resized to before being randomly cropped into with random horizontal flipping .;We use stochastic gradient descent with momentum 0.9 .;Implementation Details;14
80130;34273979fd2a62fd7b49ee6d14a925864ff94e74;Material;0;;;BaBi;BaBi and Sort - of - CLEVR require a few steps , Pretty - CLEVR requires up to eight steps and Sudoku requires more than ten steps .;We have proposed a general relational reasoning model for solving tasks requiring an order of magnitude more complex relational reasoning than the current state - of - the art .;Our relational reasoning module can be added to any deep learning model to add a powerful relational reasoning capacity .;Discussion;13
50090;1e5b9e512c01e244287fe7afb05e03c96d5c1cd0;Material;1;;;English PTB data;We also applied the approach to the benchmark English PTB data , where our model achieved 97.9 using the standard train / dev / test split , which constitutes a relative reduction in error of 12 % over the previous best system .;As we will see , a pattern emerged where gains were largest for morphologically rich languages , especially those in the Slavic family group .;;Introduction;2
37177;1518039b5001f1836565215eb047526b3ac7f462;Material;1;?;;EN→DE;"We first focus on unigram F 1 , where all systems improve over the baseline , especially for rare words ( 36.8%→41.8 % for EN→DE ; 26.5%→29.7 % for EN→RU ) .";All subword systems operate without a back - off dictionary .;For OOVs , the baseline strategy of copying unknown words works well for English→German .;Translation experiments;9
37178;1518039b5001f1836565215eb047526b3ac7f462;Material;1;?;;EN→RU;"We first focus on unigram F 1 , where all systems improve over the baseline , especially for rare words ( 36.8%→41.8 % for EN→DE ; 26.5%→29.7 % for EN→RU ) .";All subword systems operate without a back - off dictionary .;For OOVs , the baseline strategy of copying unknown words works well for English→German .;Translation experiments;9
57337;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;kein dataset;;Alexa;The demand for answering questions in natural language is increasing rapidly , and this has led to the development of smart devices such as Siri and Alexa .;Yu18 , DevlinCLT18 .;However , in comparison with answer span extraction , the natural language generation;Introduction;1
50181;1e5b9e512c01e244287fe7afb05e03c96d5c1cd0;Material;1;;;CoNLL Shared Task 2017 treebanks;In this section , we present the experimental setup and the selected hyperparameter for the main experiments where we use the CoNLL Shared Task 2017 treebanks and compare with the best systems of the shared task .;;;Experiments and Results;10
102181;42e80c73867bff9eaff6beceb8730fc1276283b9;Material;1;;;News Crawl monolingual corpora;More concretely , our training data consists of the concatenation of all News Crawl monolingual corpora from 2007 to 2013 , which make a total of 749 million tokens in French , 1 , 606 millions in German , and 2 , 109 millions in English , from which we take a random subset of 2 , 000 sentences for tuning ( Section [ reference ] ) .;In order to make our experiments comparable to previous work , we use the French - English and German - English datasets from the WMT 2014 shared task .;Preprocessing is done using standard Moses tools , and involves punctuation normalization , tokenization with aggressive hyphen splitting , and truecasing .;Experiments and results;9
58792;23dcfda130aada27c158c0b5f394cac489c9c795;Material;1;;;WIDER Face Dataset;For BIWI we run a Faster R - CNN face detector trained on the WIDER Face Dataset and deployed in a Docker container .;AFLW2000 images are small and are cropped around the face .;We loosely crop the faces around the bounding box in order to conserve the rest of the head .;Fine - Grained Pose Estimation on the AFLW2000 and BIWI Datasets;10
27862;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;0;kein dataset;;photos;This is to be expected since the original filters are highly - tuned for photos .;We also observe that the baseline performs slightly better than other architectures for ‘ Photo ’ style .;However , its performance for ‘ Art ’ is relatively lower compared to;Results;15
101426;424aef7340ee618132cc3314669400e23ad910ba;Material;0;;;PTB 3;Wikitext - 2 is a dataset released recently as an alternative to PTB 3 .;The version of this dataset which we use is the one processed in [ reference ] , with the most frequent 10k words selected to be in the vocabulary and rest replaced with a an < unk > token 2 .;"It contains 2 , 088k training , 217k validation , and 245k test tokens , and has a vocabulary of 33 , 278 words ; therefore , in comparison to PTB , it is roughly 2 times larger in dataset size , and 3 times larger in vocabulary .";EXPERIMENTS;6
81415;34f63959ea4a13a05948274a1558c6854a051150;Material;1;section name;;Recognizing Textual Entailment dataset;The Recognizing Textual Entailment dataset is collected from a series of annual challenges on textual entailment .;;The task is similar to MNLI , but uses only two labels : entailment and not_entailment .;RTE;24
44550;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;newstest2014;For English Czech , we trained our baseline model on the complete WMT16 parallel training set ( including CzEng 1.6pre ) , until we observed convergence on our heldout set ( newstest2014 ) .;;This took approximately 1 M minibatches , or 3 weeks .;English Czech;10
106477;45b559e6271570598602fcf9777ed6f2f2d133e6;Material;1;;;SWB - 1 training data;We then evaluate the very deep CNNs on the Hub5’00 benchmark ( using the 262 hours of SWB - 1 training data ) achieving a word error rate of 11.8 % after cross - entropy training , a 1.4 % WER improvement ( 10.6 % relative ) over the best published CNN result so far .;We evaluate the improvements first on a Babel task for low resource speech recognition , obtaining an absolute 5.77 % WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages .;TomSercu ChristianPuhrsch BrianKingsbury YannLeCun CenterforDataScience , CourantInstituteofMathematicalSciences , NewYorkUniversity IBMT.J.WatsonResearchCenter , YorktownHeights , NY , 10598 , U.S.A. { tsercu , bedk } @us.ibm.com , 1cpuhrsch@nyu.edu ,;Very Deep Multilingual Convolutional Neural Networks for LVCSR;0
43556;19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7;Material;1;;;Breakout;We selected hyperparameter values by performing an informal search on the games of Breakout , Pong and Seaquest which were then fixed for all the games .;The random agent selected actions uniformly at random at 10Hz and it was evaluated using the same starting states as the agents for both kinds of evaluations ( null op starts and human starts ) .;We have trained Gorila DQN 5 times on each game using the same fixed hyperparameter settings and random network initializations .;Evaluation;12
67076;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;kein dataset;;real noisy images;Though the real noisy images are generally 8 - bit quantized , we empirically found that the learned model still works effectively on real noisy images .;More specifically , FFDNet model is trained on the noisy images without quantization to 8 - bit integer values .;For FFDNet - Clip , as mentioned in Sec .;Dataset Generation and Network Training;14
99101;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;1;;;SUN - RGBD Training;paragraph : SUN - RGBD Training;We picked the early stopped ( 200 epochs ) snapshot models for evaluation .;The data set consists of 10 , 355 RGB - D images captured from various depth sensors for indoor scenes ( bedrooms , dining rooms etc . ) .;SUN - RGBD Training;33
28682;0f810eb4777fd05317951ebaa7a3f5835ee84cf4;Material;1;;;Arcade Learning Environment ( ALE );We evaluate our algorithm on five games from the Arcade Learning Environment ( ALE ) , which has recently become a standard high - dimensional benchmark for RL .;;The reward signal is computed from the game score .;Setup;25
67659;2b0d7e51efd004fe3847f54863540c79312f3546;Material;1;;;MultiMNIST;Our initial experiments are on MultiMNIST , an MTL version of the MNIST dataset;;[ reference ] .;MultiMNIST;9
67130;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;kein dataset;;center - cropped images;The Kodak24 dataset consists of 24 center - cropped images of size 500 500 from the original Kodak dataset .;BSD68 dataset .;The McMaster dataset is a widely - used dataset for color demosaicing , which contains 18 cropped images of size 500 500 .;Dataset Generation and Network Training;14
75215;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;0;;;IJB - A face recognition dataset;In contrast , the newly released IJB - A face recognition dataset unifies evaluation of one - to - many face identification with one - to - one face verification over templates , or sets of imagery and videos for a subject .;Face recognition performance evaluation has traditionally focused on one - to - one verification , popularized by the Labeled Faces in the Wild dataset for imagery and the YouTubeFaces dataset for videos .;In this paper , we study the problem of template adaptation , a form of transfer learning to the set of media in a template .;Template Adaptation for Face Verification and Identification;0
73307;2e57bccb74bcb46cbc5b4225b62679023ed1f9da;Material;0;;;CNN / Daily Mail datasets;Meanwhile , the analysis in [ reference ] also illustrates the huge variations in the di culty level with respect to questions in the CNN / Daily Mail datasets;This behavior generally varies from document to document or question to question because it is related to the sophistication of the document or the di culty of the question .;[ reference ] .;INTRODUCTION;2
91441;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;Knowledge Base;The key differentiator of our VQA model is that it is able to usefully combine image information with that extracted from a Knowledge Base , within the LSTM framework .;;The novelty lies in the fact that this is achieved by representing both of these disparate forms of information as text before combining them .;A VQA Model with External Knowledge;9
100758;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;COCO data;We also report results on the MS COCO dataset and investigate the improvements on PASCAL VOC using the COCO data .;Using the expensive very deep models of , our detection method still has a frame rate of 5fps ( including all steps ) on a GPU , and thus is a practical object detection system in terms of both speed and accuracy .;Code has been made publicly available at ( in MATLAB ) and ( in Python ) .;Introduction;1
62855;2788a2461ed0067e2f7aaa63c449a24a237ec341;Material;1;;;TriNet;We use the ResNet - 18 , ResNet - 34 , and ResNet - 50 architectures for IDE and TriNet , and ResNet - 50 for SVDNet .;Note that , we use 256 128 as the input size to train SVDNet which achieves higher performance than the original paper using size 224 224 .;We fine - tune them on the model pre - trained on ImageNet .;Experiment Settings;17
97105;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;SemCor dataset;In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM ’s context representation of “ play ” in the source sentence .;They are spread across several parts of speech ( e.g. , “ played ” , “ playing ” as verbs , and “ player ” , “ game ” as nouns ) but concentrated in the sports - related senses of “ play ” .;In these cases , the biLM is able to disambiguate both the part of speech and word sense in the source sentence .;What information is captured by the biLM ’s representations ?;12
13294;074b6fe0cc6848fb86a6703d1c52074494177c79;Material;1;;;SYNTHIA video sequences;We use the SYNTHIA video sequences , which are rendered across a variety of environments , weather conditions , and lighting conditions .;For our first evaluation , we consider the SYNTHIA dataset ros_cvpr16 , which contains synthetic renderings of urban scenes .;This provides a synthetic testbed for evaluating adaptation techniques .;Semantic Segmentation Adaptation;6
103395;434bf475addfb580707208618f99c8be0c55cf95;Material;1;;;MMI Facial Expression Databse;The standard datasets , i.e. Extended Cohn - Kanade ( CKP ) and MMI Facial Expression Databse are used for the quantitative evaluation .;We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding .;On the CKP set the current state of the art approach , using CNNs , achieves an accuracy of 99.2 % .;DeXpression : Deep Convolutional Neural Network for Expression Recognition;0
18080;0a34fe39e9938ae8c813a81ae6d2d3a325600e5c;Material;1;;;300W test set;These 1026 images , collectively , form the 300W test set .;As a test set , we used the standard union consisting of the LFPW test set ( 224 images ) , the HELEN test set ( 330 ) , AFW ( 337 ) , and IBUG ( 135 ) .;Note that unlike others , we did not use AFW to train our method , allowing us to use it for testing .;Landmark detection accuracy;8
61505;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;0;;;IMDB - WIKI;It is clear that except IMDB - WIKI , we have the comparatively largest scale in terms of the number of pictures and the number of individuals .;Table [ reference ] fairly compares our dataset with existing released cross - age datasets .;Furthermore , as IMDB - WIKI is collected by automatically online crawling , some of the downloaded data might be redundant and noise - severe .;Dataset Statistics;8
26706;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;1;;;Seaquest;Asterix , Beam Rider , Freeway , Seaquest and Space Invaders .;Our training set consisted of five games :;The parameter search involved finding suitable values for the parameters to the SARSA ( ) algorithm , i.e. the learning rate , exploration rate , discount factor , and the decay rate .;Evaluation Methodology;14
53843;2116b2eaaece4af9c28c32af2728f3d49b792cf9;Material;1;;;CIFAR - 10 with dropout;Our model for CIFAR - 10 with dropout is similar , but because dropout imposes a strong regularization on the network , we are able to use more parameters .;All convolutional layers have 64 filter banks and use a filter size of ( times the number of channels in the preceding layer ) .;Therefore we add a fourth weight layer , which takes its input from the third pooling layer .;Models for CIFAR - 10;20
28087;0f0cab9235bbf185acdd4f9713fd111ca50effca;Material;0;;;RAF dataset;The RAF dataset was prepared by collecting images from various search engines and the facial landmarks were annotated manually by 40 independent labelers .;The landmarks thus obtained were then used for alignment .;The dataset contains 15331 images labeled with seven basic emotion categories of which 3068 are to be used for validation and 12271 for training .;Image - based Facial Expression Recognition;16
89435;39dba6f22d72853561a4ed684be265e179a39e4f;Material;1;;;WMT’14 dataset;Our main result is that on an English to French translation task from the WMT’14 dataset , the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set , where the LSTM ’s BLEU score was penalized on out - of - vocabulary words .;Our method uses a multilayered Long Short - Term Memory ( LSTM ) to map the input sequence to a vector of a fixed dimensionality , and then another deep LSTM to decode the target sequence from the vector .;Additionally , the LSTM did not have difficulty on long sentences .;Sequence to Sequence Learning with Neural Networks;0
70081;2d294bde112b892068636f3a48300b3c033d98da;Material;0;;;AFLW;There are three challenging benchmarks AFLW , COFW , and IBUG , which are used for evaluating face alignment with severe occlusion and large variations of pose , expression , and illumination .;;The provided face bounding boxes are employed to crop face patches during testing .;Datasets;14
37342;1518039b5001f1836565215eb047526b3ac7f462;Material;1;?;;English→German;"English→German translation results are shown in Table 2 ; English→Russian results in Table 3 .";;Our baseline WDict is a word - level model with a back - off dictionary .;Translation experiments;9
91027;3a8d537bcec370d37990d39eab01c729496ad057;Material;1;;;ModelNet40;The architecture of the VSL experimented with in this paper consisted of 5 local latent codes , each made up of 10 variables for ModelNet40 and 5 for ModelNet10 .;Training was the same across all experiments , with only minor details that were task - dependent .;For ModelNet40 , the global latent code was set to a dimensionality of 20 variables , while for ModelNet10 , it was set to 10 variables .;Training Protocol;11
99381;41232a69c0f8d4b993e6c6e00b16c223442c962f;Material;1;;;Gigaword test set;We use the same Gigaword test set as .;It comes up with about 3.8 M sentence - headline pairs as the training set and 189 K pairs as the development set .;It contains 2000 sentence - headline pairs .;Datasets;9
74537;2f0c30d6970da9ee9cf957350d9fa1025a1becb4;Material;1;;;CityScapes;For CityScapes , following the protocols in , training and evaluation are performed on images in the train set and images in the validation set , respectively .;Evaluation is performed on images in the validation set .;There are semantic categories plus a background category .;Experiment Setup and Implementation;9
95187;3e7f54801c886ea2061650fd24fc481e39be152f;Material;0;;;front;The iterative error feedback mechanism proposed by demonstrates promising results on front and side view RGB images .;The goal of our model is to achieve viewpoint invariant pose estimation .;However , a fundamental challenge remains unsolved : how can a model learn to be viewpoint invariant ?;Model;3
57390;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;scale corpus;Next , it uses contextualized word representations , ELMo PetersNIGCLZ18 , which is a character - level two - layer bidirectional language model pre - trained on a large - scale corpus .;First , this layer projects each of the one - hot vectors ( of size ) into a - dimensional continuous vector space with a pre - trained weight matrix such as GloVe PenningtonSM14 .;ELMo representations allow our model to use morphological clues to form robust representations for out - of - vocabulary words unseen in training .;Word Embedding Layer;12
60886;2594a77a3f0dd5073f79ba620e2f287804cec630;Material;1;;;images of Shoulder - Pain dataset;To be consistent with face verification , we perform the same pre - processing on the images of Shoulder - Pain dataset .;The face verification network is trained on CASIA - WebFace dataset , which contains 494 , 414 training images from 10 , 575 identities .;To be specific , we leverage MTCNN model to detect faces and facial landmarks .;Dataset and Training Details;8
57582;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;DuoRC KhapraSSA18;"Moreover , DuoRC KhapraSSA18 and CoQA ReddyCM18 contain abstractive answers ; most of the answers are short phrases .";The documents are long , averaging 62 , 528 ( 659 ) words in stories ( summaries ) , while the answers are relatively short , averaging 4.73 words .;;RC with NLG .;45
13050;072fd0b8d471f183da0ca9880379b3bb29031b6a;Material;1;;;Cityscapes validation set;We used the Cityscapes validation set for testing .;Cityscapes labels→photo 2975 training images from the Cityscapes training set , trained for 200 epochs , with random jitter and mirroring .;To compare the U - net against an encoder - decoder , we used a batch size of 10 , whereas for the objective function experiments we used batch size 1 .;Training details;24
76553;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;;heldout - 20;For test , we use another three shards from the heldout set , namely [ heldout - 20 , heldout - 30 , heldout - 40 ] .;For validation , we use two shards from the heldout set , namely [ heldout - 00 , heldout - 10 ] .;The hyper - parameters are listed below .;1B Word Dataset;30
34836;139768cf7714beb9309efba734460f8562c60c78;Material;0;;;CoNLL 2014 training dataset;For example , the error types investigated by cover only 35.74 % of all errors present in the CoNLL 2014 training dataset , providing no additional information for the majority of errors .;There has been very limited research on generating artificial data for all types , which is important for general - purpose error detection systems .;In this paper , we investigate two supervised approaches for generating all types of artificial errors .;Introduction;2
67788;2b0d7e51efd004fe3847f54863540c79312f3546;Material;1;;;Cityscapes dataset;For scene understanding experiments , we use the Cityscapes dataset;;[ reference ] .;C.3 Scene understanding;17
91349;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;image captions;In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a multi - label classification problem and train a corresponding deep CNN by minimizing an element - wise logistic loss function .;The model includes an image analysis part and a caption generation part .;Secondly , a fixed length vector is created for each image , whose length is the size of the attribute set .;Image Captioning using Attributes;6
90880;3a8d537bcec370d37990d39eab01c729496ad057;Material;0;;;ImageNet contest;However , ever since the ImageNet contest of 2012 , deep convolutional networks ( ConvNets ) have swept the vision industry , becoming nearly ubiquitous in countless applications .;Early efforts often combined simple image classification methods with hand - crafted shape descriptors , requiring intensive effort on the side of the human data annotator .;Research in learning probabilistic generative models has also benefited from the advances made by artificial neural networks .;Related Work;2
91454;3b1b94441010615195a5c404409ce2416860508c;Material;1;;;DBpeida;The external data source that we use here is DBpeida as a source of general background information , although any such KB could equally be applied .;;DBpeida is a structured database of information extracted from Wikipedia .;Relating to the Knowledge Base;10
103529;434bf475addfb580707208618f99c8be0c55cf95;Material;0;;;emotion - labeled sequences;In total this set has 593 emotion - labeled sequences .;They are both grayscale and colored .;The emotions consist of Anger , Disgust , Fear , Happiness , Sadness , Surprise , and Contempt .;CKP Dataset;12
74121;2ebfc12285f5d426e0d0e8d2befa1af27f99a56e;Material;1;;;BSDS300;SYMMAX300 is built on the Berkeley Segmentation Dataset ( BSDS300 ) , which contains 200 train images and 100 test images .;It consists of 648 train images and 787 test images .;Both foreground and background symmetries are considered .;Dataset and evaluation protocol;9
47172;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;0;;;ImageNet ILSVRC;In particular , CNNs pre - trained on datasets such as ImageNet ILSVRC have been shown to obtain excellent results in recognition in other domains [ reference ] , in object detection [ reference ] , in semantic segmentation [ reference ] , in human pose estimation [ reference ] , and in many other tasks .;One of the most striking aspects of CNNs is their ability to learn generic visual features that generalise to many tasks .;In this paper we look at how the power of CNNs can be leveraged in weakly supervised detection ( WSD ) , which is the problem of learning object detectors using only imagelevel labels .;Introduction;2
79121;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;0;;;Health dataset;"As for the Health dataset ; this dataset is extremely imbalanced , with only 15 % of the patients being admitted to a hospital .";The MMD penalty in VFAE did seem improve the discrimination scores over the original VAE , while the accuracy on the labels remained similar .;Therefore , each of the classifiers seems to predict the majority class as the label for every point .;Fair classification;12
40464;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;Broadcast News;"In the area of speech recognition , much of the pioneering early work was driven by a series of carefully designed tasks with DARPA - funded datasets publicly released by the LDC and NIST : first simple ones like the “ resource management ” task with a small vocabulary and carefully controlled grammar ; then read speech recognition in the Wall Street Journal task ; then Broadcast News ; each progressively more difficult for automatic systems .";Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go to simple speech recognition tasks like carefully read newspaper speech and rigidly constrained small - vocabulary tasks in noise .;One of last big initiatives in this area was in conversational telephone speech ( CTS ) , which is especially difficult due to the spontaneous ( neither read nor planned ) nature of the speech , its informality , and the self - corrections , hesitations and other disfluencies that are pervasive .;Introduction;1
67312;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;wegen figure;;RNI6 images;[ reference ] compares the grayscale image denoising results of Noise Clinic , BM3D , DnCNN , DnCNN - B and FFDNet on RNI6 images .;Fig .;As one can see , Noise Clinic reduces much the noise , but it also generates many algorithm - induced artifacts .;Experiments on Real Noisy Images;18
59383;23f5854b38a15c2ae201e751311665f7995b5e10;Material;1;;;Pinterest dataset;On the larger Pinterest dataset , Mult - dae even improves over the pre - trained ncf model by a big margin .;Multdae significantly outperforms ncf without pre - training on both datasets .;How well does multinomial likelihood perform ?;Experimental results and analysis;18
18856;0a49b4de21363d86599d4a058aaf4f5aed019495;Material;1;;;WMT’15;We evaluate neural machine translation models with a character - level decoder on four language pairs from WMT’15 to make our evaluation as convincing as possible .;To answer this question , we focus on representing the target side as a character sequence .;We represent the source side as a sequence of subwords extracted using byte - pair encoding from sennrich2015neural , and vary the target side to be either a sequence of subwords or characters .;Introduction;1
11786;06b4d8409837dce9d6eb919efd1debdaecc40d01;Material;1;;;MINIST;Overall , we observe state - of - the - art classification error in all four datasets ( without data augmentation ) , for MINIST , for CIFAR - 10 , for CIFAR - 100 , and for SVHN ( for CIFAR - 10 with data augmentation ) .;"The performance gain is more evident in presence of small training data ( see Figure ( [ reference ] .b ) ) ; this might partially ease the burden of requiring large training data for DL .";All results are achieved without using averaging , which is not exclusive to our method .;Experiments;10
66486;2ad7cef781f98fd66101fa4a78e012369d064830;Material;0;kein dataset;;IJB - A.;IJB - A. This is because the face variations in this dataset are relatively small ( compare the examples in Fig . 6;The gaps between NAN and the best - performing baselines are smaller compared to the results on;and Fig .;High weight Low weight;18
89283;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;VOC 2012 training dataset;The VOC 2012 training dataset therefore helps our model learn this task effectively .;This can be understood because the primary advantage of our model comes from delineating the objects and improving fine segmentation boundaries .;The results of this experiment are shown in Table 2 , below the bar , and we see that our approach sets a new state - of - the - art on the VOC 2012 dataset .;Pascal VOC Datasets;18
87343;372bc106c61e7eb004835e85bbfee997409f176a;Material;1;;;Faces;Faces : We applied CoGAN to learn a joint distribution of face images with different .;We note that for the case that the supports of the two domains are different such as the color and depth image domains , the conditional model can not even be applied .;We trained several CoGANs , each for generating a face with an attribute and a corresponding face without the attribute .;Experiments;4
56891;231af7dc01a166cac3b5b01ca05778238f796e41;Material;0;;;bedrooms category;The Large - scale CelebFaces Attributes ( CelebA ) dataset , aligned and cropped , the training dataset of the bedrooms category of the large scale image database ( LSUN ) , the CIFAR - 10 training dataset , the Street View House Numbers training dataset ( SVHN ) , and the One Billion Word Benchmark .;We used the following datasets to evaluate GANs :;All experiments rely on the respective reference implementations for the corresponding GAN model .;Used Software , Datasets , Pretrained Models , and Implementations;67
26519;0e8753f550350e53824358ca3f0f8cfd2f2dc2f7;Material;1;;;MovieLens 10 M matrix;After a row and column permutation , the original MovieLens 10 M matrix is partitioned in blocks , where is the matrix that we use for our experiments ( Figure [ reference ] ) .;The resulting matrix has observed values that correspond to the ratings that a user has given to a movie .;We treat as the users feature matrix , as the movies feature matrix and discard the remaining matrix .;Movielens dataset;9
26901;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;;;Infinite Mario;Each year new domains are proposed , including standard RL benchmarks , Tetris , and Infinite Mario .;Starting in 2004 as a conference workshop , the Reinforcement Learning competition was held until 2009 ( a new iteration of the competition has been announced for 2013 ) .;In a typical competition domain , the agent ’s state information is summarized through a series of high - level state variables rather than direct sensory information .;Evaluation Frameworks for General Agents;34
57579;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;NarrativeQA KociskySBDHMG18;NarrativeQA KociskySBDHMG18 proposed a dataset about stories or summaries of books or movie scripts .;Many of the answers are long and relatively far from the source documents compared with those from MS MARCO .;The documents are long , averaging 62 , 528 ( 659 ) words in stories ( summaries ) , while the answers are relatively short , averaging 4.73 words .;RC with NLG .;45
107161;45fdc73a239e9c6ea65e98c96f6a2d6dc35d6f72;Material;1;;;TIMIT dataset;The conducted experiments on the TIMIT dataset yielded a phoneme error rate ( PER ) of % for QCNNs which is significantly lower than the PER obtained with real - valued CNNs ( % ) , with the same input features .;The hypothesis tested here is whether these advantages lead to better generalization .;Moreover , from a practical point of view , the resulting networks have a considerably smaller memory footprint due to a smaller set of parameters .;Introduction;1
40685;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;in - domain training set;The RNN - LM output vocabulary consists of all words occurring more than once in the in - domain training set .;Training used noise - contrastive estimation ( NCE ) .;While the RNN - LM estimates a probability for unknown words , we take a different approach in rescoring : The number of out - of - set words is recorded for each hypothesis and a penalty for them is estimated for them when optimizing the relative weights for all model scores ( acoustic , LM , pronunciation ) , using the SRILM nbest - optimize tool .;RNN - LM setup;10
75013;2f92b10acf7c405e55c74c1043dabd9ded1b1800;Material;1;;;MultiNLI;We test on both the SNLI dataset Bowman2015 , a collection of sentence pairs , and the more recent MultiNLI dataset ( sentence pairs ) williams2017broad .;;Given two sentences , a premise and a hypothesis , the task is to determine whether either entails , contradicts or is neutral to .;Recognizing Textual Entailment;9
1831;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;;Chinese Poems;For real data , we use the text in EMNLP2017 WMT News , COCO Image Caption and Chinese Poems as the long , mid - length and short text corpus , respectively .;For synthetic data , LeakGAN obtains much lower negative log - likelihood than previous models with sequence length set to 20 and 40 .;In all those cases , LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test .;Introduction;2
91237;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;image and sentences;Recently , Socher et al . used neural networks to co - embed image and sentences together and Karpathy et al . co - embedded image crops and sub - sentences .;Similarly , posed the task as a retrieval problem , but based on co - embedding of images and text in the same space .;Attributes have been used in many image captioning methods to fill the gaps in predetermined caption templates .;Image Captioning;4
90762;3a7895b17db0cda7bbf86bcda52c46a3e03b6ded;Material;1;;;AVEC dataset;As both IEMOCAP and AVEC dataset contain multimodal information , we have evaluated DialogueRNN on multimodal features as used and provided by hazarika - EtAl:2018:N18 - 1 hazarika - EtAl:2018:N18 - 1 .;;We use concatenation of the unimodal features as a fusion method by following hazarika - EtAl:2018:N18 - 1 hazarika - EtAl:2018:N18 - 1 , since fusion mechanism is not a focus of this paper .;Multimodal Setting;43
91196;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;external knowledge;The image description takes the form of a set of captions , and the external knowledge is text - based information mined from a Knowledge Base .;( See Figure [ reference ] ) .;Specifically , for each of the top - attributes detected in the image we generate a query which may be applied to a Resource Description Framework ( RDF ) KB , such as DBpedia .;Introduction;1
65667;2a69ddbafb23c63e5e22401664bea229daaeb7d6;Material;1;;;DUT - OMRON dataset;The Res2Net based approach achieves greatest performance gain on the DUT - OMRON dataset , since this dataset contains the most significant object size variation than the other three datasets .;On the DUT - OMRON dataset ( containing 5168 images ) , the Res2Net based model has a improvement on F - measure and a improvement on MAE , compared with ResNet based model .;Some visual comparisons of salient object detection results on challenging examples are illustrated in Fig .;Salient Object Detection;26
91198;3b1b94441010615195a5c404409ce2416860508c;Material;0;kein dataset;;RDF ) KB;Specifically , for each of the top - attributes detected in the image we generate a query which may be applied to a Resource Description Framework ( RDF ) KB , such as DBpedia .;The image description takes the form of a set of captions , and the external knowledge is text - based information mined from a Knowledge Base .;RDF is the standard format for large KBs , of which there are many .;Introduction;1
67338;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;AWGN - like noise;It can be seen that CDnCNN - B yields very pleasing results for noisy image with AWGN - like noise such as image;[ reference ] shows the denoising results of Noise Clinic , CBM3D , CDnCNN - B and FFDNet on five color noisy images from RNI15 .;“ Frog ” , and is still unable to handle non - AWGN noise .;Experiments on Real Noisy Images;18
62030;26fe009b958e8728382d9d764bd7153632f0b869;Material;1;;;Multi - NLI test sets;Finally , Table [ reference ] shows results for different encoders on SNLI and Multi - NLI test sets .;These ablation results are shown in Tables [ reference ] , [ reference ] , [ reference ] and [ reference ] , all based on the Multi - NLI development sets .;First , Table [ reference ] shows the performance changes for different number of biLSTM layers and their varying dimension size .;Ablation Analysis Results;9
103379;43428880d75b3a14257c3ee9bda054e61eb869c0;Material;1;;;WMT'16;The translations produced by our models often match the length of the references , particularly for the large WMT'14 English - French task , or are very close for small to medium data sets such as WMT'14 English - German or WMT'16 English - Romanian .;We did not use the exact same vocabulary size because word pieces and BPE estimate the vocabulary differently .;;5;17
93760;3d734edc41c13fb4da2c3709e8255b004d083962;Material;1;;;BSD200;Following FSRCNN , SRCNN and SCN , we use three dataset , i.e. Set5 ( 5 images ) , Set14 ( 14 images ) and BSD200 ( 200 images ) for testing , which are widely used for benchmark in other works .;We set the patch size as , and use data augmentation ( rotation or flip ) to prepare training data .;;Datasets for Training and Testing;6
59344;23f5854b38a15c2ae201e751311665f7995b5e10;Material;1;;;Pinterest;The authors kindly provided the two datasets ( ML - 1 M and Pinterest ) used in the original paper , as well as the training / test split , therefore we separately compare with ncf on these two relatively smaller datasets in the empirical study .;We use the publicly available source code provided by the authors , yet can not obtain competitive performance on the datasets used in this paper - the validation metrics drop within the first few epochs over a wide range of regularization parameters .;In particular , we compare with the hybrid NeuCF model which gives the best performance in He et al .;Baselines;17
98861;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;0;;;RGB images;Therefore , we leverage mature 2D object detector to propose 2D object regions in RGB images as well as to classify objects .;The resolution of data produced by most 3D sensors , especially real - time depth sensors , is still lower than RGB images from commodity cameras .;With a known camera projection matrix , a 2D bounding box can be lifted to a frustum ( with near and far planes specified by depth sensor range ) that defines a 3D search space for the object .;Frustum Proposal;7
101583;424aef7340ee618132cc3314669400e23ad910ba;Material;1;;;Wikitext - 2;We set γ to values between 0.5 and 0.8 for the PTB dataset , and between 1.0 and 1.5 for the Wikitext - 2 dataset .;We have empirically observed that setting α , the weight of the augmented loss , according to α = γτ for all the networks works satisfactorily .;We would like to note that we have not observed sudden deteriorations in the performance with respect to moderate variations in either τ or α .;APPENDIX A MODEL AND TRAINING DETAILS;11
67201;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;Clip300 dataset;To evaluate FFDNet - Clip , Table [ reference ] shows the PSNR comparison with DCGRF and RBDN on the Clip300 dataset .;Overall , FFDNet produces the best perceptual quality of denoised images .;It can be seen that FFDNet - Clip with matched noise level achieves better performance than DCGRF and RBDN , showing that FFDNet performs well under the clipping setting .;Experiments on AWGN Removal;15
86643;36c3972569a6949ecca90bfa6f8e99883e092845;Material;1;;;VQA 2.0;As can be seen in Table [ reference ] , with features from 100 bounding - boxes , we reach 70.01 % for test - dev and 70.24 % for test - std on VQA 2.0 .;Instead of using an adaptive protocol for choosing the number of object proposals ( between 10 and 100 ) per image as as done in , we also experimented with using a simpler ( but slower ) strategy of using 100 objects proposals for all images .;;Post - Challenge Improvements;7
69023;2c761495cf3dd320e229586f80f868be12360d4e;Material;1;;;FastEval14k benchmark;On FastEval14k benchmark , model trained from ImageNet initialization performs better at the first 15 M iterations , but then becomes on par with random initialization .;Figure [ reference ] shows the training progress for these two settings .;Please note that the full training schedule takes 90 M iterations or around 10 epochs .;Monitoring Training Progress;6
47652;1c7e078611c9df412e6eb3a356f31a0da0c1f99c;Material;0;;;Amazon Picking Challenge;Recently , the 6D object pose estimation problem has received more attention thanks to the competition in the Amazon Picking Challenge ( APC ) .;In this work , we combine the advantages of both template - based methods and feature - based methods in a deep learning framework , where the network combines bottom - up pixel - wise labeling with top - down object pose regression .;Several datasets and approaches have been introduced for the specific setting in the APC .;RELATED WORK;2
3664;020a9aba95bce75dca08e3c499efc9e100f1cbb6;Material;1;;;Atari games domain;In this paper , we consider the challenging Atari games domain , which requires processing raw pixel inputs and delayed rewards .;Hence , exploration in complex domains is often performed with simple epsilon - greedy methods .;We evaluate several more sophisticated exploration strategies , including Thompson sampling and Boltzman exploration , and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics .;Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models;0
67411;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;AWGN;The results on synthetic images with AWGN demonstrated that FFDNet can not only produce state - of - the - art results when input noise level matches ground - truth noise level , but also have the ability to robustly control the trade - off between noise reduction and detail preservation .;To achieve this goal , several techniques were utilized in network design and training , such as the use of noise level map as input and denoising in downsampled sub - images space .;The results on images with spatially variant AWGN validated the flexibility of FFDNet for handing inhomogeneous noise .;Conclusion;20
12970;072fd0b8d471f183da0ca9880379b3bb29031b6a;Material;1;;;aerial photos;The aerial photos generated by our method fooled participants on of trials , significantly above the L1 baseline , which produces blurry results and nearly never fooled participants .;Results of our AMT experiment for map photo are given in Table [ reference ] .;In contrast , in the photo map direction our method only fooled participants on % of trials , and this was not significantly different than the performance of the L1 baseline ( based on bootstrap test ) .;Perceptual validation;14
54222;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;kein dataset;;native data;Fluency boost learning fully exploits both error - corrected data and native data by generating diverse error - corrected sentence pairs during training , which benefits model learning and improves the performance over the base seq2seq model , while fluency boost inference utilizes the characteristic of GEC to progressively improve a sentence ’s fluency through round - way correction .;We present a state - of - the - art convolutional seq2seq model based GEC system that uses a novel fluency boost learning and inference mechanism .;The powerful learning and inference mechanism enables our system to achieve state - of - the - art results and reach human - level performance on both CoNLL - 2014 and JFLEG benchmark datasets .;Conclusion;16
12425;07045f87709d0b7b998794e9fa912c0aba912281;Material;1;wegen section;;PhC - U373 ”;PhC - U373 ” contains Glioblastoma - astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy ( see [ reference ] a , b and Supp .;The first data set “;Material ) .;Experiments;5
81762;3526555fa0178c101ee9896252c818f9e03532a5;Material;1;;;Tobacco - 3482 dataset;Secondly , we use the Tobacco - 3482 dataset to evaluate the performance of the deep s and to investigate to which extent transfer learning from the first dataset is applicable .;The dataset is already split into a training dataset which contains images and a validation and a test dataset which each contain images .;The Tobacco - 3482 dataset contains images from ten document classes .;Datasets;12
16121;09da677bdbba113374d8fe4bb15ecfbdb4c8fe40;Material;1;;;ImagNet - 1k;Extensive experiments on three benchmark datasets , ImagNet - 1k , Places365 and PASCAL VOC , clearly demonstrate superior performance of the proposed DPN over state - of - the - arts .;To enjoy the benefits from both path topologies , our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures .;In particular , on the ImagNet - 1k dataset , a shallow DPN surpasses the best ResNeXt - 101;Dual Path Networks;0
33114;1109b663453e78a59e4f66446d71720ac58cec25;Material;0;;;1000 - category ImageNet;However , the advent of larger datasets has enabled ConvNets to significantly advance the state of the art on datasets such as the 1000 - category ImageNet .;The accuracy of ConvNets on small datasets such as Caltech - 101 , while decent , has not been record - breaking .;The main advantage of ConvNets for many such tasks is that the entire system is trained end to end , from raw pixels to ultimate categories , thereby alleviating the requirement to manually design a suitable feature extractor .;Introduction;1
75302;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;0;;;YouTubeFaces;The top performing approaches for face verification on Labeled Faces in the Wild and YouTubeFaces are all based on convolutional networks .;;VGG - Face is the application of the VGG - 16 convolutional network architecture trained on a newly curated dataset of 2.6 M images of 2622 subjects .;Related Work;2
84302;360cfa09b2f7c8e10b1831d899c5a51aefa1883e;Material;0;;;TED - talk corpus;The optimization was done using the Adaptive Moment Estimation ( Adam ) algorithm running for 22 epochs ( 35 for the TED - talk corpus ) with , , .;Moreover , the sorting approach exploits a curriculum learning strategy that has been shown to slightly improve the performance and to ensure numerical stability of gradients .;The performance on the development set was monitored after each epoch , while the learning rate was halved when the performance improvement went below a certain threshold ( ) .;RNN setting;10
83389;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;1;;;IWSLT 2014 evaluation campaign;We use data from the German - English machine translation track of the IWSLT 2014 evaluation campaign cettolo2014 .;For the translation task , our generative model is an LSTM with hidden units and it uses the same attentive encoder architecture as the one used for summarization .;The corpus consists of sentence - aligned subtitles of TED and TEDx talks .;Machine Translation;13
76554;303fef411f235e6d1125a40af1e93224f498a4d5;Material;1;;;heldout - 30;For test , we use another three shards from the heldout set , namely [ heldout - 20 , heldout - 30 , heldout - 40 ] .;For validation , we use two shards from the heldout set , namely [ heldout - 00 , heldout - 10 ] .;The hyper - parameters are listed below .;1B Word Dataset;30
40668;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;Web ) data;In order to make use of LM training data that is not fully matched to the target conversational speech domain , we start RNN - LM training with the union of in - domain ( here , CTS ) and out - of - domain ( e.g. , Web ) data .;The two RNN - LMs and the N - gram LM for each direction are interpolated with weights of ( 0.375 , 0.375 , 0.25 ) .;Upon convergence , the network undergoes a second training phase using the in - domain data only .;RNN - LM setup;10
104011;435259c5f3cffd75ef837a8e638cc8f6244e25c4;Material;1;;;iSEG Grand Challenge;Comparing the proposed Early_Ensemble approach to the top - 5 ranked methods of the iSEG Grand Challenge , our approach achieved very competitive results , ranking first or second in most cases .;Via 3D convolutions , our approach captures spatial information in volumetric data , which is confirmed by a performance improvement over 2D CNN models .;A paired sample t - test showed that the difference between our approach and the other best ranked method in the challenge ( i.e. , MSL_SKKU ) is not statistically significant .;Discussion;19
41615;1751668492bac56f0ae2b6410417515ab3215945;Material;1;;;Penn Treebank WSJ corpus;In order to demystify the effects of adversarial training in the context of NLP , we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus ( Englsih ) and the Universal Dependencies dataset ( 27 languages ) , with thorough analyses of the following points :;With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .;Effects on different target languages Vocabulary statistics and tagging accuracy Influence on downstream tasks Representation learning of words;Introduction;1
33369;1109b663453e78a59e4f66446d71720ac58cec25;Material;1;;;ILSVRC 2013 datasets;We applied it to the ILSVRC 2013 datasets , and it currently ranks 4 th in classification , 1 st in localization and 1 st in detection .;We have presented a multi - scale , sliding window approach that can be used for classification , localization and detection .;A second important contribution of our paper is explaining how ConvNets can be effectively used for detection and localization tasks .;Discussion;15
82097;35502af359aa60ae8047df172e29503cfb29c3f9;Material;0;;;JFT - 300 M dataset;While there are more complex methods in literature such as PSPNet and which augment training with additional data ( e.g. , COCO or JFT - 300 M dataset ) and utilize ensembles and post - processing , we focus on a simple experiment training the base model with / without the proposed pixel pair embedding loss to demonstrate the effectiveness .;Similar to our proposal detection model , we use a 64 - dimension embedding space on top of DeepLab - v3 as our base model .;In addition to reporting mean intersection over union ( mIoU ) over all classes , we also computed mIoU restricted to a narrow band of pixels around the ground - truth boundaries .;Semantic Instance Detection;15
77046;3112d2d95d66b3d54a72c55072647aab937e410e;Material;0;;;WEATHERGOV;Several benchmark datasets have been used in recent years for the text generation task , the most popular of these being WEATHERGOV [ reference ][ reference ][ reference ] . Recently , neural generation systems have show strong results on these datasets , with the system of [ reference ] achieving BLEU scores in the 60s and 70s on WEATHERGOV , and BLEU scores of almost 30 even on the smaller ROBOCUP dataset .;A dataset for training data - to - document systems typically consists of ( s , y 1:T ) pairs , where y 1:T is a document consisting of a gold ( i.e. , human generated ) summary for database s.;These results are quite promising , and suggest that neural models are a good fit for text generation .;Data - to - Text Datasets;3
2896;01959ef569f74c286956024866c1d107099199f7;Material;1;;;open - ended - real;Screenshots of leaderboards for open - ended - real and multiple - choice - real are shown in Fig . 13 .;| mc - abstract ] . 2 ) Compare their test - standard accuracies with those on the corresponding test2015 leaderboards [ oe - real - leaderboard | oe - abstract - leaderboard | mc - real - leaderboard | mcabstract - leaderboard ] . For more details , please see the challenge page [ reference ] .;We also compare the test - standard accuracies of our best model ( deeper LSTM Q + norm I ) for both open - ended and multiple - choice tasks ( real images ) with other entries ( as of October 28 , 2016 ) on the corresponding leaderboards in Table 5 .;VQA CHALLENGE AND WORKSHOP;17
89735;3a28fe49e7a856ddd60d134696a891ed7bca5962;Material;1;;;KITTI dataset;Beyond these , the existence of annotation bias in KITTI dataset is disclosed and analyzed .;On CityPersons dataset , our proposed method obtains superior performance in occlusion cases without any bells and whistles .;;Introduction;1
3613;0209389b8369aaa2a08830ac3b2036d4901ba1f1;Material;0;;;DensePose - COCO;As mentioned above , this difference in accuracy indicates the merit of having at our disposal DensePose - COCO for discriminative training .;We observe that while being orders of magnitude faster ( 0.04 - 0.25 ” vs 60 - 200 ” ) our bottom - up , feedforward method largely outperforms the iterative , model fitting result .;;FCNN - vs Model - based pose estimation;16
54966;223319a93dcf3912bbc1e5f949e5ab4d53906e62;Material;0;kein dataset;;DSLR;We finally evaluate our method on Office dataset , which is a collection of three distinct domains : Amazon , DSLR , and Webcam .;Office dataset .;Unlike previously discussed datasets , Office is rather small - scale with only 2817 labeled images spread across 31 different categories in the largest domain .;Results;8
104659;44c5dec4d1295d34f052d3243d8e08f14a3c0990;Material;1;;;enwiki8;Our best model in this settings use attention lengths of 1 , 600 and 3 , 800 on WikiText - 103 and enwiki8 respectively .;Similar to , we gradually increase the attention length at test time until no further noticeable improvement ( 0.1 % relative gains ) can be observed .;In addition , we devise a metric called Relative Effective Context Length ( RECL ) that aims to perform a fair comparison of the gains brought by increasing the context lengths for different models .;Introduction;1
26921;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;;;Acrobot;Thus a typical competition domain reflects existing research directions : Mountain Car and Acrobot remain staples of the RL competition .;While competitions seek to spur new research and evaluate existing algorithms through a standardized set of benchmarks , they are not independently developed , in the sense that the vast majority of domains are provided by the research community .;These competitions also focus their research effort on domains that provide high - level state variables , for example the location of robots in the floor - painting domain described above .;Evaluation Frameworks for General Agents;34
54350;220a0b46840a2a1421c62d3d343397ab087a3f17;Material;1;;;Flying Chairs;We report similar performance as FlowNet on Flying Chairs and Sintel but are significantly more accurate than FlowNet on Middlebury and KITTI after fine tuning .;We call the method SPyNet , for Spatial Pyramid Network , and train it using the same Flying Chairs data as FlowNet .;The total size of SPyNet is 96 % smaller than FlowNet , meaning that it runs faster , and uses much less memory .;Introduction;1
34702;1329206dbdb0a2b9e23102e1340c17bd2b2adcf5;Material;1;;;CUB images;In order to make the deep CNN - derived features more discriminative for the target task of fine - grained bird classification , we also fine - tune the ImageNet pre - trained CNN for the 200 - way bird classification task from ground truth bounding box crops of the original CUB images .;In one set of experiments , we extract deep convolutional features from an ImageNet pre - trained CNN , similar to DeCAF .;In particular , we replace the original 1000 - way fc8 classification layer with a new 200 - way fc8 layer with randomly initialized weights drawn from a Gaussian with and .;Fine - grained categorization;10
45685;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;0;;;TrecQA;For this reason , recent initiatives are returning to the original setting of directly answering from text using datasets like TrecQA , which is based on classical Trec resources , and WikiQA , which is extracted from Wikipedia .;Due to the sparsity of KB data , however , the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs automatically —not an easy problem .;Both benchmarks are organized around the task of answer sentence selection , where a system must identify the sentence containing the correct answer in a collection of documents , but need not return the actual answer as a KB - based system would do .;Related Work;2
12080;06c06885fd53b2cbd407704cf14f658842ed48e5;Material;0;;;Dataset B100;Dataset B100 consists of natural images in the Berkeley Segmentation Dataset .;Datasets Set5 and Set14 are often used for benchmark .;Finally , dataset Urban100 , urban images recently provided by Huang et al . , is very interesting as it contains many challenging images failed by existing methods .;Datasets;10
44568;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;news - commentary v11;After training this to convergence , we restarted training from the baseline model using 5 M sentences of back - translated data , 5 copies of news - commentary v11 , and a matching quantity of data sampled from Czeng 1.6pre .;After training a baseline model on all the WMT16 parallel set , we continued training with a parallel corpus consisting of 2 copies of the 2.5 M sentences of back - translated data , 5 copies of news - commentary v11 , and a matching quantity of data sampled from Czeng 1.6pre .;We repeated this with 7.5 M sentences from news2015 monolingual , and then with 10 M sentences of news2015 .;English Czech;10
97025;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;Reuters RCV1 corpus;CoNLL2003NER consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .;The CoNLL 2003 NER task;Following recent state - of - the - art systems lample -;Evaluation;8
45107;1a6b67622d04df8e245575bf8fb2066fb6729720;Material;1;;;Hutter Prize Wikipedia Prize;For the latter , we use the Penn Treebank Character ( PTBC ) ( ) and the Hutter Prize Wikipedia Prize ( ) ( also known as Enwik8 ) datasets .;For the former , we evaluate our method on the Penn Treebank ( PTB ) ( ) and the WikiText - 2 ( WT2 ) ( ) datasets .;Key statistics for these datasets is presented in Table [ reference ] .;Experiments;3
52093;2019ede61cc0be14859908312e18458a7c79908f;Material;1;;;WIKIBIO;We introduce a new dataset for text generation , WIKIBIO , a corpus of 728 , 321 articles from English Wikipedia;;[ reference ] .;Biography dataset;15
102551;42f20d37f4eba56284a941d5f9f58609ee650de0;Material;1;;;WED dataset;Because the proposed method operates on RGB channels rather than Y channel in YCbCr color space , we collect a large - scale color images for training , including BSD images , training images from DIV2 K dataset and images from WED dataset .;For the noise level range , we set it as .;Then , given an HR image , we synthesize LR image by blurring it with a blur kernel k and bicubicly downsampling it with a scale factor , followed by an addition of AWGN with noise level .;Training Data Synthesis and Network Training;13
14472;0875fc92cce33df5cf7df169590dbf0ca00d2652;Material;1;;;mscoco;We further illustrate how our method , learned on Microsoft COCO mscoco , generalizes to captions describing novel scenes that are not seen in the dataset , such as “ A stop sign is flying in blue skies ” ( see Fig .;The images generated by our alignDRAW model are refined in a post - processing step by a deterministic Laplacian pyramid adversarial network denton_lapgan .;[ reference ] ) .;Introduction;1
79029;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;0;;;health dataset;The health dataset is derived from the Heritage Health Prize .;Both of these are obtained from the UCI machine learning repository UCI .;It is the largest of the three datasets with entries .;Datasets;9
3639;0209389b8369aaa2a08830ac3b2036d4901ba1f1;Material;1;;;COCO - DensePose;We have introduced COCO - DensePose , a large - scale dataset of ground - truth image - surface correspondences and developed novel architectures that allow us to recover highly - accurate dense correspondences between images and the body surface in multiple frames per second .;In this work we have tackled the task of dense human pose estimation using discriminative trained models .;We anticipate that this will pave the way both for downstream tasks in augmented reality or graphics , but also help us tackle the general problem of associating images with semantic 3D object representations .;Conclusion;19
2481;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;wegen figure;;ImageNet - 5K.;Error ( % ) on ImageNet - 5K.;Table 6 .;The models are trained on ImageNet - 5 K and tested on the ImageNet - 1 K val set , treated as a 5 K - way classification task or a 1 K - way classification task at test time .;224×224;12
47365;1c0e8c3fb143eb5eb5af3026eae7257255fcf814;Material;0;;;PASCAL test set;Differently from AP , which is measured on the PASCAL test set , CorLoc is evaluated on the union of the training and validation subset of PASCAL .;[ reference ] . CorLoc is the percentage of images that contain at least one instance of the target object class for which the most confident detected bounding box overlaps by at least 50 % with one of these instances .;For classification , we use the standard PASCAL VOC protocol and report AP .;Benchmark data .;10
97759;40193e7ba0fbd7153a1fe15e95563463b67c71f3;Material;0;;;"in - the - wild "" face images";"# ITW "" indicates the number of the "" in - the - wild "" face images used for training .";Num .;The numbers in bold are the best results of each stage .;Ablation study;17
93546;3d5d9d8e74b215609eabba80ef79a35ebf460e49;Material;1;;;Cropped LineMod;Following PixelDA , we conduct experiments on the classification and pose estimation tasks using MNIST to MNIST - M , and Synthetic Cropped LineMod to Cropped LineMod .;We demonstrate that the proposed image - to - image translation scheme can benefit unsupervised domain adaptation .;Several example images in these datasets are shown in Figure [ reference ];Domain Adaptation;10
91261;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;text;Similarly , Kiros et al . constructed a joint multimodal embedding space using a powerful deep CNN model and an LSTM that encodes text .;Mao et al . , for instance , proposed a multimodal RNN ( m - RNN ) to estimate the probability distribution of the next word given previous words and the deep CNN feature of an image at each time step .;Karpathy and Li also proposed a multimodal RNN generative model , but in contrast to , their RNN is conditioned on the image information only at the first time step .;Image Captioning;4
12234;06c5b86b638b2f3572b9cdd9ef0be4740b16781b;Material;0;;;Movie Review;Movie Review ( MR ) blue and Stanford Sentiment Treebank;;( SST ) blue are used to evaluate our model .;Datasets and Sentiment Resources;7
25542;0dab72129b4458d9e3dbf1f109848c2d6d7af8a8;Material;1;;;Stanford Natural Language Inference;To address this , this paper introduces the Stanford Natural Language Inference;They are generally too small for training modern data - intensive , wide - coverage models , many contain sentences that were algorithmically generated , and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality .;( SNLI ) corpus , a collection of sentence pairs labeled for entailment , contradiction , and semantic independence .;Introduction;1
103810;435259c5f3cffd75ef837a8e638cc8f6244e25c4;Material;0;;;ImageNet 2012 classification benchmark;For example , Krizhevsky et al . found that , on the ImageNet 2012 classification benchmark , an ensemble of 5 CNNs achieved a top - 5 validation error rate of 16.4 , compared to 18.2 for a single CNN model .;Recent studies have demonstrated that averaging the predictions of similar CNNs can lead to an increase in performance .;By adding another CNN to this ensemble , and making small changes to the network architecture , Zeiler and Fergus were able to decrease this error further to 14.7 .;Ensemble learning to suggest local corrections;7
98705;40b4596a0ae4f4ff065f3f13f36db39543e50068;Material;1;;;cityscapes classes;In our experiment , we use the SYNTHIA - RAND - CITYSCAPES subset , which contains 9 , 400 images compatible with the cityscapes classes .;The rendering are across a variety of environments and weather conditions .;All experiment settings remain unchanged with the previous experiments .;Additional Results on SYNTHIA;15
52457;207e0ac5301a3c79af862951b70632ed650f74f7;Material;0;;;CUHK01;The deep learning based method does not fare well on this small dataset despite the fact that the model has been pre - trained on the far - larger CUHK01 + CUHK03 datasets .;-5;This suggests that the model learned from other datasets are not transferable by the simple model fine - tuning strategy and small sample size remains a bottle - neck for applying deep learning to re - i d .;Fully Supervised Learning Results;12
97008;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;1;;;OntoNotes benchmark Pradhan2013TowardsRL;As shown in Table [ reference ] , when adding ELMo to a re - implementation of He2017DeepSR the single model test set F jumped 3.2 % from 81.4 % to 84.6 % – a new state - of - the - art on the OntoNotes benchmark Pradhan2013TowardsRL , even improving over the previous best ensemble result by 1.2 % .;He2017DeepSR modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following Zhou2015EndtoendLO .;Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .;Evaluation;8
67382;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;0;;;denoising color images;First , BM3D spends much more time on denoising color images than grayscale images .;From Table [ reference ] , we have the following observations .;The reason is that , compared to gray - BM3D , CBM3D needs extra time to denoise the chrominance components after luminance - chrominance color transformation .;Running Time;19
81397;34f63959ea4a13a05948274a1558c6854a051150;Material;1;section name;;Quora Question Pairs dataset;The Quora Question Pairs dataset is a collection of question pairs extracted from the community question - answering website Quora .;;The task is to predict whether two questions are semantically equivalent .;QQP;21
61510;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;MORPH Album 2;For a direct and fair comparison to the existing work in this field , we evaluate our approach on existing public - domain cross - age face benchmark datasets MORPH Album 2 , CACD - VS and FG - NET .;;We also evaluate our algorithm on LFW for verifying the generalization performance on GFR .;Experiments;9
89236;39978ba7c83333475d6825d0ff897692933895fc;Material;1;;;Pascal VOC challenge;The standard metric used in the Pascal VOC challenge is the average intersection over union ( IU ) , which we also use here to report the results .;, we used the standard softmax loss function , that is , the log - likelihood error function described in [ reference ] .;In our experiments we found that high values of IU on the validation set were associated to low values of the averaged softmax loss , to a large extent .;Implementation Details;16
28219;0f2f4edb7599de34c97f680cf356943e57088345;Material;0;;;MPII benchmarks;State - of - the - art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods .;We refer to the architecture as a “ stacked hourglass ” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions .;[ ] [ ] captabfigfigure [ ];Stacked Hourglass Networks for Human Pose Estimation;0
79949;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;REDDIT - MULTI dataset;For social network datasets , we have a significant gain of atleast accuracy ( highest being on REDDIT - MULTI dataset ) against graph kernels as observed in Table [ reference ] .;It is worth mentioning that the most deep learning models ( like ours ) are also scalable while graph kernels are more fine tuned towards handling small graphs .;But this is expected as deep learning methods tend to do better with the large amount of data available for training on social networks datasets .;Experiment and Results;12
40584;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;kein dataset;;2 - dimensional image;First , each vector of activations is re - interpreted as a 2 - dimensional image .;The smoothing is implemented as a regularization term on the activations between layers of the acoustic model .;For example , if there are 512 neurons , they are interpreted as the pixels of a 16 by 32 image .;Spatial Smoothing;6
62941;2788a2461ed0067e2f7aaa63c449a24a237ec341;Material;1;;;CIFAR100;Experiment conducted on CIFAR10 , CIFAR100 , and Fashion - MNIST with various architectures validate the effectiveness of our method .;It is easy to implemented : Random Erasing randomly occludes an arbitrary region of the input image during each training iteration .;Moreover , we obtain reasonable improvement on object detection and person re - identification , demonstrating that our method has good performance on various recognition tasks .;Conclusion;19
68465;2c03df8b48bf3fa39054345bafabfeff15bfd11d;Material;1;;;CIFAR - 10 dataset;We conducted more studies on the CIFAR - 10 dataset [ reference ] , which consists of 50k training images and 10k testing images in 10 classes .;;We present experiments trained on the training set and evaluated on the test set .;CIFAR - 10 and Analysis;12
43465;19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7;Material;0;;;distributed database;Second , a global replay memory aggregates the experience into a distributed database .;If a single machine has sufficient memory to store experience tuples , then the overall memory capacity becomes .;In this approach the overall memory capacity is independent of and may be scaled as desired , at the cost of additional communication overhead .;Distributed Architecture;7
40732;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;DARPA EARS program;The 4 - gram language model for decoding was trained on the available CTS transcripts from the DARPA EARS program : Switchboard ( 3 M words ) , BBN Switchboard - 2 transcripts ( 850k ) , Fisher ( 21 M ) , English CallHome ( 200k ) , and the University of Washington conversational Web corpus ( 191 M ) .;;A separate N - gram model was trained from each source and interpolated with weights optimized on RT - 03 transcripts .;Training data;12
86099;36a03f648b40d209ce361550dbe1c823ddb715b5;Material;1;;;NYU;We compared the performance of the V2V - PoseNet on the three 3D hand pose estimation datasets ( ICVL [ reference ] NYU [ reference ] , and MSRA [ reference ] ) with most of the stateof - the - art methods , which include latent random forest ( LRF );;[ reference ] , cascaded hand pose regression ( Cascade );Comparison with state - of - the - art methods;16
79058;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;1;;;small German dataset;On all of the datasets we used 50 latent dimensions for and , except for the small German dataset , where we used 30 latent dimensions for both variables .;Finally , for the Amazon reviews and Extended Yale B datasets we had one hidden layer with 500 , 400 units respectively for the encoder , decoder , and 300 , 100 units respectively for the encoder and decoder .;For the predictive posterior we used a simple Logistic regression classifier .;Experimental Setup;10
46948;1bea6bbdb4aed87fff5390d42934a1d9b0a7bec4;Material;0;;;CNN and Daily Mail news articles;hermann2015teaching seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points , and show that a neural network can then be trained to give good performance on this task .;A key factor impeding its solution by machine learned systems is the limited availability of human - annotated data .;In this paper , we conduct a thorough examination of this new reading comprehension task .;A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task;0
69600;2cb8497f9214735ffd1bd57db645794459b8ff41;Material;1;;;CNN validation dataset;We expand on the analysis of the attention mechanism presented in the paper by including visualisations for additional queries from the CNN validation dataset below .;;We consider examples from the Attentive Reader as well as the Impatient Reader in this appendix .;Additional Heatmap Analysis;20
62209;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;0;;;Spin Images;Many geometric descriptors have been proposed including Spin Images [ reference ] , Geometry Histograms [ reference ] , and Signatures of Histograms [ reference ] , Feature Histograms [ reference ] . Many of these descriptors are now available in the Point Cloud Library [ reference ] .;Hand - crafted 3D Local Descriptors .;While these methods have made significant progress , they still struggle to handle noisy , low - resolution , and incomplete real - world data from commodity range sensors .;Related Work;3
95074;3e79a574d776c46bbe6d34f41b1e83b5d0f698f2;Material;1;;;movie review;The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5 , respectively .;;In addition to training time per epoch , test times are additionally reported .;Final Results for Classification;11
91298;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;annotated images;Geman et al . proposed an automatic ‘ query generator ’ that is trained on annotated images and produces a sequence of binary questions from any given test image .;built a query answering system based on a joint parse graph from text and videos .;Each of these approaches places significant limitations on the form of question that can be answered .;Visual Question Answering;5
79208;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;0;;;Health;The health dataset is derived from the Heritage Health Prize .;Both of these are obtained from the UCI machine learning repository UCI .;It is the largest of the three datasets with entries .;Datasets;9
62195;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;RANSAC;Results show that 3DMatch is considerably better than state - ofthe - art methods at matching keypoints , and outperforms other algorithms for geometric registration when combined with standard RANSAC .;In this paper , we train 3DMatch over 8 million correspondences from a collection of 62 RGB - D scene reconstructions [ 37 , [ reference ][ reference ][ reference ][ reference ] and demonstrate its ability to match 3D data in several applications .;Furthermore , we demonstrate that 3DMatch can also generalize to different tasks and spatial resolutions .;Introduction;2
54560;220a0b46840a2a1421c62d3d343397ab087a3f17;Material;1;;;Driving;We then fine - tune the model on Driving and Monkaa scenes from and evaluate the fine - tuned model;We evaluate KITTI scenes using the base model SPyNet trained on Flying Chairs .;SPyNet + ft .;KITTI and Middlebury .;10
53293;20cc4bfdb648fd7947c71252589fc867d4d16933;Material;1;;;ImageNet - Random dataset;As expected , PC obtains a larger gain in classification accuracy ( 1.45 % ) on ImageNet - Dogs as compared to the ImageNet - Random dataset ( 0.54 % ± 0.28 ) .;In Table 3 , we compare the performance of training from scratch with - and without - PC across three models : GoogLeNet , ResNet - 50 , and DenseNet - 161 .;In the second experiment , we utilize the CIFAR - 10 and CIFAR - 100 datasets , which contain the same number of total images .;Additional Experiments;14
101258;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;ILSVRC & COCO 2015 competitions;Faster R - CNN in ILSVRC & COCO 2015 competitions;Figure [ reference ] shows some results on the MS COCO test - dev set .;We have demonstrated that Faster R - CNN benefits more from better features , thanks to the fact that the RPN completely learns to propose regions by neural networks .;Experiments on MS COCO;12
61414;269c7aeca29dae51dca8208815f1c4c81bd471c2;Material;1;;;LFW;We verify the effectiveness of OE - CNNs with extensive experiments on three face aging datasets ( MORPH Album2 , CACD - VS and FG - NET ) and one GFR dataset ( LFW ) , and achieve the state - of - the - art performances .;Nevertheless , this work merely studies the generalization of networks rather than specifically modeling the age into decomposed features in the AIFR application scenario .;The major contributions of this paper are summarized as follows : 1 .;Introduction;1
71220;2d83dbf4c8eabc6bdef3326c4a30d5f33ffc944e;Material;1;;;Visual QA Dataset;subsection : Visual QA Dataset;section : Experiments;We choose the Visual QA ( VQA ) dataset for the evaluation of our models .;Visual QA Dataset;9
26637;0ecd4fdce541317b38124967b5c2a259d8f43c91;Material;0;;;Atari 2600;Despite the number and variety of games developed for the Atari 2600 , the hardware is relatively simple .;Space Invaders are two well - known examples – were ported to the console .;It has a 1.19Mhz CPU and can be emulated much faster than real - time on modern hardware .;The Atari 2600;3
47834;1c7e078611c9df412e6eb3a356f31a0da0c1f99c;Material;1;;;OccludedLINEMOD Dataset;subsection : Results on the OccludedLINEMOD Dataset;With ICP refinement , the accuracy of the 6D pose is further improved .;The OccludedLINEMOD dataset is challenging due to significant occlusions between objects .;Results on the OccludedLINEMOD Dataset;18
33171;1109b663453e78a59e4f66446d71720ac58cec25;Material;0;;;ImageNet 2012 competition;The authors also entered the ImageNet 2012 competition , winning both the classification and localization challenges .;[ ] recently demonstrated impressive classification performance using a large ConvNet .;Although they demonstrated an impressive localization performance , there has been no published work describing how their approach .;Introduction;1
34904;139768cf7714beb9309efba734460f8562c60c78;Material;1;;;FCE dataset;For comparison , we also report the performance of the error detection system by [ reference ] , trained using the same FCE dataset .;[ reference ] . F 0.5 calculates a weighted harmonic mean of precision and recall , which assigns twice as much importance to precision - this is motivated by practical applications , where accurate predictions from an error detection system are more important compared to coverage .;The results show that error detection performance is substantially improved by making use of artificially generated data , created by any of the described methods .;Evaluation;11
43110;19839ffab4c30db1556d7fd9275d1344a6e3fa46;Material;1;;;CoNLL - 2005;For span SRL , we test model on the common span SRL datasets from CoNLL - 2005 and CoNLL - 2012 shared tasks .;Our models are evaluated on two PropBank - style SRL tasks : span and dependency .;For dependency SRL , we experiment on CoNLL 2008 and 2009 benchmarks .;Experiments;13
92096;3b9732bb07dc99bde5e1f9f75251c6ea5039373e;Material;0;;;deterministic Atari 2600 games;DQN combines Q - learning with a flexible deep neural network and was tested on a varied and large set of deterministic Atari 2600 games , reaching human - level performance on many games .;To test whether overestimations occur in practice and at scale , we investigate the performance of the recent DQN algorithm .;In some ways , this setting is a best - case scenario for Q - learning , because the deep neural network provides flexible function approximation with the potential for a low asymptotic approximation error , and the determinism of the environments prevents the harmful effects of noise .;Deep Reinforcement Learning with Double Q - learning;0
65112;29c19276b8fff231717c3e342cb24144d2b77726;Material;1;;;Slovene;This model ( ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .;The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre - trained embeddings .;Initializing the word embeddings ( + Polyglot ) with off - the - shelf language - specific embeddings further improves accuracy .;Results;7
52779;20926884a62778a2bf3f9f3c56f30976749ad763;Material;0;;;IHDP );compiled a dataset for causal effect estimation based on the Infant Health and Development Program ( IHDP ) , in which the covariates come from a randomized experiment studying the effects of specialist home visits on future cognitive test scores .;;The treatment groups have been made imbalanced by removing a biased subset of the treated population .;Simulated outcome : IHDP;18
80618;346578304ff943b97b3efb1171ecd902cb4f6081;Material;1;;;MNIST;Specifics for the MNIST architecture and training are :;Code to reproduce experiments and plots is at https: // github.com / iDurugkar / GMAN .;Generator latent variables;A.8 EXPERIMENTAL SETUP;22
83405;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;1;;;MSCOCO dataset mscoco;For the image captioning task , we use the MSCOCO dataset mscoco .;;We use the entire training set provided by the authors , which consists of around k images .;Image Captioning;14
67124;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;Kodak24;As for color image denoising , we employ four datasets , namely CBSD68 , Kodak24 , McMaster , and;Note that all the testing images are not included in the training dataset .;“ RNI15 ” .;Dataset Generation and Network Training;14
96874;3febb2bed8865945e7fddc99efd791887bb7e14f;Material;0;;;CoVe McCann2017LearnedIT;In the simplest case , ELMo just selects the top layer , , as in TagLM Peters2017SemisupervisedST and CoVe McCann2017LearnedIT .;For inclusion in a downstream model , ELMo collapses all layers in into a single vector , .;More generally , we compute a task specific weighting of all biLM layers : In ( [ reference ] ) , are;ELMo;5
67335;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;RNI15;[ reference ] shows the denoising results of Noise Clinic , CBM3D , CDnCNN - B and FFDNet on five color noisy images from RNI15 .;Fig .;It can be seen that CDnCNN - B yields very pleasing results for noisy image with AWGN - like noise such as image;Experiments on Real Noisy Images;18
29759;0fbd17a4f791e04bbf8f240f7c48c178900e30a6;Material;1;;;COCO val split;To obtain results in Table [ reference ] , we have used COCO val split , our person bounding box results and the keypoint results from the official source code of the papers .;Consequently , PRN performed better grouping ( see Table [ reference ] ) than their methods : Part Affinity Fields [ ] and Associative Embedding [ ] by improving both detection results by 1 mAP .;Note that running PRN on keypoints that were not generated by MultiPoseNet is unfair to PRN because it is trained with our detection architecture .;Pose Residual Network Design;23
12967;072fd0b8d471f183da0ca9880379b3bb29031b6a;Material;1;;;map aerial photograph;We validate the perceptual realism of our results on the tasks of map aerial photograph and grayscale color .;;Results of our AMT experiment for map photo are given in Table [ reference ] .;Perceptual validation;14
45775;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;1;;;MovieLens metadata;We built a KB using OMDb and MovieLens metadata with entries for each movie and nine different relation types : director , writer , actor , release year , language , genre , tags , IMDb rating and IMDb votes , with 10k related actors , 6k directors and 43k entities in total .;Our set of movies were also matched to the MovieLens dataset .;"The KB is stored as triples ; see Table [ reference ] for examples .";KB;14
6013;03184ac97ebf0724c45a29ab49f2a8ce59ac2de3;Material;0;;;CUB;( 4 ) There is still a large gap between the performance of unsupervised output embeddings and human - annotated attributes on AWA and CUB , suggesting that better methods are needed for learning discriminative output embeddings from text .;[ reference ] ) .;( 5 ) Finally , supporting , encoding continuous nature of attributes significantly improve upon binary attributes for zero - shot classification ( Tab .;Conclusion;14
2056;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;;4 - line 5 - character poems;The dataset consists of 4 - line 5 - character poems .;To evaluate the performance of LeakGAN in short text generation , we pick the dataset of Chinese poems which is proposed by [ reference ] and most related work such as [ reference ][ reference ][ reference ] ) .;Following the above work , we use the BLEU - 2 scores as the evaluating metrics .;Short Text Generation : Chinese Poems;14
75631;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;1;;;IJB - A;In this section , we describe the results for evaluation of the experimental system on the IJB - A verification and identification protocols .;;IJB - A contains 5712 images and 2085 videos of 500 subjects , for an average of 11.4 images and 4.2 videos per subject .;IJB - A Evaluation;6
83402;35c1668dc64d24a28c6041978e5fcca754eb2f4b;Material;0;;;English dictionary;The English dictionary has words while the German has words .;The test set is a concatenation of dev2010 , dev2012 , tst2010 , tst2011 and tst2012 which results in sentence pairs .;;Machine Translation;13
40932;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;out - of - domain data;On the language modeling side , we achieve a performance boost by combining multiple LSTM - LMs in both forward and backward directions , and by using a two - phase training regimen to get best results from out - of - domain data .;Our use of lattice - free MMI is distinctive , and extends previous work by proposing the use of a mixed triphone / phoneme history in the language model .;For our best CNN system , LSTM - LM rescoring yields a relative word error reduction of 23 % , and a 20 % relative gain for the combined recognition system , considerably larger than previously reported for conversational speech recognition .;Relation to Prior Work;24
48817;1d696a1beb42515ab16f3a9f6f72584a41492a03;Material;1;;;WDRef dataset;Second , our training data is enlarged by merging the CelebFaces + dataset , the WDRef dataset , and some newly collected identities exclusive from LFW .;The final feature representation is increased from to dimensions .;The larger DeepID2 + net is trained with around face images from identities compared to images from identities used to train the DeepID2 net .;DeepID2 + nets;3
62404;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;0;;;Shelf & Tote benchmark;In our first experiment , the task is to register pre - scanned object models to RGB - D scanning data for the Shelf & Tote benchmark in the Amazon Picking Challenge ( APC ) setting [ reference ] , as illustrated in Fig . 7 .;Object Pose Estimation by model alignment .;This scenario is different from scene level reconstruction in the following two aspects : ( 1 ) object sizes and their geometric features are much smaller in scale and ( 2 ) the alignment here is from full pre - scanned models to partial scan data , instead of partial scans to partial scans .;6D;18
99184;40eb1e54cb5382dfd3b7efd16dc7df826262ea52;Material;1;;;LiDAR image;[ reference ] : blue arrows in right LiDAR image ) .;[ reference ] : left RGB image ) , bird ’s eye view based RPN successfully detects them ( Fig .;;Results;38
66945;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;abundant data units;By training the model with abundant data units , where M is exactly the noise level map of y , the model is expected to perform well when the noise level matches the ground - truth one ( see Fig .;;[ reference ] ( a ) ) .;Examining the Role of Noise Level Map;9
20969;0b5aef2894d3248fb5ecc955d50501f0aa276036;Material;1;;;mosi;We evaluated this setup with CMU - MOSI dataset ( sec : mosi ) and two feature sets : the feature set used in and the set of unimodal features discussed in UFE .;The results of our experiments are presented in table : hfusion .;Our model outperformed , which employed MKL , for all bimodal and trimodal scenarios by a margin of 1–1.8 % .;Hierarchical Fusion ( HFusion );35
43343;19fd2c2c9d4eecb3cf1befa8ac845a860083e8e7;Material;0;kein dataset;;experience replay memory;A conceptually distinct set of distributed learners reads samples of stored experience from the experience replay memory , and updates the value function or policy according to a given RL algorithm .;In addition to generating more data , distributed actors can explore the state space more effectively , as each actor behaves according to a slightly different policy .;Specifically , we focus in this paper on a variant of the DQN algorithm , which applies ASGD updates to the parameters of the Q - network .;Introduction;1
66210;2a94c84383ee3de5e6211d43d16e7de387f68878;Material;1;;;COCO;Table 4 compares our method with the single - model results of the COCO competition winners , including the 2016 winner G - RMI and the 2015 winner Faster R - CNN +++ .;We have not evaluated its feature - sharing version due to limited time , which should be slightly better as implied by Table 5 .;Without adding bells and whistles , our single - model entry has surpassed these strong , heavily engineered competitors .;Comparing with COCO Competition Winners;14
40535;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;Fisher training data;We note that the bulk of the Fisher training data , and the bulk of the data overall , was transcribed with the “ quick transcription ” guidelines .;While this is a different test set , the numbers are in line with our findings .;Thus , the current state of the art is actually far exceeding the noise level in its own training data .;Human Performance;2
92907;3d18ce183b5a5b4dcaa1216e30b774ef49eaa46f;Material;1;;;AFLW database;Experiments on the challenging AFLW database show that our approach achieves significant improvements over state - of - the - art methods .;We also propose a method to synthesize large - scale training samples in profile views to solve the third problem of data labelling .;;Face Alignment Across Large Poses : A 3D Solution;0
67362;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;kein dataset;;denoising grayscale and color images;Table [ reference ] lists the running time results of BM3D , DnCNN and FFDNet for denoising grayscale and color images with size 256 256 , 512 512 and 1 , 024 1 , 024 .;;The evaluation was performed in Matlab ( R2015b ) environment on a computer with a six - core Intel ( R );Running Time;19
89921;3a28fe49e7a856ddd60d134696a891ed7bca5962;Material;0;;;semantic segmentation dataset CityScapes;CityPersons is a new pedestrian detection dataset on top of the semantic segmentation dataset CityScapes and consists more crowded scenes compared with Caltech , and over 20 % of pedestrian annotations overlap with another annotated pedestrian whose IoU is above 0.3 .;The standard test set of 4024 images is used for evaluation under different protocols .;As CityPersons provides image samples only , its validation set with 500 images is used for the evaluation of our single - shot network .;Experiment Settings;13
91517;3b1b94441010615195a5c404409ce2416860508c;Material;1;kein dataset;;human captions;Because most of previous works in image captioning are not evaluated on the official split for Flickr30k and MS COCO , for fair comparison , we report results with the widely used publicly available splits in the work of .We further tested on the actually MS COCO test set consisting of 40775 images ( human captions for this split are not available publicly ) , and evaluated them on the COCO evaluation server .;In our reported results , we use pre - defined splits for Flickr8k .;;Dataset;15
48143;1cf6bc0866226c1f8e282463adc8b75d92fba9bb;Material;1;;;MS COCO Detection dataset;We select 2639 training cat images and 1395 testing cat images from MS COCO Detection dataset .;yes ” answer .;Our SMem - VQA one - hop model achieves 96 % test accuracy on this synthetic task , while the baseline model ( iBOWIMG ) accuracy is around 75 % .;Relative Position Recognition;9
44581;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;Romanian English;We found that the use of diacritics was inconsistent in the Romanian training ( and development ) data , so for Romanian English we removed diacritics from the Romanian source side , obtaining improvements of 1.3–1.4 Bleu .;This language pair has the smallest amount of parallel training data , and we found dropout to be very effective , yielding improvements of 4–5 Bleu .;Synthetic training data gives improvements of 4.1–5.1 Bleu .;English Romanian;11
57622;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;;;DuoRC;NemaKLR17 proposed an attentional encoder - decoder model , and KhapraSSA18 reported that it performed worse than BiDAF on DuoRC .;Query - based abstractive summarization has been rarely studied .;"HasselqvistHK17 proposed a pointer - generator based model ; however , it does not consider copying words from the question and multiple passages .";Abstractive summarization .;49
33178;1109b663453e78a59e4f66446d71720ac58cec25;Material;1;;;ImageNet 2013 competition;In this paper we use the terms localization and detection in a way that is consistent with their use in the ImageNet 2013 competition , namely that the only difference is the evaluation criterion used and both involve predicting the bounding box for each object in the image .;Our paper is thus the first to provide a clear explanation how ConvNets can be used for localization and detection for ImageNet data .;;Introduction;1
13316;074b6fe0cc6848fb86a6703d1c52074494177c79;Material;1;kein dataset;;fall domain image;In Figure [ reference ] we show the result of pixel only adaptation as we generate a winter domain image ( b ) from a fall domain image ( a ) , and visa versa ( c - d ) .;Therefore , we use this setting as an example where we may directly visualize the shift from fall to winter and inspect the intermediate pixel level adaptation result from our algorithm .;We may clearly see the changes of adding or removing snow .;Cross - season adaptation;7
64067;28703eef8fe505e8bd592ced3ce52a597097b031;Material;1;kein dataset;;English sentences;For instance , we might view machine translation in this way , where in particular we attempt to generate English sentences from ( corresponding ) French sentences .;In the simplest seq2seq scenario , we are given a collection of source - target sequence pairs and tasked with learning to generate target sequences from source sequences .;Seq2seq models are part of the broader class of “ encoder - decoder ” models , which first use an encoding model to transform a source object into an encoded representation .;Background and Notation;3
88816;38d7920f0e8a3a672ea37c8612b2b2947b9ba9d1;Material;1;;;OpenImagesV4 dataset;This result improves to 46.8 % if we pre - train the detector on the OpenImagesV4 dataset .;To the best of our knowledge , for a Faster - RCNN architecture with a ResNet - 101 backbone ( with deformable convolutions ) , our reported result of 46.1 % is state - of - the - art .;Adding an instance segmentation head and training the detection network along with it improves the performance to 47.6 % .;Comparison with State - of - the - art;14
75562;30180f66d5b4b7c0367e4b43e2b55367b72d6d2a;Material;1;;;CASIA Web - Face dataset;Next , we experimented with the CASIA Web - Face dataset .;However , a larger negative set would dilute the effect of the discriminating between gallery subjects , which is the primary goal of the evaluation , so a focused negative set would be appropriate .;The best negative set for probe adaptation is a set drawn from the same distribution as the templates .;Negative Set Study;8
57072;2329a46590b2036d508097143e65c1b77e571e8c;Material;0;;;English text transcriptions;The core of our system is a recurrent neural network ( RNN ) trained to ingest speech spectrograms and generate English text transcriptions .;;Let a single utterance and label be sampled from a training set .;RNN Training Setup;2
62249;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;RGB - D scene reconstructions;We learn the function ψ by making use of data from existing high quality RGB - D scene reconstructions .;Given any two points , an ideal function ψ maps their local 3D patches to two descriptors , where a smaller 2 distance between the descriptors indicates a higher likelihood of correspondence .;The advantage of this approach is threefold : First , reconstruction datasets can provide large amounts of training correspondences since each reconstruction contains millions of points that are observed from multiple different scanning views .;Learning From Reconstructions;4
27739;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;0;kein dataset;;Art;( c ) ) , henceforth referred to as Switch , that determines the depiction style of the input image and passes the image to corresponding depiction sub - network ( Photo or Art ) .;To realize the switching mechanism mentioned in Section [ reference ] , we design and train a switch network ( see Figure [ reference ];The Switch has two convolution layers which capture depiction - discriminative features such as edges , textures , corners , colors and their conjunctions .;Switch;5
94260;3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a;Material;1;;;union of VOC07 trainval;We perform similar experiments for VOC10 and 2012 , for which we construct a dataset of 21.5k images from the union of VOC07 trainval , test , and VOC12 trainval .;When training on this dataset we use 60k mini - batch iterations instead of 40k .;When training on this dataset , we use 100k SGD iterations and lower the learning rate by 0.1× each 40k iterations ( instead of each 30k ) .;Do we need more training data ?;22
91724;3b1b94441010615195a5c404409ce2416860508c;Material;0;kein dataset;;China Jiliang University;He received a Bachelor in mathematical sciences from China Jiliang University , a Masters in Computer Science , and a PhD in computer vision from the University of Bath ( UK ) in 2012 and 2015 , respectively .;His research interests include cross - depiction object detection and classification , attributes learning , neural networks , and image captioning .;Chunhua Shen is a Professor of computer science at the University of Adelaide .;References;23
54221;218b80da3eb15ae35267d280dcc4a806d515334a;Material;0;kein dataset;;error - corrected data;Fluency boost learning fully exploits both error - corrected data and native data by generating diverse error - corrected sentence pairs during training , which benefits model learning and improves the performance over the base seq2seq model , while fluency boost inference utilizes the characteristic of GEC to progressively improve a sentence ’s fluency through round - way correction .;We present a state - of - the - art convolutional seq2seq model based GEC system that uses a novel fluency boost learning and inference mechanism .;The powerful learning and inference mechanism enables our system to achieve state - of - the - art results and reach human - level performance on both CoNLL - 2014 and JFLEG benchmark datasets .;Conclusion;16
18635;0a3a003457f5d7758a42a0e4b7278b39a86ed0bd;Material;1;;;miniImageNet 1 - shot;In Figure [ reference ] , we show the statistic histograms of learned SS parameters , taking miniImageNet 1 - shot as an example setting .;;Scaling parameters are initialized as 1 and shifting parameters as 0 .;Interpretation of meta - learned SS;21
1830;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;;COCO Image Caption;For real data , we use the text in EMNLP2017 WMT News , COCO Image Caption and Chinese Poems as the long , mid - length and short text corpus , respectively .;For synthetic data , LeakGAN obtains much lower negative log - likelihood than previous models with sequence length set to 20 and 40 .;In all those cases , LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test .;Introduction;2
55207;228db5326a10cd67605ce103a7948207a65feeb1;Material;1;;;COCO image caption data set;The data set uses images in the COCO image caption data set .;VQA is created through human labeling .;Unlike the other data sets , for each image , there are three questions and for each question , there are ten answers labeled by human annotators .;Data sets;10
51986;2019ede61cc0be14859908312e18458a7c79908f;Material;0;kein dataset;;Facebook;* Rémi performed this work while interning at Facebook .;Previous work experimented with datasets that contain only a few tens of thousands of records such as WEATHERGOV or the ROBOCUP dataset , while our dataset contains over 700k biographies from;Wikipedia .;Introduction;2
22385;0c36c988acc9ec239953ff1b3931799af388ef70;Material;1;;;WIDER FACE dataset;We perform evaluation on WIDER FACE dataset [ reference ] which contains more challenges , including small scale , illumination , occlusion , background clutter and extreme poses when compared with other benchmarks .;;A total of 393 , 703 labeled faces in 32 , 203 images from 61 different scenes are collected , of which 40 % are chosen as train set , 10 % as validation set and other 50 % as test set .;Experiments;7
40468;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;0;;;Fisher data collections;The Switchboard and later Fisher data collections of the 1990s and early 2000s provide what is to date the largest and best studied of the conversational corpora .;One of last big initiatives in this area was in conversational telephone speech ( CTS ) , which is especially difficult due to the spontaneous ( neither read nor planned ) nature of the speech , its informality , and the self - corrections , hesitations and other disfluencies that are pervasive .;The history of work in this area includes key contributions by institutions such as IBM , BBN , SRI , AT & T , LIMSI , Cambridge University , Microsoft and numerous others .;Introduction;1
2750;01959ef569f74c286956024866c1d107099199f7;Material;1;;;VQA train dataset;In this section , we provide an analysis of the questions and answers in the VQA train dataset .;;To gain an understanding of the types of questions asked and answers provided , we visualize the distribution of question types and answers .;VQA DATASET ANALYSIS;6
96088;3f3a483402a3a2b800cf2c86506a37f6ef1a5332;Material;1;;;LSP;Funetuning VGG on LSP achieves remarkable 82.8 % PCK and 57.0 % AUC .;Using the deeper VGG architecture improves over more shallow AlexNet ( 77.9 vs. 72.4 % PCK , 50.0 vs. 44.6 % AUC ) .;Strong increase in AUC ( 57.0 vs. 50 % ) characterizes the improvement for smaller PCK evaluation thresholds .;LSP Person - Centric ( PC );18
62448;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;SUN3D dataset;For selected scenes of the SUN3D dataset , we use the method from Halber et al . to estimate camera poses .;[ reference ] , with 54 scenes for training and 8 scenes for testing .;For the precise train / test scene splits , see our project webpage .;A.1 . RGB - D Reconstruction Datasets;22
27673;0f0a25d3be0d50a134f6f68e6a82bd8a2f668882;Material;1;;;Photo - Art dataset;We compare SwiDeN with alternative architectures and prior work on a - category Photo - Art dataset containing objects depicted in multiple styles .;‘ deep ’ depictive style - based switching mechanism which appropriately addresses the depiction - specific and depiction - invariant aspects of the problem .;Experimental results show that SwiDeN outperforms other approaches for the depiction - invariant object recognition problem .;SwiDeN : Convolutional Neural Networks For Depiction Invariant Object Recognition;0
91324;3b1b94441010615195a5c404409ce2416860508c;Material;0;;;KBs;Large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia , have been used successfully in several natural language Question Answering ( QA ) systems .;Our framework also exploits both CNN and RNNs , but in contrast to preceding approaches which use only image features extracted from a CNN in answering a question , we employ multiple sources , including image content , generated image captions and mined external knowledge , to feed to an RNN to answer questions .;However , VQA systems exploiting KBs are still relatively rare .;Visual Question Answering;5
28696;0f810eb4777fd05317951ebaa7a3f5835ee84cf4;Material;0;;;Venture;We conducted five independent learning trials for Montezuma and Venture , and two trials for the remaining three games .;We chose to focus more resources on Montezuma ’s Revenge and Venture , for two reasons : ( 1 ) we hypothesise that - EB will produce more improvement in sparse reward games , and ( 2 ) leading algorithms with which we seek to compare - EB have also focused on these games .;All agents were trained for 100 million frames on the no - op metric .;Setup;25
91696;3b1b94441010615195a5c404409ce2416860508c;Material;0;kein dataset;;captions;In each case they encode the image with the final hidden layer from VggNet , and questions and captions are encoded using a BOW representation .;Antol et al . provide several results for this dataset .;A softmax neural network classifier with 2 hidden layers and 1000 hidden units ( dropout 0.5 ) in each layer with tanh non - linearity is then trained , the output space of which is the 1000 most frequent answers in the training set .;Results on VQA;20
49738;1db9bd18681b96473f3c82b21edc9240b44dc329;Material;1;;;CelebA data set of celebrity faces;We trained both our 1D Local and 2D Local models on the standard CelebA data set of celebrity faces with cropped boundaries .;;With the 1D Local , we achieve a negative log likelihood ( NLL ) of bits / dim on the dev set , using , memory size of , self - attention and feed - forward layers , , attention heads , dimensions in the feed - forward layers , and a dropout of .;CelebA;15
100011;41d08fb733f3e50ac183490f84d6377dffccf350;Material;0;;;RGBD database;As commodity 3D sensors become popular , RGBD database has been built and used to train learningbased systems;[ reference ][ reference ] .;[ reference ][ reference ] .;Related Work;4
73207;2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79;Material;1;;;Arxiv - GRQC;The datasets are summarized in Table [ reference ] and include networks for Protein interactions , Metabolic pathways , military Conflict between countries , the U.S. PowerGrid , collaboration between users on the BlogCatalog social website , and publication citations from the Cora , Citeseer , Pubmed , Arxiv - GRQC databases .;We evaluate our proposed autoencoder models on nine graph - structured datasets , spanning multiple application domains , from which previous graph embedding methods have achieved strong results for LPNC .;{ Protein , Metabolic , Conflict , PowerGrid } are reported in .;Datasets and Baselines;7
74543;2f0c30d6970da9ee9cf957350d9fa1025a1becb4;Material;1;;;PASCAl VOC;We use mIoU@V and mIoU@C for PASCAl VOC and Cityscapes , respectively .;For evaluation , we use the mean intersection - over - union ( mIoU ) metric defined over image pixels , following the standard protocols .;In training and inference , the images are resized to have a shorter side of pixels for PASCAL VOC and pixels for Cityscapes .;Experiment Setup and Implementation;9
93361;3d5d9d8e74b215609eabba80ef79a35ebf460e49;Material;1;;;Cropped LineMod datasets;We achieve competitive results on both the MNIST - M and the Cropped LineMod datasets .;We demonstrate the application of our model on unsupervised domain adaptation .;Our code , data and more results are available at .;Introduction;1
2082;0116899fce00ffa4afee08b505300bb3968faf9f;Material;1;;;EMNLP2017;EMNLP2017;( 2 ) A couple of kids in front of a bathroom that is in a bathroom .;WMT ( 1 );Turing Test and Generated Samples;16
49767;1db9bd18681b96473f3c82b21edc9240b44dc329;Material;1;;;CIFAR - 10 data set;We also trained a super - resolution model on the CIFAR - 10 data set .;;Our model reached a negative log - likelihood of using 1D local attention and using 2D local attention on the test set .;CIFAR - 10;16
57573;2393447b8b0b79046afea1c88a8ed3949338949e;Material;0;kein dataset;;natural language;To the best of our knowledge , there are no datasets for providing answers in natural language with multiple styles except MS MARCO 2.1 , although there are some datasets that provide abstractive answers .;However , it does not handle the extended vocabulary in order to generate words appearing in the question and passages .;DuReader HeLLLZXLWWSLWW18 , a Chinese multi - document RC dataset , provides the top - 10 ranked entire documents from Baidu Search and Zhidao .;RC with NLG .;45
33176;1109b663453e78a59e4f66446d71720ac58cec25;Material;0;;;ImageNet data;Our paper is thus the first to provide a clear explanation how ConvNets can be used for localization and detection for ImageNet data .;Although they demonstrated an impressive localization performance , there has been no published work describing how their approach .;In this paper we use the terms localization and detection in a way that is consistent with their use in the ImageNet 2013 competition , namely that the only difference is the evaluation criterion used and both involve predicting the bounding box for each object in the image .;Introduction;1
69604;2cb8497f9214735ffd1bd57db645794459b8ff41;Material;1;;;CNN validation set;Figure [ reference ] shows two positive examples from the CNN validation set that require reasonable levels of lexical generalisation and co - reference in order to be answered .;;The first query in Figure [ reference ] contains strong lexical cues through the quote , but requires identifying the entity quoted , which is non - trivial in the context document .;Positive Instances;22
100759;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;0;kein dataset;;MATLAB;Code has been made publicly available at ( in MATLAB ) and ( in Python ) .;We also report results on the MS COCO dataset and investigate the improvements on PASCAL VOC using the COCO data .;A preliminary version of this manuscript was published previously .;Introduction;1
89719;3a28fe49e7a856ddd60d134696a891ed7bca5962;Material;1;;;CityPersons datasets;Empirical evaluation reveals the novel TLL networks with or without temporal feature aggregation both lead to state - of - the - art performance on Caltech and CityPersons datasets .;Moreover , we design a scheme to utilize temporal information by aggregating features of adjacent frames to further improve performance .;In summary our key contributions are as follows :;Introduction;1
98719;40b4596a0ae4f4ff065f3f13f36db39543e50068;Material;1;;;STYNHIA;We evaluate the proposed method on Cityscapes , using GTAV and STYNHIA as the source domain .;Those two modules can be integrated with different semantic segmentation networks to improve their generalizability when applying to a new domain .;The experiments on benchmark datasets have clearly demonstrated the effectiveness of our proposed ROAD - Net .;Conclusion;16
59359;23f5854b38a15c2ae201e751311665f7995b5e10;Material;1;;;Netflix data - sets;Multvae pr significantly outperforms Mult - dae on ML - 20 M and Netflix data - sets .;Both Mult - vae pr and Mult - dae significantly outperform the baselines across datasets and metrics .;In most of the cases , non - linear models ( Mult - vae pr , Multdae , and cdae ) prove to be more powerful collaborative filtering models than state - of - the - art linear models .;Experimental results and analysis;18
79930;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;D & D dataset;From Table [ reference ] , it is clear that our GCAPS - CNN model consistently outperforms most of the considered deep learning methods on bioinformatics datasets ( except on D & D dataset ) with a significant margin of classification accuracy gain ( highest being on NCI1 dataset ) .;Graph Classification Results :;Again , this trend is continued to be the same on social network datasets as shown in Table [ reference ] .;Experiment and Results;12
9814;05ee231749c9ce97f036c71c1d2d599d660a8c81;Material;1;;;IJB - B datasets;In the IJB - A and IJB - B datasets , there are images and videos available for each subject .;Network deployment .;Here we follow the established approach of [ reference ][ reference ] to balance the contributions of face examples from different sources , as otherwise a single very long video could completely dominate the representation .;Networks , deployment and baselines;11
65111;29c19276b8fff231717c3e342cb24144d2b77726;Material;1;;;Hebrew;This model ( ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .;The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre - trained embeddings .;Initializing the word embeddings ( + Polyglot ) with off - the - shelf language - specific embeddings further improves accuracy .;Results;7
44553;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;Czeng 1.6pre;Then we continued training the model on a new parallel corpus , comprising 8.2 M sentences back - translated from the Czech monolingual news2015 , 5 copies of news - commentary v11 , and 9 M sentences sampled from Czeng 1.6pre .;This took approximately 1 M minibatches , or 3 weeks .;The model used for back - translation was a neural MT model from earlier experiments , trained on WMT15 data .;English Czech;10
74686;2f0c30d6970da9ee9cf957350d9fa1025a1becb4;Material;1;;;VOC;For PASCAL VOC , following the protocol in , training is performed on the union of VOC 2007 trainval and VOC 2012 trainval .;We use PASCAL VOC and COCO datasets .;Evaluation is on VOC 2007 test .;Experiment Setup and Implementation;9
52462;207e0ac5301a3c79af862951b70632ed650f74f7;Material;1;;;PRID2011;Results on PRID2011;This suggests that the model learned from other datasets are not transferable by the simple model fine - tuning strategy and small sample size remains a bottle - neck for applying deep learning to re - i d .;We compare the state - of - the - art results reported on PRID2011 in Table [ reference ] .;Fully Supervised Learning Results;12
96596;3f45d73a7b8d10a59a68688c11950e003f4852fc;Material;0;;;CUHK03 dataset;The CUHK03 dataset was captured with six surveillance cameras over months , with each person observed by two disjoint camera views and having an average of 4.8 images in each view .;It is currently the largest publicly available person re - identification dataset .;In addition to manually cropped pedestrian images , samples detected with a state - of - the - art pedestrian detector is also provided .;Experiments on CUHK03;17
64640;289e91654f6da968d625481ef21f52892052d4fc;Material;0;kein dataset;;Autocar Conversation;The topics “ Car ” and “ Second - hand Car ” pay much attention to the words like “ Second - hand TIIDA ” and the other topic like “ Autocar Conversation ” focuses more on “ got some problems ” .;In Figure [ reference ] , we observed that when classifying different topics , the interaction features are different .;The results clearly signify that the interaction feature between the word and class is well - learned and highly meaningful .;Interaction Visualization;25
67123;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;CBSD68;As for color image denoising , we employ four datasets , namely CBSD68 , Kodak24 , McMaster , and;Note that all the testing images are not included in the training dataset .;“ RNI15 ” .;Dataset Generation and Network Training;14
99808;41951953579a0e3620f0235e5fcb80b930e6eee3;Material;1;;;CelebFaces +;In particular , we use the CelebFaces + dataset for training , which contains face images of identities ( celebrities ) collected from the Internet .;Therefore , we rely on a larger outside dataset for training , as did by all recent high - performance face verification algorithms .;People in CelebFaces +;Experiments;4
52096;2019ede61cc0be14859908312e18458a7c79908f;Material;1;kein dataset;;Stanford CoreNLP;We extract and tokenize the first sentence of each article with Stanford CoreNLP;It comprises all biography articles listed by WikiProject Biography 1 which also have a table ( infobox ) .;[ reference ] .;Biography dataset;15
86813;3729a9a140aa13b3b26210d333fd19659fc21471;Material;1;;;CoNLL 2000 shared task;For chunking , we also used the WSJ corpus , and followed the standard split for the training ( Section 15 - 18 ) and test ( Section 20 ) sets as in the CoNLL 2000 shared task .;Chunking :;We used Section 19 as the development set and employed the IOBES tagging scheme .;Datasets;18
44742;1a67622ca58aa851afe36ad6c6e78f9fb9d691d2;Material;0;kein dataset;;Low dimensional - When labeled data;Low dimensional - When labeled data is scarce , lowdimensional models generalize better , and speed up convergence and inference .;•;Continuous - We require latent representations to model partial community membership in continuous space .;LEARNING SOCIAL REPRESENTATIONS;5
76215;303065c44cf847849d04da16b8b1d9a120cef73a;Material;1;;;Photoface database;For experimental evaluation we employ images of 100 subjects from the Photoface database .;As a second evaluation , we use our technique to find per - pixel normals and compare against two well established Shape - from - Shading ( SfS ) techniques : PS - NL and IMM .;As a set of four illumination conditions are provided for each subject then we can generate ground - truth facial surface normals using calibrated 4 - source Photometric Stereo .;Quantitative Normal Recovery;14
70086;2d294bde112b892068636f3a48300b3c033d98da;Material;0;;;MUCT;Compared with other datasets like MUCT and LFPW , AFLW exhibits larger pose variations and extreme partial occlusions .;[ ] contains faces under real - world conditions gathered from Flickr .;Following the settings of , images are used for testing , and images annotated with landmarks are used for training , which includes LFW images and web images .;Datasets;14
79589;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;0;;;2D / 3D images;With remarkable successes of deep learning approaches in image classification and object recognition that attain “ superhuman ” performance , there has been a surge of research interests in generalizing convolutional neural networks ( CNNs ) to structures beyond regular grids , i.e. , from 2D / 3D images to arbitrary structures such as graphs .;Learning on graphs such as graph semi - supervised learning , graph classification or graph evolution have found wide applications in domains such as bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .;These convolutional networks on graphs are now commonly known as Graph Convolutional Neural Networks ( GCNNs ) .;Introduction;1
56532;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;LSUN Bedrooms datasets;We used the WGAN - GP image model to test TTUR with the CIFAR - 10 and LSUN Bedrooms datasets .;;In contrast to the original code where the discriminator is trained five times for each generator update , TTUR updates the discriminator only once ,;WGAN - GP on Image Data .;12
71031;2d5dba33c706d907733f15e7b57fde9909894e29;Material;1;;;TD500;The training set of TD500 only has 300 images , which are not enough for finetuning our model .;TD500 is taken as the dataset for this experiment , as it consists of oriented and multi - lingual text .;We mix the training set of TD500 with the training set of IC15 , in the way that every batch has half of its images coming from each dataset .;Detecting Multi - Lingual Text in Long Lines;26
23426;0cb8f50580cc69191144bd503e268451ce966fa6;Material;0;;;QM9 dataset;Here we focus on the QM9 dataset as such a benchmark .;In general , the search for practically effective machine learning ( ML ) models in a given domain proceeds through a sequence of increasingly realistic and interesting benchmarks .;QM9 consists of 130k molecules with 13 properties for each molecule which are approximated by an expensive quantum mechanical simulation method ( DFT ) , to yield 13 corresponding regression tasks .;Introduction;1
40177;1672ffebacadf849188668f24bcd377a19ae4051;Material;1;;;MovieLens data;Datasethttps: // grouplens.org / datasets / movielens / 20m / . MovieLens data contains 138 , 493 users , 27 , 278 movies , 21 categories and 20 , 000 , 263 samples .;MovieLens;To make it suitable for CTR prediction task , we transform it into a binary classification data .;Datasets and Experimental Setup;16
4524;027f9695189355d18ec6be8e48f3d23ea25db35d;Material;1;;;SNLI dataset;We encode sentences in the test split of SNLI dataset using the trained 300D model and find nearest neighbors given a query sentence .;Nearest neighbors;Table 3 presents five nearest neighbors for each selected query sentence .;Qualitative Analysis;13
79865;33a8d0a35390fde736744d4a0dd20dff7961c777;Material;1;;;benchmark datasets;To evaluate our GCAPS - CNN model , we perform graph classification tasks on variety of benchmark datasets .;Datasets :;In first round , we used bioinformatics datasets namely : PTC , PROTEINS , NCI1 , NCI109 , D & D , and ENZYMES .;Experiment and Results;12
63746;27e4b65121d3c88643d86dc91a9bdafdf223b988;Material;0;;;Chinese dataset;They show promising results on their Chinese dataset using an encoder - decoder RNN , but do not report experiments on English corpora .;In another paper that is closely related to our work , hu:2015:EMNLP introduce a large dataset for Chinese short text summarization .;In another very recent work , jianpeng used RNN based encoder - decoder for extractive summarization of documents .;Related Work;7
46989;1bea6bbdb4aed87fff5390d42934a1d9b0a7bec4;Material;0;kein dataset;;news article;A news article is usually associated with a few ( e.g. , 3–5 ) bullet points and each of them highlights one aspect of its content .;The goal is to infer the missing entity ( answer ) from all the possible entities which appear in the passage .;The text has been run through a Google NLP pipeline .;The Reading Comprehension Task;2
55834;22aab110058ebbd198edb1f1e7b4f69fb13c0613;Material;1;;;ImageNet ILSVRC 2012;We evaluate our models on ImageNet ILSVRC 2012 ILSVRC2015 at 128 128 , 256 256 , and 512 512 resolutions , employing the settings from Table [ reference ] , row 8 .;;The samples generated by our models are presented in Figure [ reference ] , with additional samples in Appendix [ reference ] , and online .;Evaluation on ImageNet;11
23638;0cb8f50580cc69191144bd503e268451ce966fa6;Material;1;;;QM - 9 dataset;The QM - 9 dataset has 130462 molecules in it .;We used a linear learning rate decay that began between 10 % and 90 % of the way through training and the initial learning rate decayed to a final learning rate , using a decay factor in the range .;We randomly chose 10000 samples for validation , 10000 samples for testing , and used the rest for training .;Training;12
95001;3e79a574d776c46bbe6d34f41b1e83b5d0f698f2;Material;1;;;CoNLL test set;As can be seen in ( Table 7 ) , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .;For NER , the step number is set to 9 , with a development F1 - score of 94.98 % .;Stacking more layers of BiLSTMs leads to slightly better F1 - scores compared with a single - layer BiL - STM .;Final Results for Sequence Labelling;12
100420;420c46d7cafcb841309f02ad04cf51cb1f190a48;Material;0;;;Pascal VOC 2012 training set;This simplified prediction module was trained on the Pascal VOC 2012 training set , augmented by the annotations created by .;Intermediate padding was used in the original classification network , but is neither necessary nor justified in dense prediction .;We did not use images from the VOC - 2012 validation set for training and therefore only used a subset of the annotations of .;Front End;4
69011;2c761495cf3dd320e229586f80f868be12360d4e;Material;1;;;FastEval14k;Unlike labels in JFT - 300 M , the images in FastEval14k are densely annotated and there are around 37 labels per image on average .;FastEval14k consists of 14000 images with labels from 6000 classes ( subset of 18291 classes from JFT - 300 M ) .;We use the same mAP@100 metric as in , which is computed as the mean average precision ( mAP ) for top - 100 predictions .;Monitoring Training Progress;6
79028;32b36a9837da7eb19fb9e6bd85d48d38a81ff75f;Material;0;;;UCI machine learning repository UCI;Both of these are obtained from the UCI machine learning repository UCI .;The sensitive variable is age .;The health dataset is derived from the Heritage Health Prize .;Datasets;9
2258;0171bdeb1c6e333287be655c667cfba5edb89b76;Material;1;;;ImageNet - 1 K dataset;On the ImageNet - 1 K dataset , we empirically show that even under the restricted condition of maintaining complexity , increasing cardinality is able to improve classification accuracy .;"This strategy exposes a new dimension , which we call "" cardinality "" ( the size of the set of transformations ) , as an essential factor in addition to the dimensions of depth and width .";Moreover , increasing cardinality is more effective than going deeper or wider when we increase the capacity .;Abstract;1
44575;1a5ea605111eb3403868d4b679315e944beee8c6;Material;1;;;Czech English;Our final Czech English was an ensemble of 8 systems – the last 4 save - points of the 10 M synthetic data run , and the last 4 save - points of the 7.5 M run .;The back - translations were , as for English Czech , created with an earlier NMT model trained on WMT15 data .;We show this as ensemble8 in Table [ reference ] , and the + synthetic results are on the last ( i.e. 10 M ) synthetic data run .;English Czech;10
96591;3f45d73a7b8d10a59a68688c11950e003f4852fc;Material;1;;;CUHK Campus dataset;The CUHK Campus dataset was captured with two camera views in a campus environment .;;Different from the above datasets , images in this dataset are of higher resolution .;Experiments on CUHK Campus;16
54078;218b80da3eb15ae35267d280dcc4a806d515334a;Material;1;;;Lang - 8.com;In addition , we also collect 2 , 865 , 639 non - public error - corrected sentence pairs from Lang - 8.com .;Table [ reference ] shows the stats of the datasets .;The native data we use for fluency boost learning is English Wikipedia that contains 61 , 677 , 453 sentences .;Dataset and evaluation;12
2702;01959ef569f74c286956024866c1d107099199f7;Material;1;;;Microsoft Common Objects in Context;We use the 123 , 287 training and validation images and 81 , 434 test images from the newly - released Microsoft Common Objects in Context ( MS COCO );Real Images .;[ reference ] dataset .;VQA DATASET COLLECTION;4
40663;16cd50316e41cbb1d9dfeafeb524b31654cef37a;Material;1;;;LM training data;In order to make use of LM training data that is not fully matched to the target conversational speech domain , we start RNN - LM training with the union of in - domain ( here , CTS ) and out - of - domain ( e.g. , Web ) data .;The two RNN - LMs and the N - gram LM for each direction are interpolated with weights of ( 0.375 , 0.375 , 0.25 ) .;Upon convergence , the network undergoes a second training phase using the in - domain data only .;RNN - LM setup;10
101141;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;VOC 2012 trainval;and VOC 2012 trainval .;trainval + test;Table [ reference ] and Table [ reference ] show the detailed numbers .;Experiments on PASCAL VOC;11
34430;12f008bea798a05ebfa2864ec026999cb375bcd9;Material;1;;;CNN , for DailyMail;The p - values are for CNN , for DailyMail , for CBT - NE , and for WDW .;For each of the 4 datasets on which GA achieves the top performance , we conducted one - sample proportion tests to test whether GA is significantly better than the second - best baseline .;In other words , GA statistically significantly outperforms all other baselines on 3 out of those 4 datasets at the significance level .;Performance Comparison;11
77775;31ae4873da19b1e28eca8787a17f49bba08627e5;Material;1;;;VOC2012;Our experimental results indicate a 2.3 % mAP boost on VOC07 and a 2.6 % mAP boost on VOC2012 object detection challenge compared to the Fast - RCNN pipeline .;In our framework both the original detector and adversary are learned in a joint manner .;We also release the code for this paper .;A - Fast - RCNN : Hard Positive Generation via Adversary for Object Detection;0
101290;424561d8585ff8ebce7d5d07de8dbf7aae5e7270;Material;1;;;PASCAL VOC;This evaluation is possible because the categories on COCO are a superset of those on PASCAL VOC .;As a simple baseline , we directly evaluate the COCO detection model on the PASCAL VOC dataset , without fine - tuning on any PASCAL VOC data .;The categories that are exclusive on COCO are ignored in this experiment , and the softmax layer is performed only on the 20 categories plus background .;From MS COCO to PASCAL VOC;13
91455;3b1b94441010615195a5c404409ce2416860508c;Material;1;kein dataset;;general background information;The external data source that we use here is DBpeida as a source of general background information , although any such KB could equally be applied .;;DBpeida is a structured database of information extracted from Wikipedia .;Relating to the Knowledge Base;10
62226;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;0;;;3D ShapeNets;3D ShapeNets [ reference ] introduced 3D deep learning for modeling 3D shapes , and several recent works [ reference ][ reference ][ reference ] also compute deep features from 3D data for the task of object retrieval and classification .;There has also been rapid progress in learning geometric representations on 3D data .;While these works are inspiring , their focus is centered on extracting features from complete 3D object models at a global level .;Related Work;3
28081;0f0cab9235bbf185acdd4f9713fd111ca50effca;Material;1;;;Real - world Affective Faces;For comparing our deep learning architectures for image - based facial expression recognition against standard results , we use Static Facial Expressions in the Wild ( SFEW ) 2.0 dataset and Real - world Affective Faces ( RAF ) dataset .;;SFEW 2.0 contains 1394 images , of which 958 are to be used for training and 436 for validation .;Image - based Facial Expression Recognition;16
3674;020a9aba95bce75dca08e3c499efc9e100f1cbb6;Material;1;kein dataset;;Atari Learning domain;In addition to raw game - scores , we also develop an AUC - 100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark .;In the Atari domain , our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods .;;Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models;0
91709;3b1b94441010615195a5c404409ce2416860508c;Material;1;;;captioning datasets;Indeed , at the time of submitting this paper , our image captioning model outperforms the state - of - the - art on several captioning datasets .;We have shown that an explicit representation of image content improves V2L performance , in all cases .;Secondly , in this paper we have shown that it is possible to extend the state - of - the - art RNN - based VQA approach so as to incorporate the large volumes of information required to answer general , open - ended , questions about images .;Conclusions;21
65654;2a69ddbafb23c63e5e22401664bea229daaeb7d6;Material;1;;;ECSSD , PASCAL - S;Following , we train those two models using the MSRA - B dataset , and evaluate results on ECSSD , PASCAL - S , HKU - IS , and DUT - OMRON datasets .;For fair comparison , we only replace the backbone with ResNet - 50 and our proposed Res2Net - 50 , while keeping other configurations unchanged .;The F - measure and Mean Absolute Error ( MAE ) are used for evaluation .;Salient Object Detection;26
47060;1bea6bbdb4aed87fff5390d42934a1d9b0a7bec4;Material;0;;;Kaggle competitions;We use this ranking algorithm since our problem is naturally a ranking problem and forests of boosted decision trees have been very successful lately ( as seen , e.g. , in many recent Kaggle competitions ) .;For training our conventional classifier , we use the implementation of LambdaMART in the RankLib package .;We do not use all the features of LambdaMART since we are only scoring 1 / 0 loss on the first ranked proposal , rather than using an IR - style metric to score ranked results .;Training Details;9
56895;231af7dc01a166cac3b5b01ca05778238f796e41;Material;1;;;Street View House Numbers training dataset;The Large - scale CelebFaces Attributes ( CelebA ) dataset , aligned and cropped , the training dataset of the bedrooms category of the large scale image database ( LSUN ) , the CIFAR - 10 training dataset , the Street View House Numbers training dataset ( SVHN ) , and the One Billion Word Benchmark .;We used the following datasets to evaluate GANs :;All experiments rely on the respective reference implementations for the corresponding GAN model .;Used Software , Datasets , Pretrained Models , and Implementations;67
101509;424aef7340ee618132cc3314669400e23ad910ba;Material;1;;;Penn Treebank corpus;In our experiments on the Penn Treebank corpus and Wikitext - 2 , we showed that our framework outperforms the conventional one , and that even the simple modification of reusing the word embedding in the output projection layer is sufficient for large networks .;We empirically validated the theoretical link , and verified that both proposed changes do in fact belong to the same framework .;The improvements achieved by our framework are not unique to vanilla language modeling , and are readily applicable to other tasks which utilize language models such as neural machine translation , speech recognition , and text summarization .;QUALITATIVE RESULTS;10
66864;2aec8d465e9a74c27f956ed1136f3e8a3ba0a833;Material;1;;;grayscale image;By considering the balance of complexity and performance , we empirically set the number of convolution layers as 15 for grayscale image and 12 for color image .;Since FFDNet operates on downsampled sub - images , it is not necessary to employ the dilated convolution to further increase the receptive field .;As to the channels of feature maps , we set 64 for grayscale image and 96 for color image .;Network Architecture;6
78422;325093f2c5b33d7507c10aa422e96aa5b10a33f1;Material;0;;;COCO images;COCO images ( with 40k for training , 5k for validation , 5k for test - dev and 15k as challenge test set ) with annotations for 91 stuff classes and 1 void class .;This dataset comprises 65k;Images are smaller than in Cityscapes and with varying sizes , and the provided semantic annotations are based on superpixel segmentations , consequently suffering from considerable mislabelings .;Semantic Segmentation;10
92863;3c78c6df5eb1695b6a399e346dde880af27d1016;Material;0;;;TriviaQA;We note that for TriviaQA web we do not subsample as was done by triviaqa , instead training on the full 530k question - document training pairs .;;We also observed that the metrics for TriviaQA are computed after applying a small amount of text normalization ( stripping punctuation , removing articles , ect . );Preprocessing;13
36411;14ad9d060c1e8f0449e697ee189ac346353fbfbc;Material;1;;;BC5CDR - disease;Since Wang et al . used BC5CDR - both for their experiments , we reran their models on BC5CDR - chem and BC5CDR - disease for a fair comparison with other models .;Table [ reference ] shows the results of the single - task models ( STMs ) where Table [ reference ] shows the comparison between the existing state - of - the - art multi - task learning model ( MTM ) and our CollaboNet .;The rerun scores are denoted with asterisks .;Results;14
5436;02e85d62fbd8249a046d00ac10e39546511b2a51;Material;1;;;BRATS 2015;Additionally , our pipeline achieved state - of - the - art performance on both public benchmarks of brain tumors ( BRATS 2015 ) and stroke lesions ( SISS ISLES 2015 ) .;Important to note is that the latter work focused only on segmentation of contusions , while our system has been shown capable of segmenting even small and diffused pathologies .;We believe performance can be further improved with task - and data - specific adjustments , for instance in the pre - processing , but our results show the potential of this generically designed segmentation system .;Discussion and Conclusion;30
62261;2742a33946e20dd33140b8d6e80d5fd04fced1b2;Material;1;;;RGB - D Scenes;Specifically , we use a total of over 200 K RGB - D images of 62 different scenes collected from Analysis - by - Synthesis [ 37 ] , 7 - Scenes [ reference ] , SUN3D [ reference ] , RGB - D Scenes v.2;Third , by learning from multiple reconstruction datasets , we can optimize 3DMatch to generalize and robustly match local geometry in real - world partial 3D data under a variety of conditions .;[ reference ] , and Halber et al .;Learning From Reconstructions;4
95969;3f3a483402a3a2b800cf2c86506a37f6ef1a5332;Material;0;;;We Are Family;"We Are Family ” ( WAF ) with training and testing group shots of people ;";“;“ MPII Human Pose ”;Multi Person Pose Estimation;15
44900;1a67622ca58aa851afe36ad6c6e78f9fb9d691d2;Material;1;;;YouTube network;The YouTube network is considerably larger than the previous ones we have experimented on , and its size prevents two of our baseline methods ( SpectralClustering and Modularity ) from running on it .;;It is much closer to a real world graph than those we have previously considered .;YouTube;27
57850;23ae5fa0e8d581b184a8749d764d2ded128fd87e;Material;1;;;unaugmented CIFAR10;, we performed another set of experiments on unaugmented CIFAR10 .;All - CNN ) , or a simple fixed 50 / 50 proportion in max - avg pooling;From the baseline error rate of 9.10 % , replacing each of the 2 max pooling layers with stacked stride 2 conv :;Quick comparison : tree pooling;9
41834;1751668492bac56f0ae2b6410417515ab3215945;Material;1;;;English ( PTB - WSJ;Next , we perform an analysis on representation learning of words ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .;cluster tightness;We hypothesize that adversarial training ( AT ) helps to learn better word embeddings so that the POS tag prediction of a word can not be influenced by a small perturbation in the input embedding .;Effects on Representation Learning;27
26343;0e37c8f19eefeb0c20d92f5cb4df4153077c116b;Material;0;kein dataset;;100;C 10 / 100;C96 + 32M1 ⇥ 1 !;1 ⇥ 1 !;Title;0
45659;1b29786b7e43dda1a4d6ee93f520a2960b1e3126;Material;1;;;WikiMovies;WikiMovies contains 100k questions in the movie domain , and was designed to be answerable by using either a perfect KB ( based on OMDb ) , Wikipedia pages or an imperfect KB obtained through running an engineered IE pipeline on those pages .;To this end , this paper introduces WikiMovies , a new analysis tool that allows for measuring the performance of QA systems when the knowledge source is switched from a KB to unstructured documents .;To bridge the gap between using a KB and reading documents directly , we still lack appropriate machine learning algorithms .;Introduction;1
43118;19839ffab4c30db1556d7fd9275d1344a6e3fa46;Material;0;;;OntoNotes;The larger CoNLL - 2012 dataset is extracted from OntoNotes v5.0 corpus , which contains both verbal and nominal predicates .;The test set consists of section 23 of WSJ for in - domain evaluation together with 3 sections from Brown corpus for out - of - domain evaluation .;CoNLL 2008 and 2009 CoNLL - 2008 and the English part of CoNLL - 2009 shared tasks use the same English corpus , which merges two treebanks , PropBank and NomBank .;Datasets;14
78410;325093f2c5b33d7507c10aa422e96aa5b10a33f1;Material;0;;;Mapillary Vistas;State - of - the - art segmentations are typically obtained by combining classification models pretrained on ImageNet ( typically referred to as body ) with segmentation - specific head architectures and jointly fine - tuning them on suitable , ( densely ) annotated training data like Cityscapes , COCO - Stuff , ADE20 K or Mapillary Vistas .;The goal of semantic segmentation is to assign categorical labels to each pixel in an image .;Datasets used for Evaluation .;Semantic Segmentation;10
71606;2d876ed1dd2c58058d7197b734a8e4d349b8f231;Material;1;;;TED.tst2013;Our best performance on a development set ( TED.tst2013 ) was achieved using a four - layer encoder – decoder QRNN with 320 units per layer , no dropout or regularization , and gradient rescaling to a maximum magnitude of 5 .;We remove training sentences with more than 300 characters in English or German , and use a unified vocabulary of 187 Unicode code points .;Inputs were supplied to the encoder reversed , while the encoder convolutions were not masked .;Character - level Neural Machine Translation;7
;;nicht used;311;246;65;Nicht used ohne Anmerkung;246;;;;
;;;;;;;;;;;
;;used;622;595;27;Used ohne Anmerkung;595;;;;
;;;933;841;92;;841;;;;

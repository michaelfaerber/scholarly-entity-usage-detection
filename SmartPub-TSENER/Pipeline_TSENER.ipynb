{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term and Sentence Expansion for the training of a NER Tagger\n",
    "\n",
    "In the first pipeline we go over the preparation required for the TSE-NER, from data collection, extraction (to MongoDB), indexing (to Elasticsearch) and other preliminary steps (word2vec and doc2vec models). \n",
    "\n",
    "In this second pipeline we lay out the steps of Term Expansion, Sentence Expansion, NER Training, and NER Tagging. In short, the steps are:\n",
    "\n",
    "1. Initial Data Generation\n",
    "2. Term Expansion\n",
    "3. Sentence Expansion\n",
    "4. Training Data Generation\n",
    "5. Train NER Tagger\n",
    "6. Extract new entities\n",
    "7. Filtering\n",
    "\n",
    "The papers collected are expendable at this point, but the Elasticsearch indexes and embedding models are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in case you update modules while working\n",
    "#%load_ext autoreload \n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Configuration\n",
    "\n",
    "Before we start, this process requires certain high-level configuration parameters that can be introduced below (you can also dive into the code, we are trying to make it as clear as possible for further customization).\n",
    "\n",
    "* model_name: a one-word name for the model, it should be representative of the facet that the model focuses on\n",
    "* seeds: list (in format [‘item’, ‘item’ ... ]) of representative entities of the type\n",
    "* context_words: list (in format [‘item’, ‘item’ ... ]) of words that are usually surrounding the entities, that often appear in sentences together (only required when PMI filtering is applied)\n",
    "* sentence_expansion: True or False if the sentence expansion step should be performed (term expansion is always done) \n",
    "* training_cycles: number of the training cycles to perform\n",
    "* filtering_pmi: True or False if Pointwise Mutual Information filtering should be used at the end of each cycle\n",
    "* filtering_st: True or False if Similarity\tTerms filtering should be used at the end of each cycle\n",
    "* filtering_ws: True or False if Stopword + WordNet filtering should be used at the end of each cycle\n",
    "* filtering_kbl: True or False if Knowledge Base Lookup filtering should be used at the end of each cycle\n",
    "\n",
    "For example, we can provide with a number of entities of the **dataset** type, which is also the name of our model. In this context, datasets are collections of information that were constructed with a specific structure and a purpose, such as comparing performance of different technologies in the same task.\n",
    "\n",
    "For example, 50 entities of the dataset facet with the rest of initial configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dataset_50'\n",
    "\n",
    "seeds = ['buzzfeed', 'pslnl', 'dailymed', 'robust04', 'scovo', 'ask.com', 'cacm', 'stanford large network dataset', \n",
    "         'mediaeval', 'lexvo', 'spambase', 'shop.com', 'orkut', 'jnlpba', 'cyworld', 'citebase', 'blog06', 'worldcat', \n",
    "         'booking.com', 'semeval', 'imagenet', 'nasdaq', 'brightkite', 'movierating', 'webkb', 'ionosphere', 'moviepilot', \n",
    "         'duc2001', 'datahub', 'cifar', 'tdt', 'refseq', 'stack overflow', 'wikiwars', 'blogpulse', 'ws-353', 'gerbil', \n",
    "         'wikia', 'reddit', 'ldoce', 'kitti dataset', 'specweb', 'fedweb', 'wt2g', 'as3ap', 'friendfeed', 'new york times', \n",
    "         'chemid', 'imageclef', 'newegg', 'mnist']\n",
    "\n",
    "context_words = ['dataset', 'corpus', 'collection', 'repository', 'benchmark']\n",
    "sentence_expansion = True\n",
    "training_cycles = 5\n",
    "filtering_pmi = True\n",
    "filtering_st = True\n",
    "filtering_ws = True\n",
    "filtering_kbl = True\n",
    "filtering_majority = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** In addition to this configuration, we need to find the `config.py` file and edit the ROOTPATH and STANFORD_NER_PATH to the respective locations! In that file we can also edit the ports used for Elasticsearch.\n",
    "\n",
    "We also import all the scripts required for the process, as mentioned before, you can check he code for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd-sem-ie/SmartPub-TSENER/m1_postprocessing/extract_new_entities.py:127: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if counter % 20 is 0: print(f'Tagged {counter}/' + str(len(res['hits']['hits'])), 'full texts for ' + conference)\n",
      "/home/kd-sem-ie/SmartPub-TSENER/m1_postprocessing/extract_new_entities.py:194: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if counter % 20 is 0: print(f'Tagged {counter}/' + str(len(res['hits']['hits'])), 'full texts')\n"
     ]
    }
   ],
   "source": [
    "from m1_preprocessing import seed_data_extraction, term_sentence_expansion, training_data_generation, ner_training\n",
    "from m1_postprocessing import extract_new_entities, filtering\n",
    "import config as cfg\n",
    "import gensim\n",
    "import elasticsearch\n",
    "import time\n",
    "import re \n",
    "import string\n",
    "\n",
    "doc2vec_model = gensim.models.Doc2Vec.load('embedding_models/doc2vec.model') #this is the path of the model created in the previous pipeline\n",
    "es = elasticsearch.Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its creation, TSE-NER was imagined as a cyclic process, where we generate training data, train the NER model, extract the entities from our full-corpus and then use those as the new seeds (filtering out as much noise as possible, of course). However, in this demo we will go step by step for a single cycle, and at the end show how it would look like for a cyclic process.\n",
    "\n",
    "Therefore, we create a new variable that should iterate but will be fixed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSE-NER Process\n",
    "### 1 - Initial Data Generation\n",
    "\n",
    "Once we have the seeds and basic configuration, the first step consists of searching our entire corpus for sentences that contain the seed terms. This will create a txt file with the seeds, and one with the sentences in the `processing_files` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started initial training data extraction\n",
      "Extracting sentences for 51 seed terms\n",
      "...................................................Process finished with 51 seeds and 37 sentences added for training in cycle number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd-sem-ie/SmartPub-TSENER/m1_preprocessing/seed_data_extraction.py:70: ElasticsearchDeprecationWarning: [types removal] Specifying types in search requests is deprecated.\n",
      "  res = es.search(index=\"twosent\", doc_type=\"twosentnorules\",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Methods/Statistical Analysis: To study the mentioned objective the websites referred at the end like tech-recipes  , stack overflow and msdn were referred for gaining an idea on how to implement the objective. Background/Objectives: The objective of this paper is to develop a testing tool that which tests the Functionality trait of an application that gets installed on the Phone with an Operating System called Windows.',\n",
       "  'More specifically  , we first learn a codebook in an unsupervised way from 15 million images collected from ImageNet. Instead of using domain knowledge to extract features from labeled OM images  , we construct features based on a dataset entirely OM-irrelevant.',\n",
       "  'Subsequently  , image patches were extracted and used to fine-tune Google`s Inception-V3 and ResNet50 convolutional neural networks CNNs  , both pre-trained on the ImageNet database  , enabling them to learn domain-specific features  , necessary to classify the histology images. The histology images  , provided as part of the BACH 2018 grand challenge  , were first normalized to correct for color variations resulting from inconsistencies during slide preparation.',\n",
       "  'We thus avoid comparing terms from widely different periods in time and overcome a severe limitation of existing methods for named entity evolution  , as shown by the high recall of 90% on the New York Times corpus. By analyzing only these time periods using a sliding window co-occurrence method we capture evolving terms in the same context.',\n",
       "  'Method to predict sentiment in informal texts using unsupervised dependency parsing.Algorithm based on sentiment propagation using linguistic content without training.Method to create lexicon using polarity expansion algorithm for specific domains.Our method compares favorably well with other unsupervised and supervised methods. The results obtained for the Cornell Movie Review  , Obama-McCain Debate and SemEval-2015 datasets confirm the competitive performance and the robustness of the system.',\n",
       "  'All the existing image steganography methods use manually crafted features to hide binary payloads into cover images. To this end  , we make following three major contributions: i we propose a deep learning based generic encoder-decoder architecture for image steganography; ii we introduce a new loss function that ensures joint end-to-end training of encoder-decoder networks; iii we perform extensive empirical evaluation of proposed architecture on a range of challenging publicly available datasets MNIST  , CIFAR10  , PASCAL-VOC12  , ImageNet  , LFW and report state-of-the-art payload capacity at high PSNR and SSIM values.',\n",
       "  'The key idea is to localize objects of a target class for which annotations are not available  , by transferring knowledge from related source classes with available annotations. In this paper we propose to automatically populate ImageNet with many more bounding-boxes  , by leveraging existing manual annotations.',\n",
       "  'The results obtained for the Cornell Movie Review  , Obama-McCain Debate and SemEval-2015 datasets confirm the competitive performance and the robustness of the system. These lexicons were created by means of a semiautomatic polarity expansion algorithm in order to improve accuracy in specific application domains.',\n",
       "  'Our method achieves comparable to better accuracy on MNIST dataset than manually labelled two layer networks for the same sized hidden layer. A SNN is trained for classification of handwritten digits with multiple layers of spiking neurons  , including both the feature extraction and classification layer  , using the proposed STDP rule.',\n",
       "  'We compare several relatedness measures for filtering to improve precision. We thus avoid comparing terms from widely different periods in time and overcome a severe limitation of existing methods for named entity evolution  , as shown by the high recall of 90% on the New York Times corpus.',\n",
       "  'In this paper  , a novel structure is proposed to extend standard support vector classifier to multi-class cases. Our approach is applied to two applications: handwritten digital recognition on MNIST database and face recognition on Cambridge ORL face database  , experimental results reveal that our method is effective and efficient.',\n",
       "  '56% smaller error for dense motion fields on the KITTI and an about 76% smaller error for sparse motion fields on the Sintel dataset. Experimental results shows tremendous improvements  , e.g.',\n",
       "  'Jossey Bass  , Sanfrancisco  , CA  , 2008; Wagner  , The global achievement gap: why even our best schools don\\\\u2019t teach the new survival skills our children need\\\\u2014and what we can do about it. New York: Peter Lang Publishing  , Inc.  , 2007; National Center on Education and the Economy  , Tough choices or tough times: The report of the new commission on the skills of the American workforce  , Revised.',\n",
       "  'Our approach is applied to two applications: handwritten digital recognition on MNIST database and face recognition on Cambridge ORL face database  , experimental results reveal that our method is effective and efficient. The accuracy rate is improved while the computational cost will not increase too much.',\n",
       "  'Unfortunately only a small fraction of them is manually annotated with bounding-boxes. ImageNet is a large-scale database of object classes with millions of images.',\n",
       "  'Through experiments on 0.5 million images from 219 classes we show that our technique i annotates a wide range of classes with bounding-boxes; ii effectively exploits the hierarchical structure of ImageNet  , since all sources and types of knowledge we propose contribute to the results; iii scales efficiently. Finally  , we employ the combined distribution in a procedure to jointly localize objects in all images of the target class.',\n",
       "  'Experiments on the benchmark CIFAR-10 show that our method outperforms several state-of-the-art algorithms. The feature responses are calculated using a learned dictionary  , which is learned using the sparse coding algorithm  , instead of the vector quantization VQ.',\n",
       "  'ImageNet is a large-scale database of object classes with millions of images. Through experiments on 0.5 million images from 219 classes we show that our technique i annotates a wide range of classes with bounding-boxes; ii effectively exploits the hierarchical structure of ImageNet  , since all sources and types of knowledge we propose contribute to the results; iii scales efficiently.',\n",
       "  'Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork with random weights that is smaller than  , but matches the performance of a ResNet-34 trained on ImageNet. In contrast  , we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values.',\n",
       "  'Not only do these \\\\\"untrained subnetworks\\\\\" exist  , but we provide an algorithm to effectively find them. Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork with random weights that is smaller than  , but matches the performance of a ResNet-34 trained on ImageNet.',\n",
       "  'We demonstrate our results on MNIST with a convolutional neural network and on ImageNet with Inception-v3 and ResNet-101. \\\\r\\\\nIn this paper  , we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image  , found through a gradient descent step.',\n",
       "  \"The model was tested on a manually tagged data set constructed from the last five years students' comments from Sukkur IBA University as well as on a standard SemEval-2014 data set. The first layer predicts the aspects described within the feedback and later specifies the orientation positive  , negative  , and neutral of those predicted aspects.\",\n",
       "  'New York: Peter Lang Publishing  , Inc.  , 2007; National Center on Education and the Economy  , Tough choices or tough times: The report of the new commission on the skills of the American workforce  , Revised. However  , these skills are not the stuff of the current standard-based regime influencing K-12 education Gee  , Good video games and good learning: collected essays on video games  , learning and literacy.',\n",
       "  'To this end  , we make following three major contributions: i we propose a deep learning based generic encoder-decoder architecture for image steganography; ii we introduce a new loss function that ensures joint end-to-end training of encoder-decoder networks; iii we perform extensive empirical evaluation of proposed architecture on a range of challenging publicly available datasets MNIST  , CIFAR10  , PASCAL-VOC12  , ImageNet  , LFW and report state-of-the-art payload capacity at high PSNR and SSIM values. Here we propose a convolutional neural network based encoder-decoder architecture for embedding of images as payload.',\n",
       "  'While deep neural networks have proven to be a powerful tool for many recognition and classification tasks  , their stability properties are still not well understood. We demonstrate our results on MNIST with a convolutional neural network and on ImageNet with Inception-v3 and ResNet-101.',\n",
       "  'We exhaustively evaluated our proposal on 23 confidence measures  , including 5 top-performing ones based on random-forests and CNNs  , training our networks with two popular stereo algorithms and a small subset 25 out of 194 frames of the KITTI 2012 dataset. This paper aims at increasing  , by means of a deep network  , the effectiveness of state-of-the-art confidence measures exploiting the local consistency assumption.',\n",
       "  'The ResNet50 network based on residual learning achieved a test classification accuracy of 97.50% for four classes  , outperforming the Inception-V3 network which achieved an accuracy of 91.25%. Subsequently  , image patches were extracted and used to fine-tune Google`s Inception-V3 and ResNet50 convolutional neural networks CNNs  , both pre-trained on the ImageNet database  , enabling them to learn domain-specific features  , necessary to classify the histology images.',\n",
       "  'We achieve this by exploiting a buffer overflow vulnerability to smash the call stack and intrude a remote node over the radio channel. In this paper  , we demonstrate how to execute malware on wireless sensor nodes that are based on the Von Neumann architecture.',\n",
       "  'We also analyze the parameter space to provide rationales for parameter fine-tuning and provide additional methods to improve noise resilience and input intensity variations. Our method achieves comparable to better accuracy on MNIST dataset than manually labelled two layer networks for the same sized hidden layer.',\n",
       "  'In this paper we propose to automatically populate ImageNet with many more bounding-boxes  , by leveraging existing manual annotations. This prevents useful developments  , such as learning reliable object detectors for thousands of classes.',\n",
       "  'The tools which involved in giving a shape to the idea involved are Microsoft Visual Studio 2012 Express for Phones  , Windows WVGA 512MB  , Windows Phone 8.0 SDK  , Windows 8.1 Professional Operating System. Methods/Statistical Analysis: To study the mentioned objective the websites referred at the end like tech-recipes  , stack overflow and msdn were referred for gaining an idea on how to implement the objective.',\n",
       "  'In this work we address the problem of feature extraction for image object recognition. Experiments on the benchmark CIFAR-10 show that our method outperforms several state-of-the-art algorithms.',\n",
       "  \"Unlike many other LSTM models proposed for other domains  , the proposed model is quite simple in terms of architecture which results in less complexity. The model was tested on a manually tagged data set constructed from the last five years students' comments from Sukkur IBA University as well as on a standard SemEval-2014 data set.\",\n",
       "  'By breaking the malware into multiple packets  , the attacker can inject arbitrarily long malicious code to the node and completely take control of it. We achieve this by exploiting a buffer overflow vulnerability to smash the call stack and intrude a remote node over the radio channel.',\n",
       "  'The codebook gives us what the encoders consider being the fundamental elements of those 15 million images. More specifically  , we first learn a codebook in an unsupervised way from 15 million images collected from ImageNet.',\n",
       "  'Experimental results show that our approach dramatically increases the effectiveness of all the 23 confidence measures on the remaining frames. We exhaustively evaluated our proposal on 23 confidence measures  , including 5 top-performing ones based on random-forests and CNNs  , training our networks with two popular stereo algorithms and a small subset 25 out of 194 frames of the KITTI 2012 dataset.',\n",
       "  'Sparse motion estimation with local optical flow methods is fundamental for a wide range of computer vision application. 56% smaller error for dense motion fields on the KITTI and an about 76% smaller error for sparse motion fields on the Sintel dataset.'],\n",
       " ['mediaeval',\n",
       "  'imageclef',\n",
       "  'gerbil',\n",
       "  'chemid',\n",
       "  'scovo',\n",
       "  'refseq',\n",
       "  'stanford large network dataset',\n",
       "  'jnlpba',\n",
       "  'pslnl',\n",
       "  'brightkite',\n",
       "  'ldoce',\n",
       "  'worldcat',\n",
       "  'new york times',\n",
       "  'kitti dataset',\n",
       "  'webkb',\n",
       "  'duc2001',\n",
       "  'cifar',\n",
       "  'moviepilot',\n",
       "  'ionosphere',\n",
       "  'wt2g',\n",
       "  'blog06',\n",
       "  'lexvo',\n",
       "  'cyworld',\n",
       "  'citebase',\n",
       "  'dailymed',\n",
       "  'robust04',\n",
       "  'specweb',\n",
       "  'blogpulse',\n",
       "  'newegg',\n",
       "  'datahub',\n",
       "  'wikia',\n",
       "  'shop.com',\n",
       "  'ask.com',\n",
       "  'cacm',\n",
       "  'wikiwars',\n",
       "  'friendfeed',\n",
       "  'mnist',\n",
       "  'booking.com',\n",
       "  'nasdaq',\n",
       "  'as3ap',\n",
       "  'stack overflow',\n",
       "  'movierating',\n",
       "  'spambase',\n",
       "  'imagenet',\n",
       "  'reddit',\n",
       "  'buzzfeed',\n",
       "  'semeval',\n",
       "  'ws-353',\n",
       "  'tdt',\n",
       "  'fedweb',\n",
       "  'orkut'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_data_extraction.sentence_extraction(model_name, cycle, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Term Expansion\n",
    "For the *Term Expansion* we process all the sentences we just extracted, and use the Natural Language Toolkit (NLTK) to find generic entities, these are words that could potentially be an entity given that they are nouns, they have a certain position in a sentence, and/or a relationship with other parts of the sentence, more information [here](https://www.nltk.org/book/ch07.html). \n",
    "\n",
    "Then we use the vectors of those words, obtained from word2vec, and cluster them using k-means, selecting the best number of clusters in base of their silhouette score. If there is a seed entity in the cluster, we consider that the rest of the potential entities in that same cluster should be kept as Expanded Terms. Like the previous step, this creates a txt file with the expanded terms in the `processing_files` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting term expansion\n",
      "Started to extract generic named entity from sentences...\n",
      "Finished processing sentences with 75 new possible entities\n",
      "11\n",
      "['psnr', 'cnns', 'lstm', 'imagenet', 'microsoft_visual', 'ssim', 'snn', 'kitti', 'stdp', 'resnet50', 'lfw']\n",
      "Started term clustering\n",
      "{1: ['psnr'], 0: ['cnns', 'lstm', 'imagenet', 'microsoft_visual', 'ssim', 'snn', 'kitti', 'stdp', 'resnet50', 'lfw']}\n",
      "2 ['cnns', 'lstm', 'imagenet', 'microsoft visual', 'ssim', 'snn', 'kitti', 'stdp', 'resnet50', 'lfw']\n",
      "{2: ['psnr'], 1: ['cnns', 'lstm', 'imagenet'], 0: ['microsoft_visual', 'ssim', 'snn', 'kitti', 'stdp', 'resnet50', 'lfw']}\n",
      "{2: ['psnr'], 3: ['cnns', 'lstm', 'imagenet'], 0: ['microsoft_visual', 'ssim', 'kitti', 'stdp'], 1: ['snn', 'resnet50', 'lfw']}\n",
      "{2: ['psnr'], 1: ['cnns', 'imagenet'], 3: ['lstm'], 4: ['microsoft_visual', 'ssim', 'kitti', 'stdp'], 0: ['snn', 'resnet50', 'lfw']}\n",
      "{3: ['psnr'], 4: ['cnns'], 0: ['lstm'], 2: ['imagenet', 'microsoft_visual', 'kitti'], 5: ['ssim', 'stdp'], 1: ['snn', 'resnet50', 'lfw']}\n",
      "{4: ['psnr'], 1: ['cnns'], 3: ['lstm'], 6: ['imagenet'], 2: ['microsoft_visual', 'kitti'], 5: ['ssim', 'stdp'], 0: ['snn', 'resnet50', 'lfw']}\n",
      "{2: ['psnr'], 7: ['cnns'], 5: ['lstm'], 1: ['imagenet'], 3: ['microsoft_visual', 'kitti'], 6: ['ssim'], 4: ['snn', 'resnet50', 'lfw'], 0: ['stdp']}\n",
      "Added 10 expanded terms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cnns',\n",
       " 'ssim',\n",
       " 'snn',\n",
       " 'imagenet',\n",
       " 'stdp',\n",
       " 'lfw',\n",
       " 'microsoft visual',\n",
       " 'kitti',\n",
       " 'resnet50',\n",
       " 'lstm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_sentence_expansion.term_expansion(model_name, cycle, \"embedding_models/modelword2vecbigram.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK produces over 4 thousand potential entities, after clustering and selecting, we keep 160 Expanded Terms.\n",
    "\n",
    "### 3 - Sentence Expansion\n",
    "In *Sentence Expansion*, we use doc2vec to find a single similar sentence (this can be modified in the code, for instance, in line 248) to each one of the sentences that we obtained in Step 1. If the sentence has a consine similarity above 0.5 (also can be changed), we add it to our set of Expanded Sentences. This set is stored in the `processing_files`.\n",
    "\n",
    "*Note:* There is a chance that this process runs out of memmory if the doc2vec model is too large, this is because it needs to compare the current sentence against ALL other sentences to find the most similar. To fix this, you might have to retrace your steps and create a smaller model in the Preparation Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentence expansion\n",
      "Finding similar sentences to the 55 starting sentences\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd-sem-ie/SmartPub-TSENER/m1_preprocessing/term_sentence_expansion.py:219: ElasticsearchDeprecationWarning: [types removal] Specifying types in search requests is deprecated.\n",
      "  res = es.search(index=\"devtwosentnew\", doc_type=\"devtwosentnorulesnew\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 53 expanded sentences to the 55 original\n"
     ]
    }
   ],
   "source": [
    "term_sentence_expansion.sentence_expansion(model_name, cycle, doc2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Expansion generates **130** new sentences, if they include one of the Expanded Terms, they will be used as possitive examples for the training, if they do not include any entity, they are also very helpful as similar sentences but as negative examples. We argue that this helps to improve the performance of the NER Tagger.\n",
    "\n",
    "### 4 - Training Data Generation\n",
    "For the [Stanford NER Tagger](https://nlp.stanford.edu/software/CRF-NER.html) model training, a specific file is required. The format consists of sentences, compiled as a list of *word -> label*, where entities are labelled either with the current entity, say **DATASET**, or with **O** if they are not. \n",
    "\n",
    "    ...\n",
    "    we     O\n",
    "    apply  O\n",
    "    this   O\n",
    "    to     O\n",
    "    the    O\n",
    "    Wikia  DATASET\n",
    "    corpus O\n",
    "    ...\n",
    "    \n",
    "For this, we take all the Sentences + Expanded Sentences, and label all the Seed Terms + Expanded Terms in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling sentences in the required format\n",
      "108 lines labelled\n"
     ]
    }
   ],
   "source": [
    "training_data_generation.sentence_labelling(model_name, cycle, sentence_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - NER Tagger Training\n",
    "Once we have the file with all the labelled sentences, we have to create a [property file](https://nlp.stanford.edu/software/crf-faq.shtml#a) for the Tagger. In this file we can edit certain configurations, point to the correct training file, and the location of the Tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating property file for Stanford NER training\n"
     ]
    }
   ],
   "source": [
    "ner_training.create_prop(model_name, cycle, sentence_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data in place, and the property file ready, we can start training. This script executes a Java process like command line, which will generate a CRF (Conditional Random Field) file: the NER Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "ner_training.train_model(model_name, cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started extraction for the dataset_50 model, in cycle number 0\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd-sem-ie/SmartPub-TSENER/m1_postprocessing/extract_new_entities.py:185: ElasticsearchDeprecationWarning: [types removal] Specifying types in search requests is deprecated.\n",
      "  res = es.search(index=\"ir\", doc_type=\"publications\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................Tagged 20/10000 full texts\n",
      "....................Tagged 40/10000 full texts\n",
      "....................Tagged 60/10000 full texts\n",
      "....................Tagged 80/10000 full texts\n",
      "....................Tagged 100/10000 full texts\n",
      "....................Tagged 120/10000 full texts\n",
      "....................Tagged 140/10000 full texts\n",
      "....................Tagged 160/10000 full texts\n",
      "....................Tagged 180/10000 full texts\n",
      "....................Tagged 200/10000 full texts\n",
      "....................Tagged 220/10000 full texts\n",
      "....................Tagged 240/10000 full texts\n",
      "....................Tagged 260/10000 full texts\n",
      "....................Tagged 280/10000 full texts\n",
      "....................Tagged 300/10000 full texts\n",
      "....................Tagged 320/10000 full texts\n",
      "....................Tagged 340/10000 full texts\n",
      "....................Tagged 360/10000 full texts\n",
      "....................Tagged 380/10000 full texts\n",
      "....................Tagged 400/10000 full texts\n",
      "....................Tagged 420/10000 full texts\n",
      "....................Tagged 440/10000 full texts\n",
      "....................Tagged 460/10000 full texts\n",
      "....................Tagged 480/10000 full texts\n",
      "....................Tagged 500/10000 full texts\n",
      "....................Tagged 520/10000 full texts\n",
      "....................Tagged 540/10000 full texts\n",
      "....................Tagged 560/10000 full texts\n",
      "....................Tagged 580/10000 full texts\n",
      "....................Tagged 600/10000 full texts\n",
      "....................Tagged 620/10000 full texts\n",
      "....................Tagged 640/10000 full texts\n",
      "....................Tagged 660/10000 full texts\n",
      "....................Tagged 680/10000 full texts\n",
      "....................Tagged 700/10000 full texts\n",
      "....................Tagged 720/10000 full texts\n",
      "....................Tagged 740/10000 full texts\n",
      "....................Tagged 760/10000 full texts\n",
      "....................Tagged 780/10000 full texts\n",
      "....................Tagged 800/10000 full texts\n",
      "....................Tagged 820/10000 full texts\n",
      "....................Tagged 840/10000 full texts\n",
      "....................Tagged 860/10000 full texts\n",
      "....................Tagged 880/10000 full texts\n",
      "....................Tagged 900/10000 full texts\n",
      "....................Tagged 920/10000 full texts\n",
      "....................Tagged 940/10000 full texts\n",
      "....................Tagged 960/10000 full texts\n",
      "....................Tagged 980/10000 full texts\n",
      "....................Tagged 1000/10000 full texts\n",
      "....................Tagged 1020/10000 full texts\n",
      "....................Tagged 1040/10000 full texts\n",
      "....................Tagged 1060/10000 full texts\n",
      "....................Tagged 1080/10000 full texts\n",
      "....................Tagged 1100/10000 full texts\n",
      "....................Tagged 1120/10000 full texts\n",
      "....................Tagged 1140/10000 full texts\n",
      "....................Tagged 1160/10000 full texts\n",
      "....................Tagged 1180/10000 full texts\n",
      "....................Tagged 1200/10000 full texts\n",
      "....................Tagged 1220/10000 full texts\n",
      "....................Tagged 1240/10000 full texts\n",
      "....................Tagged 1260/10000 full texts\n",
      "....................Tagged 1280/10000 full texts\n",
      "....................Tagged 1300/10000 full texts\n",
      "....................Tagged 1320/10000 full texts\n",
      "....................Tagged 1340/10000 full texts\n",
      "....................Tagged 1360/10000 full texts\n",
      "....................Tagged 1380/10000 full texts\n",
      "....................Tagged 1400/10000 full texts\n",
      "....................Tagged 1420/10000 full texts\n",
      "....................Tagged 1440/10000 full texts\n",
      "....................Tagged 1460/10000 full texts\n",
      "....................Tagged 1480/10000 full texts\n",
      "....................Tagged 1500/10000 full texts\n",
      "....................Tagged 1520/10000 full texts\n",
      "....................Tagged 1540/10000 full texts\n",
      "....................Tagged 1560/10000 full texts\n",
      "....................Tagged 1580/10000 full texts\n",
      "....................Tagged 1600/10000 full texts\n",
      "....................Tagged 1620/10000 full texts\n",
      "....................Tagged 1640/10000 full texts\n",
      "....................Tagged 1660/10000 full texts\n",
      "....................Tagged 1680/10000 full texts\n",
      "....................Tagged 1700/10000 full texts\n",
      "....................Tagged 1720/10000 full texts\n",
      "....................Tagged 1740/10000 full texts\n",
      "....................Tagged 1760/10000 full texts\n",
      "....................Tagged 1780/10000 full texts\n",
      "....................Tagged 1800/10000 full texts\n",
      "....................Tagged 1820/10000 full texts\n",
      "....................Tagged 1840/10000 full texts\n",
      "....................Tagged 1860/10000 full texts\n",
      "....................Tagged 1880/10000 full texts\n",
      "....................Tagged 1900/10000 full texts\n",
      "....................Tagged 1920/10000 full texts\n",
      "....................Tagged 1940/10000 full texts\n",
      "....................Tagged 1960/10000 full texts\n",
      "....................Tagged 1980/10000 full texts\n",
      "....................Tagged 2000/10000 full texts\n",
      "....................Tagged 2020/10000 full texts\n",
      "....................Tagged 2040/10000 full texts\n",
      "....................Tagged 2060/10000 full texts\n",
      "....................Tagged 2080/10000 full texts\n",
      "....................Tagged 2100/10000 full texts\n",
      "....................Tagged 2120/10000 full texts\n",
      "....................Tagged 2140/10000 full texts\n",
      "....................Tagged 2160/10000 full texts\n",
      "....................Tagged 2180/10000 full texts\n",
      "....................Tagged 2200/10000 full texts\n",
      "....................Tagged 2220/10000 full texts\n",
      "....................Tagged 2240/10000 full texts\n",
      "....................Tagged 2260/10000 full texts\n",
      "....................Tagged 2280/10000 full texts\n",
      "....................Tagged 2300/10000 full texts\n",
      "....................Tagged 2320/10000 full texts\n",
      "....................Tagged 2340/10000 full texts\n",
      "....................Tagged 2360/10000 full texts\n",
      "....................Tagged 2380/10000 full texts\n",
      "....................Tagged 2400/10000 full texts\n",
      "....................Tagged 2420/10000 full texts\n",
      "....................Tagged 2440/10000 full texts\n",
      "....................Tagged 2460/10000 full texts\n",
      "....................Tagged 2480/10000 full texts\n",
      "....................Tagged 2500/10000 full texts\n",
      "....................Tagged 2520/10000 full texts\n",
      "....................Tagged 2540/10000 full texts\n",
      "....................Tagged 2560/10000 full texts\n",
      "....................Tagged 2580/10000 full texts\n",
      "....................Tagged 2600/10000 full texts\n",
      "....................Tagged 2620/10000 full texts\n",
      "....................Tagged 2640/10000 full texts\n",
      "....................Tagged 2660/10000 full texts\n",
      "....................Tagged 2680/10000 full texts\n",
      "....................Tagged 2700/10000 full texts\n",
      "....................Tagged 2720/10000 full texts\n",
      "....................Tagged 2740/10000 full texts\n",
      "....................Tagged 2760/10000 full texts\n",
      "....................Tagged 2780/10000 full texts\n",
      "....................Tagged 2800/10000 full texts\n",
      "....................Tagged 2820/10000 full texts\n",
      "....................Tagged 2840/10000 full texts\n",
      "....................Tagged 2860/10000 full texts\n",
      "....................Tagged 2880/10000 full texts\n",
      "....................Tagged 2900/10000 full texts\n",
      "....................Tagged 2920/10000 full texts\n",
      "....................Tagged 2940/10000 full texts\n",
      "....................Tagged 2960/10000 full texts\n",
      "....................Tagged 2980/10000 full texts\n",
      "....................Tagged 3000/10000 full texts\n",
      "....................Tagged 3020/10000 full texts\n",
      "....................Tagged 3040/10000 full texts\n",
      "....................Tagged 3060/10000 full texts\n",
      "....................Tagged 3080/10000 full texts\n",
      "....................Tagged 3100/10000 full texts\n",
      "....................Tagged 3120/10000 full texts\n",
      "....................Tagged 3140/10000 full texts\n",
      "....................Tagged 3160/10000 full texts\n",
      "....................Tagged 3180/10000 full texts\n",
      "....................Tagged 3200/10000 full texts\n",
      "....................Tagged 3220/10000 full texts\n",
      "....................Tagged 3240/10000 full texts\n",
      "....................Tagged 3260/10000 full texts\n",
      "....................Tagged 3280/10000 full texts\n",
      "....................Tagged 3300/10000 full texts\n",
      "....................Tagged 3320/10000 full texts\n",
      "....................Tagged 3340/10000 full texts\n",
      "....................Tagged 3360/10000 full texts\n",
      "....................Tagged 3380/10000 full texts\n",
      "....................Tagged 3400/10000 full texts\n",
      "....................Tagged 3420/10000 full texts\n",
      "....................Tagged 3440/10000 full texts\n",
      "....................Tagged 3460/10000 full texts\n",
      "....................Tagged 3480/10000 full texts\n",
      "........................Tagged 3600/10000 full texts\n",
      "....................Tagged 3620/10000 full texts\n",
      "....................Tagged 3640/10000 full texts\n",
      "....................Tagged 3660/10000 full texts\n",
      "....................Tagged 3680/10000 full texts\n",
      "....................Tagged 3700/10000 full texts\n",
      "....................Tagged 3720/10000 full texts\n",
      "....................Tagged 3740/10000 full texts\n",
      "....................Tagged 3760/10000 full texts\n",
      "....................Tagged 3780/10000 full texts\n",
      "....................Tagged 3800/10000 full texts\n",
      "....................Tagged 3820/10000 full texts\n",
      "....................Tagged 3840/10000 full texts\n",
      "....................Tagged 3860/10000 full texts\n",
      "....................Tagged 3880/10000 full texts\n",
      "....................Tagged 3900/10000 full texts\n",
      "....................Tagged 3920/10000 full texts\n",
      "....................Tagged 3940/10000 full texts\n",
      "....................Tagged 3960/10000 full texts\n",
      "....................Tagged 3980/10000 full texts\n",
      "....................Tagged 4000/10000 full texts\n",
      "....................Tagged 4020/10000 full texts\n",
      "....................Tagged 4040/10000 full texts\n",
      "....................Tagged 4060/10000 full texts\n",
      "....................Tagged 4080/10000 full texts\n",
      "....................Tagged 4100/10000 full texts\n",
      "....................Tagged 4120/10000 full texts\n",
      "....................Tagged 4140/10000 full texts\n",
      "....................Tagged 4160/10000 full texts\n",
      "....................Tagged 4180/10000 full texts\n",
      "....................Tagged 4200/10000 full texts\n",
      "....................Tagged 4220/10000 full texts\n",
      "....................Tagged 4240/10000 full texts\n",
      "....................Tagged 4260/10000 full texts\n",
      "....................Tagged 4280/10000 full texts\n",
      "....................Tagged 4300/10000 full texts\n",
      "....................Tagged 4320/10000 full texts\n",
      "....................Tagged 4340/10000 full texts\n",
      "....................Tagged 4360/10000 full texts\n",
      "....................Tagged 4380/10000 full texts\n",
      "....................Tagged 4400/10000 full texts\n",
      "....................Tagged 4420/10000 full texts\n",
      "....................Tagged 4440/10000 full texts\n",
      "....................Tagged 4460/10000 full texts\n",
      "....................Tagged 4480/10000 full texts\n",
      "....................Tagged 4500/10000 full texts\n",
      "....................Tagged 4520/10000 full texts\n",
      "....................Tagged 4540/10000 full texts\n",
      "....................Tagged 4560/10000 full texts\n",
      "....................Tagged 4580/10000 full texts\n",
      "....................Tagged 4600/10000 full texts\n",
      "....................Tagged 4620/10000 full texts\n",
      "....................Tagged 4640/10000 full texts\n",
      "....................Tagged 4660/10000 full texts\n",
      "....................Tagged 4680/10000 full texts\n",
      "....................Tagged 4700/10000 full texts\n",
      "....................Tagged 4720/10000 full texts\n",
      "....................Tagged 4740/10000 full texts\n",
      "....................Tagged 4760/10000 full texts\n",
      "....................Tagged 4780/10000 full texts\n",
      "....................Tagged 4800/10000 full texts\n",
      "....................Tagged 4820/10000 full texts\n",
      "....................Tagged 4840/10000 full texts\n",
      "....................Tagged 4860/10000 full texts\n",
      "....................Tagged 4880/10000 full texts\n",
      "....................Tagged 4900/10000 full texts\n",
      "....................Tagged 4920/10000 full texts\n",
      "....................Tagged 4940/10000 full texts\n",
      "....................Tagged 4960/10000 full texts\n",
      "....................Tagged 4980/10000 full texts\n",
      "....................Tagged 5000/10000 full texts\n",
      "....................Tagged 5020/10000 full texts\n",
      "....................Tagged 5040/10000 full texts\n",
      "....................Tagged 5060/10000 full texts\n",
      "....................Tagged 5080/10000 full texts\n",
      "....................Tagged 5100/10000 full texts\n",
      "....................Tagged 5120/10000 full texts\n",
      "....................Tagged 5140/10000 full texts\n",
      "....................Tagged 5160/10000 full texts\n",
      "....................Tagged 5180/10000 full texts\n",
      "....................Tagged 5200/10000 full texts\n",
      "....................Tagged 5220/10000 full texts\n",
      "....................Tagged 5240/10000 full texts\n",
      "....................Tagged 5260/10000 full texts\n",
      "....................Tagged 5280/10000 full texts\n",
      "....................Tagged 5300/10000 full texts\n",
      "....................Tagged 5320/10000 full texts\n",
      "....................Tagged 5340/10000 full texts\n",
      "....................Tagged 5360/10000 full texts\n",
      "....................Tagged 5380/10000 full texts\n",
      "....................Tagged 5400/10000 full texts\n",
      "....................Tagged 5420/10000 full texts\n",
      "....................Tagged 5440/10000 full texts\n",
      "....................Tagged 5460/10000 full texts\n",
      "....................Tagged 5480/10000 full texts\n",
      "....................Tagged 5500/10000 full texts\n",
      "....................Tagged 5520/10000 full texts\n",
      "....................Tagged 5540/10000 full texts\n",
      "....................Tagged 5560/10000 full texts\n",
      "....................Tagged 5580/10000 full texts\n",
      "....................Tagged 5600/10000 full texts\n",
      "....................Tagged 5620/10000 full texts\n",
      "....................Tagged 5640/10000 full texts\n",
      "....................Tagged 5660/10000 full texts\n",
      "....................Tagged 5680/10000 full texts\n",
      "....................Tagged 5700/10000 full texts\n",
      "....................Tagged 5720/10000 full texts\n",
      "....................Tagged 5740/10000 full texts\n",
      "....................Tagged 5760/10000 full texts\n",
      "....................Tagged 5780/10000 full texts\n",
      "....................Tagged 5800/10000 full texts\n",
      "....................Tagged 5820/10000 full texts\n",
      "....................Tagged 5840/10000 full texts\n",
      "....................Tagged 5860/10000 full texts\n",
      "....................Tagged 5880/10000 full texts\n",
      "....................Tagged 5900/10000 full texts\n",
      "....................Tagged 5920/10000 full texts\n",
      "....................Tagged 5940/10000 full texts\n",
      "....................Tagged 5960/10000 full texts\n",
      "....................Tagged 5980/10000 full texts\n",
      "....................Tagged 6000/10000 full texts\n",
      "....................Tagged 6020/10000 full texts\n",
      "....................Tagged 6040/10000 full texts\n",
      "....................Tagged 6060/10000 full texts\n",
      "....................Tagged 6080/10000 full texts\n",
      "....................Tagged 6100/10000 full texts\n",
      "....................Tagged 6120/10000 full texts\n",
      "....................Tagged 6140/10000 full texts\n",
      "....................Tagged 6160/10000 full texts\n",
      "....................Tagged 6180/10000 full texts\n",
      "....................Tagged 6200/10000 full texts\n",
      "....................Tagged 6220/10000 full texts\n",
      "....................Tagged 6240/10000 full texts\n",
      "....................Tagged 6260/10000 full texts\n",
      "....................Tagged 6280/10000 full texts\n",
      "....................Tagged 6300/10000 full texts\n",
      "....................Tagged 6320/10000 full texts\n",
      "....................Tagged 6340/10000 full texts\n",
      "....................Tagged 6360/10000 full texts\n",
      "....................Tagged 6380/10000 full texts\n",
      "....................Tagged 6400/10000 full texts\n",
      "....................Tagged 6420/10000 full texts\n",
      "....................Tagged 6440/10000 full texts\n",
      "....................Tagged 6460/10000 full texts\n",
      "....................Tagged 6480/10000 full texts\n",
      "....................Tagged 6500/10000 full texts\n",
      "....................Tagged 6520/10000 full texts\n",
      "....................Tagged 6540/10000 full texts\n",
      "....................Tagged 6560/10000 full texts\n",
      "....................Tagged 6580/10000 full texts\n",
      "....................Tagged 6600/10000 full texts\n",
      "....................Tagged 6620/10000 full texts\n",
      "....................Tagged 6640/10000 full texts\n",
      "....................Tagged 6660/10000 full texts\n",
      "....................Tagged 6680/10000 full texts\n",
      "....................Tagged 6700/10000 full texts\n",
      "....................Tagged 6720/10000 full texts\n",
      "....................Tagged 6740/10000 full texts\n",
      "....................Tagged 6760/10000 full texts\n",
      "....................Tagged 6780/10000 full texts\n",
      "....................Tagged 6800/10000 full texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Tagged 6820/10000 full texts\n",
      "....................Tagged 6840/10000 full texts\n",
      "....................Tagged 6860/10000 full texts\n",
      "....................Tagged 6880/10000 full texts\n",
      "....................Tagged 6900/10000 full texts\n",
      "....................Tagged 6920/10000 full texts\n",
      "....................Tagged 6940/10000 full texts\n",
      "....................Tagged 6960/10000 full texts\n",
      "....................Tagged 6980/10000 full texts\n",
      "....................Tagged 7000/10000 full texts\n",
      "....................Tagged 7020/10000 full texts\n",
      "....................Tagged 7040/10000 full texts\n",
      "....................Tagged 7060/10000 full texts\n",
      "....................Tagged 7080/10000 full texts\n",
      "....................Tagged 7100/10000 full texts\n",
      "....................Tagged 7120/10000 full texts\n",
      "....................Tagged 7140/10000 full texts\n",
      "....................Tagged 7160/10000 full texts\n",
      "....................Tagged 7180/10000 full texts\n",
      "....................Tagged 7200/10000 full texts\n",
      "....................Tagged 7220/10000 full texts\n",
      "....................Tagged 7240/10000 full texts\n",
      "....................Tagged 7260/10000 full texts\n",
      "....................Tagged 7280/10000 full texts\n",
      "....................Tagged 7300/10000 full texts\n",
      "....................Tagged 7320/10000 full texts\n",
      "....................Tagged 7340/10000 full texts\n",
      "....................Tagged 7360/10000 full texts\n",
      "....................Tagged 7380/10000 full texts\n",
      "....................Tagged 7400/10000 full texts\n",
      "....................Tagged 7420/10000 full texts\n",
      "....................Tagged 7440/10000 full texts\n",
      "....................Tagged 7460/10000 full texts\n",
      "....................Tagged 7480/10000 full texts\n",
      "....................Tagged 7500/10000 full texts\n",
      "....................Tagged 7520/10000 full texts\n",
      "....................Tagged 7540/10000 full texts\n",
      "....................Tagged 7560/10000 full texts\n",
      "....................Tagged 7580/10000 full texts\n",
      "....................Tagged 7600/10000 full texts\n",
      "....................Tagged 7620/10000 full texts\n",
      "....................Tagged 7640/10000 full texts\n",
      "....................Tagged 7660/10000 full texts\n",
      "....................Tagged 7680/10000 full texts\n",
      "....................Tagged 7700/10000 full texts\n",
      "....................Tagged 7720/10000 full texts\n",
      "....................Tagged 7740/10000 full texts\n",
      "....................Tagged 7760/10000 full texts\n",
      "....................Tagged 7780/10000 full texts\n",
      "....................Tagged 7800/10000 full texts\n",
      "....................Tagged 7820/10000 full texts\n",
      "....................Tagged 7840/10000 full texts\n",
      "....................Tagged 7860/10000 full texts\n",
      "....................Tagged 7880/10000 full texts\n",
      "....................Tagged 7900/10000 full texts\n",
      "....................Tagged 7920/10000 full texts\n",
      "....................Tagged 7940/10000 full texts\n",
      "....................Tagged 7960/10000 full texts\n",
      "....................Tagged 7980/10000 full texts\n",
      "....................Tagged 8000/10000 full texts\n",
      "....................Tagged 8020/10000 full texts\n",
      "....................Tagged 8040/10000 full texts\n",
      "....................Tagged 8060/10000 full texts\n",
      "....................Tagged 8080/10000 full texts\n",
      "....................Tagged 8100/10000 full texts\n",
      "....................Tagged 8120/10000 full texts\n",
      "....................Tagged 8140/10000 full texts\n",
      "....................Tagged 8160/10000 full texts\n",
      "....................Tagged 8180/10000 full texts\n",
      "....................Tagged 8200/10000 full texts\n",
      "....................Tagged 8220/10000 full texts\n",
      "....................Tagged 8240/10000 full texts\n",
      "....................Tagged 8260/10000 full texts\n",
      "....................Tagged 8280/10000 full texts\n",
      "....................Tagged 8300/10000 full texts\n",
      "....................Tagged 8320/10000 full texts\n",
      "....................Tagged 8340/10000 full texts\n",
      "....................Tagged 8360/10000 full texts\n",
      "....................Tagged 8380/10000 full texts\n",
      "....................Tagged 8400/10000 full texts\n",
      "....................Tagged 8420/10000 full texts\n",
      "....................Tagged 8440/10000 full texts\n",
      "....................Tagged 8460/10000 full texts\n",
      "....................Tagged 8480/10000 full texts\n",
      "....................Tagged 8500/10000 full texts\n",
      "....................Tagged 8520/10000 full texts\n",
      "....................Tagged 8540/10000 full texts\n",
      "....................Tagged 8560/10000 full texts\n",
      "....................Tagged 8580/10000 full texts\n",
      "....................Tagged 8600/10000 full texts\n",
      "...............Tagged 8740/10000 full texts\n",
      "....................Tagged 8760/10000 full texts\n",
      "....................Tagged 8780/10000 full texts\n",
      "....................Tagged 8800/10000 full texts\n",
      "....................Tagged 8820/10000 full texts\n",
      "....................Tagged 8840/10000 full texts\n",
      "....................Tagged 8860/10000 full texts\n",
      "....................Tagged 8880/10000 full texts\n",
      "....................Tagged 8900/10000 full texts\n",
      "....................Tagged 8920/10000 full texts\n",
      "....................Tagged 8940/10000 full texts\n",
      "....................Tagged 8960/10000 full texts\n",
      "....................Tagged 8980/10000 full texts\n",
      "....................Tagged 9000/10000 full texts\n",
      "....................Tagged 9020/10000 full texts\n",
      "....................Tagged 9040/10000 full texts\n",
      "....................Tagged 9060/10000 full texts\n",
      "....................Tagged 9080/10000 full texts\n",
      "....................Tagged 9100/10000 full texts\n",
      "....................Tagged 9120/10000 full texts\n",
      "....................Tagged 9140/10000 full texts\n",
      "....................Tagged 9160/10000 full texts\n",
      "....................Tagged 9180/10000 full texts\n",
      "....................Tagged 9200/10000 full texts\n",
      "....................Tagged 9220/10000 full texts\n",
      "....................Tagged 9240/10000 full texts\n",
      "....................Tagged 9260/10000 full texts\n",
      "....................Tagged 9280/10000 full texts\n",
      "....................Tagged 9300/10000 full texts\n",
      "....................Tagged 9320/10000 full texts\n",
      "....................Tagged 9340/10000 full texts\n",
      "....................Tagged 9360/10000 full texts\n",
      "....................Tagged 9380/10000 full texts\n",
      "....................Tagged 9400/10000 full texts\n",
      "....................Tagged 9420/10000 full texts\n",
      "....................Tagged 9440/10000 full texts\n",
      "....................Tagged 9460/10000 full texts\n",
      "....................Tagged 9480/10000 full texts\n",
      "....................Tagged 9500/10000 full texts\n",
      "....................Tagged 9520/10000 full texts\n",
      "....................Tagged 9540/10000 full texts\n",
      "....................Tagged 9560/10000 full texts\n",
      "....................Tagged 9580/10000 full texts\n",
      "............Tagged 9760/10000 full texts\n",
      "....................Tagged 9780/10000 full texts\n",
      "....................Tagged 9800/10000 full texts\n",
      "....................Tagged 9820/10000 full texts\n",
      "....................Tagged 9840/10000 full texts\n",
      "....................Tagged 9860/10000 full texts\n",
      "....................Tagged 9880/10000 full texts\n",
      "....................Tagged 9900/10000 full texts\n",
      "....................Tagged 9920/10000 full texts\n",
      "....................Tagged 9940/10000 full texts\n",
      "....................Tagged 9960/10000 full texts\n",
      "....................Tagged 9980/10000 full texts\n",
      "....................Tagged 10000/10000 full texts\n",
      ".Total of 380 filtered entities added\n"
     ]
    }
   ],
   "source": [
    "from m1_postprocessing import extract_new_entities\n",
    "extract_new_entities.ne_extraction(model_name, cycle, sentence_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a Long-Tail Entity Extraction Model!\n",
    "\n",
    "### 6 - Extract New Entities\n",
    "Since this is the goal of this whole process, we will take it step by step to see what's happening in the new Entity Extraction. Python fortunately allows for very easy use of the model we trained.\n",
    "\n",
    "First, we instantiate the Tagger like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.corpus import stopwords\n",
    "path_to_model = 'crf_trained_files/dataset_50_TSE_model_0.ser.gz' \n",
    "STANFORD_NER_PATH = 'stanford_files/stanford-ner.jar' # This should be in config, but we can show it again\n",
    "ner_tagger = StanfordNERTagger(path_to_model, STANFORD_NER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example sentence from one of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    tagged = ner_tagger.tag(text_to_tag.split())\n",
    "    result = []\n",
    "    print(tagged)\n",
    "    for jj, (a, b) in enumerate(tagged):\n",
    "        no_tag = 'O'\n",
    "        if b != no_tag:\n",
    "            a = a.translate(str.maketrans('', '', string.punctuation))\n",
    "            print(a)\n",
    "            result.append(a)\n",
    "            try:\n",
    "                if tagged[jj + 1][1] != no_tag:\n",
    "                    temp = tagged[jj + 1][0].translate(str.maketrans('', '', string.punctuation))\n",
    "                    bigram = a + ' ' + temp\n",
    "                    result.append(bigram)\n",
    "            except KeyError:\n",
    "                result.append(a)\n",
    "    extracted_words = [word for word in set(result) if word not in stopwords.words('english')]\n",
    "    return extracted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'O'), ('deep', 'O'), ('neural', 'O'), ('networks', 'O'), ('have', 'O'), ('proven', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('powerful', 'O'), ('tool', 'O'), ('for', 'O'), ('many', 'O'), ('recognition', 'O'), ('and', 'O'), ('classification', 'O'), ('tasks', 'O'), (',', 'O'), ('their', 'O'), ('stability', 'O'), ('properties', 'O'), ('are', 'O'), ('still', 'O'), ('not', 'O'), ('well', 'O'), ('understood.', 'O'), ('We', 'O'), ('demonstrate', 'O'), ('our', 'O'), ('results', 'O'), ('on', 'O'), ('MNIST', 'DATASET_50'), ('with', 'O'), ('a', 'O'), ('convolutional', 'O'), ('neural', 'O'), ('network', 'O'), ('and', 'O'), ('on', 'O'), ('ImageNet', 'DATASET_50'), ('with', 'O'), ('Inception-v3', 'O'), ('and', 'O'), ('ResNet-101.', 'O')]\n",
      "MNIST\n",
      "ImageNet\n",
      "While deep neural networks have proven to be a powerful tool for many recognition and classification tasks  , their stability properties are still not well understood. We demonstrate our results on MNIST with a convolutional neural network and on ImageNet with Inception-v3 and ResNet-101.\n",
      "['MNIST', 'ImageNet']\n"
     ]
    }
   ],
   "source": [
    "text_to_tag = \"While deep neural networks have proven to be a powerful tool for many recognition and classification tasks  , their stability properties are still not well understood. We demonstrate our results on MNIST with a convolutional neural network and on ImageNet with Inception-v3 and ResNet-101.\"\n",
    "extracted_entities = get_entities(text_to_tag)\n",
    "print(text_to_tag)\n",
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagger works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We propose a rather straightforward pipeline combining deep-feature extraction using a CNN pretrained on ImageNet and a classic clustering algorithm to classify sets of images. We study the impact of different pretrained CNN feature extractors on the problem of image set clustering for object classification as well as fine-grained classification.\n",
      "['problem', 'ImageNet']\n"
     ]
    }
   ],
   "source": [
    "text_to_tag = \"We propose a rather straightforward pipeline combining deep-feature extraction using a CNN pretrained on ImageNet and a classic clustering algorithm to classify sets of images. We study the impact of different pretrained CNN feature extractors on the problem of image set clustering for object classification as well as fine-grained classification.\"\n",
    "extracted_entities = get_entities(text_to_tag)\n",
    "print(text_to_tag)\n",
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we got some noise, so we can check our filtering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'processing_files/' + model_name + '_extracted_entities_' + str(cycle) + '.txt'\n",
    "f1 = open(path, 'w', encoding='utf-8')\n",
    "for item in extracted_entities:\n",
    "    f1.write(item + '\\n')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could filter these new entities and start the process all over again with a larger set of seed terms.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Here we can apply different filters and evaluate the results. \n",
    "\n",
    "* WS\n",
    "WordNet + Stopword filtering simply filters out stop and common words, following the assumption that long-tail entities may be rare and domain specific. \n",
    "\n",
    "* ST\n",
    "Similar Terms filtering is based on the same approach as the Term Expansion, by clustering the vectors of the terms and only keeping those clusters where there is one of the original seed terms.\n",
    "\n",
    "* PMI\n",
    "Pointwise Mutual Information (PMI) filtering adopts a semantic similarity measure derived  from the number of times two given keywords appear together in a sentence in our corpus   (for example, the sentence., \"we evaluate on x\" typically indicates a dataset). A set of context words, terms that often appear with the entities in the same sentence, is required for this filtering.\n",
    "\n",
    "* KBL\n",
    "Knowledge Base Lookup, like WordNet filtering, follows the assumption that long-tail entities will not appear in a common knowledge database, such as DBpedia.\n",
    "\n",
    "* Ensemble\n",
    "To reduce the amount of false positives at the end of the process, we propose to only keep entities that remain after applying several, or all, filtering approaches to the results. In the current implementation, the resulting entities have to pass all filters.\n",
    "\n",
    "For more details about the filtering, please refer to the main article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar Term filtering relies on clustering of the vectors of the extracted entities, \n",
    "# and therefore doesn't work with the few entities of this example\n",
    "\n",
    "filtering.filter_st(model_name, cycle, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problem', 'ImageNet']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities with PMI\n",
      "2 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet', 'problem']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.filter_pmi(model_name, cycle, context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities with WordNet and Stopwords\n",
      "1 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.filter_ws(model_name, cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities with knowledge base lookup\n",
      "2 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet', 'problem']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.filter_kbl(model_name, cycle, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities by vote of selected filter methods\n",
      "1 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.majority_vote(model_name, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering we get the actual dataset for the example!\n",
    "\n",
    "## Full-text Tagging\n",
    "\n",
    "For a more extended use, we can apply it to the document in our corpus. First we define a cleaning function to get rid of some characters that can affect the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(es_doc):\n",
    "    content = doc[\"_source\"][\"content\"]\n",
    "    content = content.replace(\"@ BULLET\", \"\")\n",
    "    content = content.replace(\"@BULLET\", \"\")\n",
    "    content = content.replace(\", \", \" , \")\n",
    "    content = content.replace('(', '')\n",
    "    content = content.replace(')', '')\n",
    "    content = content.replace('[', '')\n",
    "    content = content.replace(']', '')\n",
    "    content = content.replace(',', ' ,')\n",
    "    content = content.replace('?', ' ?')\n",
    "    content = content.replace('..', '.')\n",
    "    content = re.sub(r\"(\\.)([A-Z])\", r\"\\1 \\2\", content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we can take some text to tag, for instance, we can search for some documents by their title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 27 Hits:\n",
      "arxiv_141535856 Towards Practical Verification of Machine Learning: The Case of Computer\n",
      "  Vision Systems\n",
      "arxiv_83836033 Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social\n",
      "  Media\n",
      "arxiv_84327993 Compiling LATEX to computer algebra-enabled HTML5\n",
      "arxiv_129361451 Neural Networks Architecture Evaluation in a Quantum Computer\n",
      "arxiv_86419363 Robust Computer Algebra, Theorem Proving, and Oracle AI\n",
      "arxiv_83844865 Aligned Image-Word Representations Improve Inductive Transfer Across\n",
      "  Vision-Language Tasks\n",
      "arxiv_141535714 Discriminant Projection Representation-based Classification for Vision\n",
      "  Recognition\n"
     ]
    }
   ],
   "source": [
    "res = es.search(index = \"ir\", body = {\"query\": {\"match\": {\"title\" : \"computer vision\"}}}, size = 7)\n",
    "\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "for doc in res['hits']['hits']:\n",
    "    print(doc['_id'], doc['_source']['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the full corpus and labell all entities, evaluate performance, and improve for next training cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv_141535856 Towards Practical Verification of Machine Learning: The Case of Computer\n",
      "  Vision Systems\n",
      "['Scale', 'arXiv161002357', 'MobileNet', 'arXiv', 'Recognition', 'arXiv160407316', 'Imagenet', 'ResNet50', 'corresponding', 'ImageNet', 'problem', 'correctness', 'correspond', 'DNNs', 'Journal', 'imagenet', 'arXiv170808559', 'International', 'MNIST', 'convolutional', 'Visual', 'IMAGENET', 'IEEE', 'HOcclMask']\n",
      "\n",
      "arxiv_83836033 Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social\n",
      "  Media\n",
      "['corresponding', 'convolutional', 'correspond', 'Reddit']\n",
      "\n",
      "arxiv_84327993 Compiling LATEX to computer algebra-enabled HTML5\n",
      "[]\n",
      "\n",
      "arxiv_129361451 Neural Networks Architecture Evaluation in a Quantum Computer\n",
      "['probability', 'Journal', 'arXiv', 'problems', 'arXiv14123489', 'IEEE', 'arXiv170401127', 'International']\n",
      "\n",
      "arxiv_86419363 Robust Computer Algebra, Theorem Proving, and Oracle AI\n",
      "['problem', 'problems', 'ITPs', 'probabilities', 'corresponding', 'GitHub']\n",
      "\n",
      "arxiv_83844865 Aligned Image-Word Representations Improve Inductive Transfer Across\n",
      "  Vision-Language Tasks\n",
      "['ResNet', 'problem', 'ResNet152', 'Resnet', 'correct', 'problems', 'regionR', 'Top20', 'Resnet50', 'corresponding', 'correctly', 'ImageNet']\n",
      "\n",
      "arxiv_141535714 Discriminant Projection Representation-based Classification for Vision\n",
      "  Recognition\n",
      "['Ucf50', 'problem', 'corresponding']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in res['hits']['hits']:\n",
    "    print(doc['_id'], doc['_source']['title'])\n",
    "    text_to_tag = clean_text(doc)\n",
    "    print(get_entities(text_to_tag))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3214252  -0.37969807  0.09364401 -0.37111652 -0.4576496  -0.25795797\n",
      " -0.23723868  0.23140272  0.02122142  0.16007903  0.2336716   0.29020807\n",
      " -0.25165012  0.11152215 -0.35060167 -0.07547443 -0.02158279 -0.03102365\n",
      "  0.02915746  0.33018708  0.3375638   0.25470778 -0.8887305   0.20722558\n",
      " -0.6986758  -0.08464347 -0.18436599 -0.39977515  0.16841736  0.12366491\n",
      "  0.03072015  0.22328733  0.02792841 -0.25215346 -0.11053527  0.11957163\n",
      "  0.16688249  0.19489539  0.27103406 -0.15804775 -0.26202226 -0.26076326\n",
      " -0.09959411 -0.75458723  0.13774364  0.184828    0.03487746  0.09344629\n",
      "  0.12360897  0.32798806 -0.06824207 -0.03825451  0.10674092  0.09743937\n",
      " -0.68026274  0.09825796  0.51017755  0.62084085 -0.14710866 -0.09869652\n",
      "  0.19643784 -0.06447028  0.07415327  0.08325775 -0.18586558 -0.04954641\n",
      "  0.29521918 -0.01757311 -0.08808228 -0.12483073 -0.1033829   0.20644672\n",
      " -0.02305031 -0.06689497 -0.2181705   0.80309254 -0.5229904  -0.16335449\n",
      "  0.05110175 -0.44596887  0.25897023 -0.18221134 -0.14176507  0.28860247\n",
      "  0.11481759 -0.35945344  0.27356678 -0.2742029   0.18899383  0.09501678\n",
      " -0.5353917   0.11857669 -0.1312167  -0.56977564 -0.23504594 -0.18115748\n",
      "  0.37308955 -0.42698023  0.17201096  0.24968234]\n",
      "[(30826279, 0.8250502347946167)]\n"
     ]
    }
   ],
   "source": [
    "#import gensim\n",
    "#doc2vec_model = gensim.models.Doc2Vec.load('embedding_models/doc2vec.model')\n",
    "with open(\"data/full_text_corpus.txt\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    new_vector = doc2vec_model.infer_vector(line.split())\n",
    "    print(new_vector)\n",
    "    sims = doc2vec_model.docvecs.most_similar([new_vector], topn=1)\n",
    "    print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the filtering approaches that we showed before, we can obtain the datasets used in each paper.\n",
    "\n",
    "This is only a demo of the TSE-NER approach, with a larger dataset we can improve the recall of the Tagger and extract even more entities, maintaining the precision with proper filtering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

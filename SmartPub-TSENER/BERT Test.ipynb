{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from m1_preprocessing import term_sentence_expansion\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "# BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bert_test.txt', 'r') as f:\n",
    "    sentences = [x.strip() for x in f.readlines()]\n",
    "\n",
    "all = \" \".join(sentences)\n",
    "sentences_splitted = nltk.sent_tokenize(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences 319\n",
      "Max sentence length:  138\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "print(\"# sentences\", len(sentences_splitted))\n",
    "# For every sentence...\n",
    "for sent in sentences_splitted:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  We study the applicability and potential of the algorithm to learn representations of varying depth in a handful of applications and domains  , highlighting the ability of the algorithm to provide discriminative feature representations that are able to achieve top performance. We present a hyper-parameter free  , off-the-shelf  , simple and fast unsupervised algorithm to discover hidden structure from the input data by enforcing a very strong form of sparsity.\n",
      "Token IDs: tensor([  102,   185,   527,   111, 13214,   137,  1411,   131,   111,  1172,\n",
      "          147,  6714,  6859,   131,  5543,  3826,   121,   106,  1500,  1004,\n",
      "          131,  2040,   137,  4371,   422, 18579,   111,  2495,   131,   111,\n",
      "         1172,   147,  1584, 20900,  2602,  6859,   198,   220,  2357,   147,\n",
      "         3120,  1623,  1150,   205,   103,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Segment IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "segment_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for (index, sent) in enumerate(sentences_splitted):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 150,          # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    tokens = encoded_dict['input_ids']\n",
    "    input_ids.append(tokens)\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    segment_ids.append(torch.tensor([[index % 2] * 150]))\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "segment_ids = torch.cat(segment_ids, dim=0)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Segment IDs:', segment_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "segment_ids = []\n",
    "for (index, sent) in enumerate(sentences_splitted):\n",
    "    if index == 0:\n",
    "        sent = \"[CLS] \" + sent\n",
    "    sent += \" [SEP]\"\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    tokenized_text += tokens\n",
    "    segment_ids += [index % 2] * len(tokens)\n",
    "tokenized_text = tokenized_text[:-1]\n",
    "segment_ids = segment_ids[:-1]\n",
    "assert(len(tokenized_text) == len(segment_ids))\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segment_ids])\n",
    "#text_bert = \"[CLS] \" + \" [SEP] \".join(sentences_splitted)\n",
    "#tokenized_text = tokenizer.tokenize(text_bert)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "config = AutoConfig.from_pretrained(\"allenai/scibert_scivocab_uncased\", output_hidden_states=True)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased',\n",
    "                                  config = config, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers.\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    # Evaluating the model will return a different number of objects based on\n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "    # becase we set `output_hidden_states = True`, the third item will be the\n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 319\n",
      "Number of tokens: 150\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 319, 150, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([319, 150, 13, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1, 2, 0, 3)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 150 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence #1...\n",
    "for token in token_embeddings[0]:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([ 1.8792, -0.3661, -1.6897,  1.6397,  0.7315])\n",
      "1 tensor([ 1.6456,  1.1876, -3.7473, -1.1620, -6.6588])\n",
      "2 tensor([ 3.1494, -1.6758, -4.0400,  2.4409, -3.7305])\n",
      "3 tensor([-0.6335,  0.7184,  1.3884,  0.1791, -0.4720])\n",
      "4 tensor([ 6.0752,  0.2215, -1.8379, -0.9989, -0.5707])\n",
      "5 tensor([1.1246, 0.8387, 3.8351, 2.5916, 0.5349])\n",
      "6 tensor([ 4.3702,  1.9355,  2.1857, -2.6150,  0.4570])\n",
      "7 tensor([ 0.9938,  3.9295,  2.8824, -0.3774,  3.1096])\n",
      "8 tensor([ 0.5817,  4.3221, -1.7123,  1.8197, -0.7708])\n",
      "9 tensor([ 8.9040, -1.4435, -0.3873, -0.1808,  0.9032])\n",
      "10 tensor([ 2.0552, -1.6113,  3.8797, -1.9013, -1.6329])\n",
      "11 tensor([ 1.6235,  0.1619, -0.7588, -3.4230, -6.2082])\n",
      "12 tensor([ 4.8970,  0.2624, -2.6085, -1.9068, -2.0620])\n",
      "13 tensor([-2.9151, -0.3895, -2.2679, -1.2034, -1.6059])\n",
      "14 tensor([-3.7825, -0.6049, -9.8044,  2.3338, -5.0351])\n",
      "15 tensor([ 1.7000,  2.5368, -4.1168, -4.4919, -0.7711])\n",
      "16 tensor([ 1.1474,  1.5393,  0.9041,  2.2031, -0.9126])\n",
      "17 tensor([ 4.0277, -0.1442,  0.5819,  3.2180, -5.4616])\n",
      "18 tensor([-0.2106, -3.3594, -2.6857,  6.8177, -5.8221])\n",
      "19 tensor([ 2.5888, -6.5787, -3.0450,  1.5596, -1.8963])\n",
      "20 tensor([ 2.2481, -2.8208,  1.6134, -0.7125,  2.7961])\n",
      "21 tensor([ 6.0905, -4.7881, -1.3195,  4.6589,  0.8198])\n",
      "22 tensor([-0.5725,  0.5344,  2.5931,  4.6706,  0.0925])\n",
      "23 tensor([ 9.0830, -3.1138, -0.4684,  3.3942, -1.3756])\n",
      "24 tensor([ 0.6170,  0.4810,  2.5691,  0.7587, -1.1889])\n",
      "25 tensor([-1.2021,  1.8928,  0.8081,  8.3900, -1.5993])\n",
      "26 tensor([-1.5245,  1.2594,  2.6713, -0.0947, -0.8553])\n",
      "27 tensor([ 2.7325,  3.6163,  4.4281,  0.3361, -0.9760])\n",
      "28 tensor([1.8804, 5.0951, 4.6466, 0.4769, 2.0052])\n",
      "29 tensor([-0.9357,  3.8197,  0.3862,  0.7017,  0.0134])\n",
      "30 tensor([ 8.3824,  0.3109, -1.1331,  0.1578,  0.9823])\n",
      "31 tensor([ 2.7720,  3.2861,  3.2097, -2.0097, -1.6488])\n",
      "32 tensor([ 1.5755, -1.5607, -2.1122, -2.6942, -5.5323])\n",
      "33 tensor([ 0.9194,  1.5559, -2.3504, -3.9481, -2.7145])\n",
      "34 tensor([ 1.8347,  1.7362, -4.1672, -3.2230, -2.7123])\n",
      "35 tensor([ 5.7773,  1.8100,  0.5828, -2.5095,  0.1236])\n",
      "36 tensor([-1.8334, -1.1024,  4.0868, -2.3308, -2.0697])\n",
      "37 tensor([ 3.5312, -3.3446,  5.4645, -0.7516,  0.8696])\n",
      "38 tensor([ 3.3995,  0.2939,  1.8359, -0.8811, -6.2093])\n",
      "39 tensor([ 6.3996,  5.2603,  3.9602, -0.2908, -0.1200])\n",
      "40 tensor([ 2.8548,  0.6559,  1.0180, -3.1927, -5.4116])\n",
      "41 tensor([-2.4768,  2.2868, -3.8734, -0.7574, -5.9650])\n",
      "42 tensor([-1.7676,  1.5163,  0.8029, -0.4212, -7.8125])\n",
      "43 tensor([-0.1007,  0.9746,  1.5104,  0.3310, -0.2563])\n",
      "44 tensor([ 2.9286,  0.4440, -1.3836,  2.0963, -1.1132])\n",
      "45 tensor([ 2.9670, -1.0772, -3.0482,  1.5954, -1.8978])\n",
      "46 tensor([ 2.7032, -1.7043, -3.2802,  1.3187, -1.1755])\n",
      "47 tensor([ 2.2692, -2.3005, -3.5625,  1.5467, -0.4978])\n",
      "48 tensor([ 1.8885, -2.4434, -3.6580,  1.5927, -0.4705])\n",
      "49 tensor([ 1.4629, -2.3587, -3.7161,  1.3040, -0.4402])\n",
      "50 tensor([ 1.1224, -2.1004, -3.6937,  0.8076, -0.7387])\n",
      "51 tensor([ 0.7969, -1.9385, -3.7377,  0.5307, -1.0629])\n",
      "52 tensor([ 0.5467, -2.1365, -3.7835,  0.5365, -1.1121])\n",
      "53 tensor([ 0.6354, -2.1849, -3.8013,  0.8615, -0.7797])\n",
      "54 tensor([ 0.7603, -2.0053, -3.6976,  1.2121, -0.6543])\n",
      "55 tensor([ 0.8848, -1.8440, -3.6351,  1.3607, -0.5535])\n",
      "56 tensor([ 0.8940, -1.5944, -3.6989,  1.3053, -0.5676])\n",
      "57 tensor([ 0.9728, -1.5071, -3.6404,  1.2094, -0.7603])\n",
      "58 tensor([ 0.9811, -1.5734, -3.7535,  1.1985, -0.8844])\n",
      "59 tensor([ 1.1530, -1.9117, -3.8170,  1.3171, -0.7766])\n",
      "60 tensor([ 1.4167, -2.1062, -4.0525,  1.4706, -0.3704])\n",
      "61 tensor([ 1.4107, -2.0102, -3.9172,  1.5314, -0.2293])\n",
      "62 tensor([ 1.4113, -1.8304, -3.7626,  1.3379, -0.2012])\n",
      "63 tensor([ 1.2373, -1.6517, -3.6371,  1.3355, -0.3864])\n",
      "64 tensor([ 1.1366, -1.7438, -3.5329,  1.6470, -0.3131])\n",
      "65 tensor([ 1.3453, -1.9140, -3.4322,  2.2307,  0.1152])\n",
      "66 tensor([ 1.5077, -1.7530, -3.2842,  2.6498,  0.3987])\n",
      "67 tensor([ 1.7372, -1.5947, -3.2604,  2.9293,  0.6421])\n",
      "68 tensor([ 1.7550, -1.4048, -3.2532,  2.8736,  0.8136])\n",
      "69 tensor([ 1.7378, -1.3194, -3.3120,  2.7491,  0.7417])\n",
      "70 tensor([ 1.6221, -1.3877, -3.5335,  2.6106,  0.7080])\n",
      "71 tensor([ 1.5630, -1.7591, -3.7356,  2.6351,  0.8344])\n",
      "72 tensor([ 1.6032, -2.1706, -4.0193,  2.7314,  1.2509])\n",
      "73 tensor([ 1.7255, -2.1778, -4.2634,  2.6259,  1.5769])\n",
      "74 tensor([ 1.5471, -2.1527, -4.3192,  2.3274,  1.7619])\n",
      "75 tensor([ 1.2830, -1.8831, -4.2742,  1.8010,  1.5891])\n",
      "76 tensor([ 0.9716, -1.7451, -4.2039,  1.4705,  1.1381])\n",
      "77 tensor([ 0.6891, -1.8405, -4.1311,  1.3559,  1.0028])\n",
      "78 tensor([ 0.6636, -1.9289, -3.9817,  1.4535,  1.1572])\n",
      "79 tensor([ 0.8173, -1.7390, -3.8317,  1.6232,  1.3355])\n",
      "80 tensor([ 0.7183, -1.4270, -3.6484,  1.5635,  1.3151])\n",
      "81 tensor([ 0.7856, -1.0411, -3.4566,  1.3792,  1.3184])\n",
      "82 tensor([ 0.4019, -0.7057, -3.2981,  1.0166,  0.9775])\n",
      "83 tensor([ 0.2642, -0.5356, -3.1332,  0.7644,  0.6308])\n",
      "84 tensor([ 0.2334, -0.5639, -2.9704,  0.6986,  0.4381])\n",
      "85 tensor([ 0.3291, -0.5640, -2.6851,  0.8628,  0.5529])\n",
      "86 tensor([ 0.4700, -0.4204, -2.4322,  1.1935,  0.7177])\n",
      "87 tensor([ 0.6361, -0.1838, -2.0667,  1.3674,  0.7467])\n",
      "88 tensor([ 0.8577,  0.0627, -1.9411,  1.4483,  0.6850])\n",
      "89 tensor([ 0.7679,  0.3394, -1.8739,  1.2958,  0.6040])\n",
      "90 tensor([ 0.6678,  0.4950, -1.7530,  1.1738,  0.3126])\n",
      "91 tensor([ 0.4811,  0.3971, -1.7924,  1.0598,  0.0599])\n",
      "92 tensor([ 0.4148,  0.0597, -1.8197,  1.1087,  0.0724])\n",
      "93 tensor([ 0.3048, -0.2592, -2.1305,  1.1935,  0.3942])\n",
      "94 tensor([ 0.4179, -0.3419, -2.3461,  1.2739,  0.6605])\n",
      "95 tensor([ 0.3564, -0.2949, -2.4541,  1.1681,  0.6633])\n",
      "96 tensor([ 0.0481, -0.1624, -2.5553,  0.7579,  0.4795])\n",
      "97 tensor([-0.2349, -0.0773, -2.5363,  0.3159,  0.1043])\n",
      "98 tensor([-0.5612, -0.1294, -2.5083,  0.1856, -0.3841])\n",
      "99 tensor([-0.7005, -0.2743, -2.4587,  0.2192, -0.4629])\n",
      "100 tensor([-0.5171, -0.2570, -2.3502,  0.4152, -0.3500])\n",
      "101 tensor([-0.4066, -0.0481, -2.0372,  0.7103, -0.4367])\n",
      "102 tensor([-0.2325,  0.1727, -1.8078,  0.8091, -0.5778])\n",
      "103 tensor([-0.2229,  0.5992, -1.6324,  0.8205, -0.7753])\n",
      "104 tensor([-0.3187,  0.8411, -1.3512,  0.7619, -1.1687])\n",
      "105 tensor([-0.2974,  0.9298, -1.1278,  0.9054, -1.5593])\n",
      "106 tensor([-0.0901,  0.7925, -1.1699,  1.3486, -1.4756])\n",
      "107 tensor([ 0.0246,  0.6346, -1.3134,  1.9073, -1.0183])\n",
      "108 tensor([ 0.2627,  0.5355, -1.2569,  2.2841, -0.7858])\n",
      "109 tensor([ 0.5025,  0.5432, -1.3565,  2.4660, -0.6649])\n",
      "110 tensor([ 0.3548,  0.5313, -1.5208,  2.5272, -0.5915])\n",
      "111 tensor([ 0.2097,  0.5415, -1.6942,  2.4259, -0.8032])\n",
      "112 tensor([-0.0214,  0.3779, -1.8795,  2.2530, -0.9866])\n",
      "113 tensor([-0.1430,  0.0464, -2.1131,  2.2787, -1.0660])\n",
      "114 tensor([-0.1142, -0.2777, -2.2856,  2.4491, -0.8580])\n",
      "115 tensor([ 0.0795, -0.3989, -2.3666,  2.7047, -0.6799])\n",
      "116 tensor([ 0.1977, -0.3362, -2.4189,  2.8261, -0.6171])\n",
      "117 tensor([ 0.3541, -0.3015, -2.5367,  2.8833, -0.5301])\n",
      "118 tensor([ 0.3575, -0.0429, -2.4747,  2.7746, -0.5834])\n",
      "119 tensor([ 0.3758,  0.0716, -2.4162,  2.6062, -0.8309])\n",
      "120 tensor([ 0.3272,  0.0377, -2.4483,  2.5304, -1.0349])\n",
      "121 tensor([ 0.4177, -0.0881, -2.4431,  2.6478, -1.0246])\n",
      "122 tensor([ 0.7699, -0.2908, -2.3933,  2.9138, -0.7031])\n",
      "123 tensor([ 1.1626, -0.3423, -2.4451,  3.2766, -0.2198])\n",
      "124 tensor([ 1.4046, -0.3609, -2.3716,  3.4944, -0.0372])\n",
      "125 tensor([ 1.7828, -0.3937, -2.3357,  3.5037,  0.0252])\n",
      "126 tensor([ 1.4087, -0.2883, -2.4718,  2.9723, -0.0791])\n",
      "127 tensor([ 1.5768, -0.4911, -2.5987,  3.1411, -0.3636])\n",
      "128 tensor([ 0.9898, -0.3184, -2.6420,  3.0693,  0.0055])\n",
      "129 tensor([ 1.2763, -0.7349, -2.5154,  3.2885, -0.1199])\n",
      "130 tensor([ 1.5122, -1.0158, -2.8649,  3.3911,  0.2280])\n",
      "131 tensor([ 1.7978, -0.9892, -2.9572,  3.3828,  0.5295])\n",
      "132 tensor([ 1.8386, -0.9182, -3.0569,  3.3129,  0.6257])\n",
      "133 tensor([ 1.9189, -0.7851, -3.0788,  3.1514,  0.5620])\n",
      "134 tensor([ 1.8422, -0.3951, -2.9779,  2.8014,  0.4026])\n",
      "135 tensor([ 1.7284, -0.2872, -2.9518,  2.3968,  0.1684])\n",
      "136 tensor([ 1.5987, -0.3557, -2.9651,  2.1925,  0.0746])\n",
      "137 tensor([ 1.7948, -0.5286, -2.9361,  2.1506,  0.2104])\n",
      "138 tensor([ 1.9807, -0.5957, -2.7156,  2.2689,  0.5619])\n",
      "139 tensor([ 2.2055, -0.4541, -2.6016,  2.3263,  0.7022])\n",
      "140 tensor([ 2.3465, -0.2589, -2.3723,  2.2846,  0.7468])\n",
      "141 tensor([ 2.4166,  0.0985, -2.2412,  2.0464,  0.6445])\n",
      "142 tensor([ 2.2442,  0.3154, -2.0654,  1.7942,  0.4796])\n",
      "143 tensor([ 2.2010,  0.3070, -1.9955,  1.7050,  0.2871])\n",
      "144 tensor([ 2.2407,  0.0758, -2.0940,  1.8766,  0.5197])\n",
      "145 tensor([ 2.2855, -0.1817, -2.0215,  2.1326,  0.8937])\n",
      "146 tensor([ 2.3699, -0.1353, -1.9579,  2.1808,  1.1559])\n",
      "147 tensor([ 2.2839, -0.0643, -1.9547,  2.1149,  1.3680])\n",
      "148 tensor([ 2.1268,  0.1941, -1.9228,  1.8371,  1.0718])\n",
      "149 tensor([ 2.0612,  0.2777, -1.9953,  1.6443,  0.7474])\n"
     ]
    }
   ],
   "source": [
    "for i, vec in enumerate(token_vecs_sum):\n",
    "    print(i, vec[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests mit Pipelines\n",
    "- feature-extraction: Generates a tensor representation for the input sequence\n",
    "- ner: Generates named entity mapping for each word in the input sequence.\n",
    "- sentiment-analysis: Gives the polarity (positive / negative) of the whole input sequence.\n",
    "- text-classification: Initialize a TextClassificationPipeline directly, or see sentiment-analysis for an example.\n",
    "- question-answering: Provided some context and a question refering to the context, it will extract the answer to the question in the context.\n",
    "- fill-mask: Takes an input sequence containing a masked token (e.g. <mask>) and return list of most probable filled sequences, with their probabilities.\n",
    "- summarization\n",
    "- translation_xx_to_yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# This pipeline extracts the hidden states from the base transformer,\n",
    "# which can be used as features in downstream tasks.\n",
    "pipe = pipeline('feature-extraction', model=model, tokenizer=tokenizer)\n",
    "sentence = \"\"\"We study the applicability and potential of the algorithm to learn representations of varying depth in a handful of applications and domains  , highlighting the ability of the algorithm to provide discriminative feature representations that are able to achieve top performance. We present a hyper-parameter free, off-the-shelf, simple and fast unsupervised algorithm to discover hidden structure from the input data by enforcing a very strong form of sparsity.\"\"\"\n",
    "out = pipe(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1\n",
      "Tokens: 82\n",
      "Dimensions: 768\n",
      "Words: 69\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences:\", len(out))\n",
    "print(\"Tokens:\", len(out[0]))\n",
    "print(\"Dimensions:\", len(out[0][0]))\n",
    "print(\"Words:\", len(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6337162256240845, 0.28518983721733093, 0.016355250030755997, -0.3005073666572571, -0.7662723064422607]\n",
      "[-0.2967832088470459, -1.0541183948516846, -1.8105967044830322, 0.5548749566078186, -0.4306401312351227]\n"
     ]
    }
   ],
   "source": [
    "print(out[0][0][:5])\n",
    "print(out[0][1][:5])\n",
    "# lul ist das einfach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  102,   185,   527,   111, 13214,   137,  1411,   131,   111,  1172,\n",
       "           147,  6714,  6859,   131,  5543,  3826,   121,   106,  1500,  1004,\n",
       "           131,  2040,   137,  4371,   422, 18579,   111,  2495,   131,   111,\n",
       "          1172,   147,  1584, 20900,  2602,  6859,   198,   220,  2357,   147,\n",
       "          3120,  1623,  1150,   205,   185,   709,   106,  1884,   579,  2318,\n",
       "          2159,   422,  1874,   579,   111,   579, 20103,   422,  2177,   137,\n",
       "          3254, 18391,  1172,   147,  9819,  8033,  1187,   263,   111,  1653,\n",
       "           453,   214, 15783,  7020,   106,  1248,  1648,   592,   131, 21123,\n",
       "           205,   103,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 150,          # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"enforcing\"\n",
    "tokens = tokenizer.tokenize(entity)\n",
    "#tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = encoded_dict['input_ids'][0].numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(input_ids) - len(ids) + 1):\n",
    "    if input_ids[i:i+len(ids)] == ids:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n",
      "SDR\n",
      "Discrete Fourier Transform\n",
      "MNIST\n",
      "ACA\n",
      "UCI Machine\n",
      "Bloom\n",
      "KNN\n",
      "Scopus\n",
      "Optimization\n",
      "Gregex\n",
      "Cubature Kalman\n",
      "Sciences\n",
      "ArcFace\n",
      "CMOS\n",
      "Hybrid Hierarchical Agglomerative Clustering\n",
      "PBtree\n",
      "Minimum Neighborhood Rough Sets\n",
      "VBNMF\n",
      "GIS\n",
      "LGAE\n",
      "Oracle10g\n",
      "DFS\n",
      "WSN\n",
      "NLMS\n",
      "LSH\n",
      "ERD\n",
      "MatchPy\n",
      "DCASE2016\n",
      "TWDA\n",
      "Laguerre\n",
      "Image Retrieval CBIR\n",
      "Cloud\n",
      "MHPCs\n",
      "Jenkins\n",
      "SCEBSS\n",
      "Text\n",
      "IMT\n",
      "SFCscore\n",
      "IIR\n",
      "autoencoder\n",
      "Ontologies\n",
      "LFP\n",
      "DNN\n",
      "Stochastic Meta\n",
      "Radial Basis Function\n",
      "EWF\n",
      "DSP\n",
      "Suffix\n",
      "Universitas\n",
      "PCC\n",
      "Monte Carlo\n",
      "WEC\n",
      "AdaGrad\n",
      "Softmax Classification\n",
      "Pairwise Latent Dirichlet Allocation\n",
      "ASR\n",
      "University\n",
      "Java\n",
      "BOW\n",
      "SPSS\n",
      "Softmax\n",
      "Developed\n",
      "Partial\n",
      "FCDA\n",
      "MTRF\n",
      "Regularized Deep Autoencoder\n",
      "SISO\n",
      "Exponentiated Gumbel\n",
      "Hybrid Extreme Random Forest\n",
      "Error\n",
      "SBADF\n",
      "LVQ\n",
      "Architecture Analysis\n",
      "NEOCIVET\n",
      "EPSO\n",
      "adaptive filter\n",
      "PSF\n",
      "Marginal Loss\n",
      "MIML\n",
      "ISA\n",
      "MEA\n",
      "Wiener\n",
      "Signal\n",
      "LIB\n",
      "Deep Neural Networks DNN\n",
      "FLWOR\n",
      "Support Vector\n",
      "Support Vector Machines\n",
      "CELP\n",
      "Ant Colony Optimization\n",
      "Artificial Bee Algorithm\n",
      "DCT\n",
      "Polyak\n",
      "Long\n",
      "Compared\n",
      "ELM\n",
      "HGP\n",
      "Microsoft\n",
      "IQMs\n",
      "RFID\n",
      "RSTRARelay Selection\n",
      "Slim\n",
      "Spherical Hashing\n",
      "SQA\n",
      "Chap\n",
      "SVMPF\n",
      "Improved Pearson Correlation\n",
      "MMSE\n",
      "sLDA\n",
      "PCA\n",
      "Boltzmann Machine\n",
      "SDD\n",
      "RMSC\n",
      "LDA\n",
      "WLAN\n",
      "XOR Hashing\n",
      "Meta Heuristic\n",
      "hierarchical clustering\n",
      "softmax\n",
      "NJW\n",
      "Consistency SVRC\n",
      "Adaptive Filtering\n",
      "BRISK\n",
      "Particle\n",
      "Random Search\n",
      "pattern matching\n",
      "PSMF\n",
      "BNDMq\n",
      "J48\n",
      "DNNs\n",
      "Bootstrapped Dendritic\n",
      "Maximum Likelihood Estimates\n",
      "AASC\n",
      "selection algorithm\n",
      "Variational Bayesian Inference VBI\n",
      "EEG\n",
      "RRI\n",
      "UBIRISv1\n",
      "Bipartite Graph Bigraph\n",
      "XQuery\n",
      "Regular Expression Functions\n",
      "CDMA\n",
      "Kernel Ridge Regression\n",
      "VWV\n",
      "Bagging\n",
      "Abstract Approximate Nearest Neighbor\n",
      "Semantic Web\n",
      "Vietnamese\n",
      "RBFN\n",
      "random indexing\n",
      "JAR\n",
      "Automata\n",
      "INSs\n",
      "SSSC\n",
      "Basic\n",
      "Markov Chain Monte Carlo\n",
      "Parallel Tempering\n",
      "Syntax\n",
      "UMQA\n",
      "SOMs\n",
      "Deep Neural Networks\n",
      "Bootstrap\n",
      "Latent Semantic Analysis\n",
      "BNDM\n",
      "Bayesian Network\n",
      "Latent Semantic Analysis Pentti Kanerva Jan Kristoferson Anders Holst\n",
      "Manual\n",
      "Contractive Autoencoders\n",
      "PageRank\n",
      "FSIM\n",
      "Simplified Chinese\n",
      "Traits\n",
      "predictive modeling\n",
      "Object\n",
      "Maryland\n",
      "Aspect Embedded\n",
      "MCMC RJMCMC\n",
      "FGT\n",
      "BRIEF\n",
      "IBK\n",
      "transformA\n",
      "DCAE\n",
      "Euclidian\n",
      "Compression\n",
      "MCMC SR\n",
      "Haar\n",
      "AdaBoost\n",
      "XML\n",
      "Angular Softmax\n",
      "AGS\n",
      "Markov\n",
      "GTX260 GPU\n",
      "DMC\n",
      "BPE\n",
      "PIP\n",
      "KerBS\n",
      "PSO\n",
      "MRD\n",
      "PID\n",
      "RBF\n",
      "Bootstrap Method\n",
      "HDD\n",
      "Search Engines\n",
      "Radiology\n",
      "BBC\n",
      "TDRBF\n",
      "WBTFSC\n",
      "Rayleigh\n",
      "CNNs\n",
      "Cartesian\n",
      "Stochastic\n",
      "Support Vector Regression Machine\n",
      "Deep Belief Network\n",
      "SGD\n",
      "REPTree\n",
      "PART\n",
      "RDS\n",
      "Enhanced Adaptive Bayesian Spam Filter\n",
      "FOFS\n",
      "BNSA\n",
      "RaPiD7 Rapid Production\n",
      "Grid Search\n",
      "ARX\n",
      "MLR\n",
      "MSSQL2005\n",
      "Nesterov\n",
      "Synthesis\n",
      "Static\n",
      "SRL\n",
      "Bayesian Spam Filter\n",
      "NPM\n",
      "Fourier Domain Optical Coherence Tomography\n",
      "HAC\n",
      "Light Adaptive Bayesian Spam Filter\n",
      "MDBUTMF\n",
      "Abstrak Telah\n",
      "Modified Differential Evolution MDE\n",
      "Modern\n",
      "EL\n",
      "Variance\n",
      "Pitch\n",
      "Pattern\n",
      "Urdu\n",
      "Lee\n",
      "SRAM\n",
      "Random Boost\n",
      "fMRI\n",
      "HeteRecom\n",
      "English\n",
      "Random Indexing\n",
      "support vector machine\n",
      "ARMA\n",
      "Locality Sensitive Hashing LSH\n",
      "SVR\n",
      "Fast Fourier\n",
      "Components Regression\n",
      "KFD\n",
      "query optimization\n",
      "Toward\n",
      "ERM\n",
      "Hidden Markov Model\n",
      "LapRLS\n",
      "BRUJA\n",
      "CRF\n",
      "restricted boltzmann machine\n",
      "IMMSC\n",
      "Latent Dirichlet Allocation LDA\n",
      "Relief\n",
      "Support\n",
      "SVMR\n",
      "Bhattacharyya\n",
      "HCSD\n",
      "Gaussian Process Bayesian Nonparametric\n",
      "GloVes\n",
      "GTZAN\n",
      "ECC\n",
      "Euclidean\n",
      "SPARQL\n",
      "MCSA Motor\n",
      "Pearson Correlation\n",
      "Mangasarian\n",
      "Saudi Arabian\n",
      "Feature\n",
      "Semantic Indexing\n",
      "TSVMTactic Support Vector Machine\n",
      "random search\n",
      "Text Samples\n",
      "Kennedy\n",
      "LITIS Rouen\n",
      "Spectral Clustering\n",
      "Zermelo\n",
      "Xilinx\n",
      "Feature Selection\n",
      "Instagram\n",
      "Smart Grids\n",
      "Ncut\n",
      "MEAN\n",
      "RLEP\n",
      "lib\n",
      "RZF\n",
      "RI\n",
      "RBM\n",
      "Weka\n",
      "Random Field\n",
      "Replicated Softmax\n",
      "Online\n",
      "PolSAR\n",
      "Discrete Fourier\n",
      "Martin\n",
      "RDF\n",
      "AUC\n",
      "MC68000\n",
      "Dynamically Growing\n",
      "AEVB\n",
      "EHR\n",
      "VAE\n",
      "Distress Analysis Interview\n",
      "Method\n",
      "Locally Consistent Parsing\n",
      "Deep Convolutional Neural Networks\n",
      "DFA\n",
      "MIMO\n",
      "Small Crush\n",
      "OQPSK\n",
      "SLU\n",
      "Recursive\n",
      "Design\n",
      "Petri Nets\n",
      "GRF\n",
      "FBADF\n",
      "Genetic\n",
      "Latent Aspect Rating Regression\n",
      "DMP\n",
      "Semantic Role\n",
      "SAGA\n",
      "Laplace\n",
      "KMeans\n",
      "LMS\n",
      "DBA\n",
      "FGN\n",
      "Random Binary Search\n",
      "Bernoulli\n",
      "MTSA\n",
      "Hybrid\n",
      "Simulation\n",
      "RNA\n",
      "MIB\n",
      "Hierarchical Clustering\n",
      "PRF\n",
      "Short Time Fourier Transform STFT\n",
      "Pearson Correlation Coefficients\n",
      "Reservoir Forecasting\n",
      "LiB\n",
      "Kalman\n",
      "Learned Image Compression\n",
      "Hangeul\n",
      "LDPC\n",
      "regular expression\n",
      "VBEM\n",
      "Doppler\n",
      "Widrow\n",
      "CGF\n",
      "Restricted Boltzmann\n",
      "HACA\n",
      "FNR\n",
      "RNAs\n",
      "MDCNN\n",
      "Recurrent Neural Networks\n",
      "FSMs\n",
      "VNF\n",
      "Dialog\n",
      "CDNSA\n",
      "POS\n",
      "Hierarchical\n",
      "LEAST\n",
      "Radial Basis Function Neural Network\n",
      "DSTC2\n",
      "Gaussian\n",
      "SVNSs\n",
      "DTW\n",
      "Cross\n",
      "CRP\n",
      "Likelihood Function\n",
      "Logit Boost\n",
      "Mixture Model DPMM\n",
      "Load\n",
      "Low Frequency Fluctuations\n",
      "Givens\n",
      "WebKB4\n",
      "QALD\n",
      "AES\n",
      "Advent\n",
      "Laser Range\n",
      "CNN\n",
      "SVSS\n",
      "Ackermann\n",
      "ABC\n",
      "XMLeXtensible Markup\n",
      "CFFT\n",
      "Naives Bayes\n",
      "INDEX\n",
      "CONSTITUTION\n",
      "MATLAB\n",
      "CPA\n",
      "KeywordPrediction\n",
      "REDFA\n",
      "OOV\n",
      "GLUE\n",
      "Pirkola\n",
      "Guided Regularized Random Forest\n",
      "fast fourier\n",
      "Pattern Matching\n",
      "Gabor\n",
      "PSVMParallel Support Vector Machinederived\n",
      "hierarchical agglomerative\n",
      "MLP\n",
      "Laplacian\n",
      "Traditional Sampling Algorithm\n",
      "Flags Properties\n",
      "Indonesia\n",
      "Third\n",
      "Menlo Park\n",
      "ReDoS\n",
      "ECG\n",
      "LAN\n",
      "Hierarchical Agglomerative Clustering\n",
      "Moldability\n",
      "Mahalanobis Taguchi System\n",
      "ML\n",
      "Autoencoded Variational Inference For Topic Model AVITM\n",
      "ROI\n",
      "Arabic\n",
      "TWSVM\n",
      "IR4QA\n",
      "CFD\n",
      "VIF\n",
      "Hilbert\n",
      "Sweden\n",
      "RDQCA\n",
      "Systems Analysis\n",
      "SCALCE\n",
      "Java Android\n",
      "Prefix Indexing\n",
      "JeromeDL\n",
      "Content\n",
      "I2P\n",
      "French\n",
      "Gaussian Cox\n",
      "SVD\n",
      "Gibbs\n",
      "VBI\n",
      "Simple Random\n",
      "LBA\n",
      "Monash University\n",
      "GWO\n",
      "FDCT\n",
      "Steepest Ascent Descent\n",
      "Reflective Random Indexing\n",
      "Partial Least Square\n",
      "SBQL\n",
      "cryoDRGN\n",
      "NAFSM\n",
      "rPPG\n",
      "NASA93\n",
      "Total\n",
      "Ristretto\n",
      "CHiME3\n",
      "IMAE\n",
      "QM\n",
      "Deep\n",
      "FFTsFast Fourier Transformers\n",
      "RSM\n",
      "TDNN\n",
      "ARM University Program\n",
      "TreeLogit\n",
      "Word\n",
      "HMM\n",
      "VESSL\n",
      "Tabu Search Algorithm\n",
      "Layerwise Interweaving Convolutional\n",
      "TREs\n",
      "OFDM\n",
      "MCMC\n",
      "Algorithms\n",
      "HCM\n",
      "Shingle\n",
      "Immune\n",
      "RPSODE\n",
      "Deep Autoencoder\n",
      "Hierarchical Adaptive Clustering HAC\n",
      "Accuracy\n",
      "ARM\n",
      "Restricted Boltzmann Machine\n",
      "MGRBM\n",
      "Transfer\n",
      "Novelty\n",
      "TSA Taboo Search Algorithm\n",
      "SSD\n",
      "Random Binary\n",
      "Interlingua English\n",
      "MAlg\n",
      "Image\n",
      "RDF Data Cubes\n",
      "BWE\n",
      "Connectivity\n",
      "Neural Networks\n",
      "C2LSH\n",
      "recursive function\n",
      "Design Language AADL\n",
      "spectral clustering\n",
      "CAE\n",
      "Random Forest\n",
      "QoS\n",
      "UGM\n",
      "Extreme Learning Machines ELM\n",
      "Random Forests\n",
      "Title\n",
      "Data\n",
      "IFSs\n",
      "LDA Latent Dirichlet Allocation\n",
      "Mandarin\n",
      "Wavelet\n",
      "SVMs\n",
      "WiBro\n",
      "Long Short Term Network\n",
      "HACA Han\n",
      "Throughput\n",
      "ADC\n",
      "Receiver\n",
      "Speech\n",
      "JobType FJT\n",
      "Lipschitz\n",
      "DST\n",
      "Kneser Ney\n",
      "Pattern Characteristics\n",
      "rMRF\n",
      "SkyNet\n",
      "UEP\n",
      "Monte Carlo MCMC\n",
      "Cost\n",
      "Random\n",
      "Turkish\n",
      "RAM\n",
      "GCNSI Graph Convolutional Networks\n",
      "FCM\n",
      "Cross Language Information Retrieval\n",
      "Reuters\n",
      "Particle Swarm\n",
      "GRE\n",
      "TILT\n",
      "DBN\n",
      "Boltzmann\n",
      "LEACH Low\n",
      "REM\n",
      "Xinjiang\n",
      "word embedding\n",
      "Data Presentation\n",
      "Hanja\n",
      "Fourier\n",
      "Resource Allocation\n",
      "UMQL\n",
      "Dynamic Movement Primitives\n",
      "RDCQA\n",
      "Permasalahan\n",
      "Affinity\n",
      "Teknik\n",
      "Convolutional Neural Network CNN\n",
      "Channel\n",
      "DFAs\n",
      "Hailstorm\n",
      "DNA\n",
      "mcmc\n",
      "Feature Cluster Grow\n",
      "Keywords\n",
      "Belief Propagation\n",
      "Cuckoo Search\n",
      "RWCP Theoretical Foundation\n",
      "SVM\n",
      "LIBLINEAR\n",
      "Exponentiated\n",
      "Pairwise Conditional Random Forest\n",
      "RFFT\n",
      "DynaMAD\n",
      "ID3\n",
      "Source Identification\n",
      "Stability\n",
      "Clustered\n",
      "Attribute Weighting Method\n",
      "Locality Sensitive Hashing\n",
      "Query Analyzer\n",
      "Log4J\n",
      "InfoMax Autoencoder IMAE\n",
      "Chinese\n",
      "Support Vector Machine Support Vector Machine\n",
      "Modified Random Forest\n",
      "Random Indexing RI\n",
      "DBNs\n",
      "Adam\n",
      "Weighted Random Forest\n",
      "Abstract\n",
      "Relations Frequency\n",
      "Germanic\n",
      "NRCF\n",
      "Pearson\n",
      "Enhanced Genetic Algorithm EGA\n",
      "CORDIC\n",
      "mulRBM\n",
      "Random Rotation Forest\n",
      "Genetic Algorithm\n",
      "OP3\n",
      "MRF\n",
      "Restricted Boltzmann Machine RBM\n",
      "LCCA\n",
      "Long Short Term\n",
      "Korean\n",
      "Hierarchical Pitman Yor Process Language Model\n",
      "Perceptual Linear\n",
      "SoC\n",
      "NOPs\n",
      "Radial Basis Function Network\n",
      "DBpedia\n",
      "PREREQ\n",
      "Direct Relation Frequency\n",
      "Berkeley Segmentation Dataset\n",
      "SOM\n",
      "SSL\n",
      "FFT\n",
      "DCNN\n",
      "IVIFSs\n",
      "Extreme Learning Machines Autoencoder\n",
      "Daphnee Rentfrow\n",
      "Machine\n",
      "Afterward\n",
      "Hal\n",
      "Alamouti\n",
      "SSIM\n",
      "SMF\n",
      "Turbo\n",
      "CD\n",
      "Latent Dirichlet Allocation\n",
      "BBFNN\n",
      "autoencoderVAE\n",
      "Network\n",
      "Delta State University\n",
      "Biogeography\n",
      "Rotation Forest\n",
      "Arduino\n",
      "SMC\n",
      "GRBM\n",
      "PSNR\n",
      "Bayesian\n",
      "CFPRF\n",
      "MDGVRPTW\n",
      "OpenBUGS\n",
      "Boruta Feature\n",
      "Convolutional Neural Network\n",
      "POP\n",
      "BPDF\n",
      "IRT\n",
      "CPU\n",
      "Snort\n",
      "Restricted Boltzmann Machines\n",
      "Relevance\n",
      "Kim\n",
      "Abstract Experimental\n",
      "Stanford University\n",
      "Particle Swarm Optimization PSO\n",
      "radial basis function network\n",
      "Particle Swarm Optimization\n",
      "Distributed Database Systems\n",
      "Expert\n",
      "likelihood function\n",
      "RKHSs\n",
      "Collaborative Filtering\n",
      "Hidden\n",
      "TreeNetreg\n",
      "Regional Homogeneity ReHo\n",
      "Extreme Learning Machine Autoencoder\n",
      "NOP\n",
      "Variational Bayesian\n",
      "RSVM\n",
      "CEAS\n",
      "Deep Neural Network\n",
      "Hopfield\n",
      "Golay\n",
      "Poisson\n",
      "ARS\n",
      "Cooley\n",
      "Mahony\n",
      "System Analysis\n",
      "Computer Science\n",
      "RG\n",
      "RMSProp\n",
      "Sound\n",
      "Google\n",
      "Siamese\n",
      "CRBM\n",
      "RBMs\n",
      "Cox\n",
      "Random Connectivity\n",
      "LASSO\n",
      "Original Research\n",
      "SPHC\n",
      "TREC\n",
      "DMOS\n",
      "LKB\n",
      "Dirichlet\n",
      "Linear\n",
      "Hybrid Radial Basis Kernel\n",
      "IFFT\n",
      "Hybrid Kernel Support Vector Machine\n",
      "Japanese\n",
      "Logistic\n",
      "Middlebury\n",
      "TNDM\n",
      "Context\n",
      "Island Code Transformation\n",
      "Virtex II\n",
      "Maximum Entropy Discriminant\n",
      "DBSCAN\n",
      "Android\n",
      "AUV\n",
      "UAVs\n",
      "Riemannian\n",
      "VSGML\n",
      "Second\n",
      "Structured Sparsity\n",
      "SVM LinSVM\n",
      "SQUARE\n",
      "Hierarchical Dirichlet Process\n",
      "Naive Bayes\n",
      "RPCA\n",
      "AFM\n",
      "Timed Regular Expression Mining\n",
      "AP\n",
      "stochastic gradient descent\n",
      "Cuckoo Search AlgorithmCSA\n",
      "Link Latent Dirichlet Allocation LinkLDA\n",
      "Hierarchical Machine Translation Model\n",
      "reinforcement learning\n",
      "OP2\n",
      "DBM\n",
      "semantic relevance\n",
      "DWT\n",
      "SAS Perl Regular Expression\n",
      "Hierarchical Clustering Agglomerative Hierarchical Clustering\n",
      "GDNSA\n",
      "Pearson Correlation Coefficient\n",
      "Normal Recovery Collaborative\n",
      "linear regression\n",
      "Nehorai\n",
      "Short\n",
      "Support Vector Machine\n",
      "Profit Sharing\n",
      "SMIB\n",
      "Radial\n",
      "SVGD\n",
      "ICI\n",
      "SpatDensReg\n",
      "MEM\n",
      "MCMC Niepert\n",
      "PICR\n",
      "TDCM\n",
      "random forest\n",
      "HRKSVM\n",
      "OWASP Core Rule Set\n",
      "Recurrent Neural Networks RNNs\n",
      "Dynamic\n",
      "Various\n",
      "UMH\n",
      "Minimum\n",
      "Box\n",
      "Instability\n",
      "Fast Fourier Transform\n",
      "User\n",
      "Lib\n",
      "Stochastic Gradient\n",
      "Performance Evaluation\n",
      "Generational Feature\n",
      "Variational Autoencoder\n",
      "Feature Elimination\n",
      "Cubature Kalman Filter\n",
      "MAB\n",
      "NVRAM\n",
      "Bayes\n",
      "Horn\n",
      "IR\n",
      "FPGA\n",
      "DLF\n",
      "LSI\n",
      "Radial Basis Functions\n",
      "Simhash\n",
      "basedonField Programmable\n",
      "Random Forest RF\n",
      "IDS\n",
      "HDN\n",
      "Volterra\n",
      "ANN\n",
      "Petroleum Engineering Department\n",
      "DFT\n",
      "Linear Regression\n",
      "LibSVM\n",
      "TSA\n",
      "Computer\n",
      "HSEG\n",
      "THE\n",
      "AMF\n"
     ]
    }
   ],
   "source": [
    "array = ['SDR', 'Discrete Fourier Transform', 'MNIST', 'ACA', 'UCI Machine', 'Bloom', 'KNN', 'Scopus', 'Optimization', 'Gregex', 'Cubature Kalman', 'Sciences', 'ArcFace', 'CMOS', 'Hybrid Hierarchical Agglomerative Clustering', 'PBtree', 'Minimum Neighborhood Rough Sets', 'VBNMF', 'GIS', 'LGAE', 'Oracle10g', 'DFS', 'WSN', 'NLMS', 'LSH', 'ERD', 'MatchPy', 'DCASE2016', 'TWDA', 'Laguerre', 'Image Retrieval CBIR', 'Cloud', 'MHPCs', 'Jenkins', 'SCEBSS', 'Text', 'IMT', 'SFCscore', 'IIR', 'autoencoder', 'Ontologies', 'LFP', 'DNN', 'Stochastic Meta', 'Radial Basis Function', 'EWF', 'DSP', 'Suffix', 'Universitas', 'PCC', 'Monte Carlo', 'WEC', 'AdaGrad', 'Softmax Classification', 'Pairwise Latent Dirichlet Allocation', 'ASR', 'University', 'Java', 'BOW', 'SPSS', 'Softmax', 'Developed', 'Partial', 'FCDA', 'MTRF', 'Regularized Deep Autoencoder', 'SISO', 'Exponentiated Gumbel', 'Hybrid Extreme Random Forest', 'Error', 'SBADF', 'LVQ', 'Architecture Analysis', 'NEOCIVET', 'EPSO', 'adaptive filter', 'PSF', 'Marginal Loss', 'MIML', 'ISA', 'MEA', 'Wiener', 'Signal', 'LIB', 'Deep Neural Networks DNN', 'FLWOR', 'Support Vector', 'Support Vector Machines', 'CELP', 'Ant Colony Optimization', 'Artificial Bee Algorithm', 'DCT', 'Polyak', 'Long', 'Compared', 'ELM', 'HGP', 'Microsoft', 'IQMs', 'RFID', 'RSTRARelay Selection', 'Slim', 'Spherical Hashing', 'SQA', 'Chap', 'SVMPF', 'Improved Pearson Correlation', 'MMSE', 'sLDA', 'PCA', 'Boltzmann Machine', 'SDD', 'RMSC', 'LDA', 'WLAN', 'XOR Hashing', 'Meta Heuristic', 'hierarchical clustering', 'softmax', 'NJW', 'Consistency SVRC', 'Adaptive Filtering', 'BRISK', 'Particle', 'Random Search', 'pattern matching', 'PSMF', 'BNDMq', 'J48', 'DNNs', 'Bootstrapped Dendritic', 'Maximum Likelihood Estimates', 'AASC', 'selection algorithm', 'Variational Bayesian Inference VBI', 'EEG', 'RRI', 'UBIRISv1', 'Bipartite Graph Bigraph', 'XQuery', 'Regular Expression Functions', 'CDMA', 'Kernel Ridge Regression', 'VWV', 'Bagging', 'Abstract Approximate Nearest Neighbor', 'Semantic Web', 'Vietnamese', 'RBFN', 'random indexing', 'JAR', 'Automata', 'INSs', 'SSSC', 'Basic', 'Markov Chain Monte Carlo', 'Parallel Tempering', 'Syntax', 'UMQA', 'SOMs', 'Deep Neural Networks', 'Bootstrap', 'Latent Semantic Analysis', 'BNDM', 'Bayesian Network', 'Latent Semantic Analysis Pentti Kanerva Jan Kristoferson Anders Holst', 'Manual', 'Contractive Autoencoders', 'PageRank', 'FSIM', 'Simplified Chinese', 'Traits', 'predictive modeling', 'Object', 'Maryland', 'Aspect Embedded', 'MCMC RJMCMC', 'FGT', 'BRIEF', 'IBK', 'transformA', 'DCAE', 'Euclidian', 'Compression', 'MCMC SR', 'Haar', 'AdaBoost', 'XML', 'Angular Softmax', 'AGS', 'Markov', 'GTX260 GPU', 'DMC', 'BPE', 'PIP', 'KerBS', 'PSO', 'MRD', 'PID', 'RBF', 'Bootstrap Method', 'HDD', 'Search Engines', 'Radiology', 'BBC', 'TDRBF', 'WBTFSC', 'Rayleigh', 'CNNs', 'Cartesian', 'Stochastic', 'Support Vector Regression Machine', 'Deep Belief Network', 'SGD', 'REPTree', 'PART', 'RDS', 'Enhanced Adaptive Bayesian Spam Filter', 'FOFS', 'BNSA', 'RaPiD7 Rapid Production', 'Grid Search', 'ARX', 'MLR', 'MSSQL2005', 'Nesterov', 'Synthesis', 'Static', 'SRL', 'Bayesian Spam Filter', 'NPM', 'Fourier Domain Optical Coherence Tomography', 'HAC', 'Light Adaptive Bayesian Spam Filter', 'MDBUTMF', 'Abstrak Telah', 'Modified Differential Evolution MDE', 'Modern', 'EL', 'Variance', 'Pitch', 'Pattern', 'Urdu', 'Lee', 'SRAM', 'Random Boost', 'fMRI', 'HeteRecom', 'English', 'Random Indexing', 'support vector machine', 'ARMA', 'Locality Sensitive Hashing LSH', 'SVR', 'Fast Fourier', 'Components Regression', 'KFD', 'query optimization', 'Toward', 'ERM', 'Hidden Markov Model', 'LapRLS', 'BRUJA', 'CRF', 'restricted boltzmann machine', 'IMMSC', 'Latent Dirichlet Allocation LDA', 'Relief', 'Support', 'SVMR', 'Bhattacharyya', 'HCSD', 'Gaussian Process Bayesian Nonparametric', 'GloVes', 'GTZAN', 'ECC', 'Euclidean', 'SPARQL', 'MCSA Motor', 'Pearson Correlation', 'Mangasarian', 'Saudi Arabian', 'Feature', 'Semantic Indexing', 'TSVMTactic Support Vector Machine', 'random search', 'Text Samples', 'Kennedy', 'LITIS Rouen', 'Spectral Clustering', 'Zermelo', 'Xilinx', 'Feature Selection', 'Instagram', 'Smart Grids', 'Ncut', 'MEAN', 'RLEP', 'lib', 'RZF', 'RI', 'RBM', 'Weka', 'Random Field', 'Replicated Softmax', 'Online', 'PolSAR', 'Discrete Fourier', 'Martin', 'RDF', 'AUC', 'MC68000', 'Dynamically Growing', 'AEVB', 'EHR', 'VAE', 'Distress Analysis Interview', 'Method', 'Locally Consistent Parsing', 'Deep Convolutional Neural Networks', 'DFA', 'MIMO', 'Small Crush', 'OQPSK', 'SLU', 'Recursive', 'Design', 'Petri Nets', 'GRF', 'FBADF', 'Genetic', 'Latent Aspect Rating Regression', 'DMP', 'Semantic Role', 'SAGA', 'Laplace', 'KMeans', 'LMS', 'DBA', 'FGN', 'Random Binary Search', 'Bernoulli', 'MTSA', 'Hybrid', 'Simulation', 'RNA', 'MIB', 'Hierarchical Clustering', 'PRF', 'Short Time Fourier Transform STFT', 'Pearson Correlation Coefficients', 'Reservoir Forecasting', 'LiB', 'Kalman', 'Learned Image Compression', 'Hangeul', 'LDPC', 'regular expression', 'VBEM', 'Doppler', 'Widrow', 'CGF', 'Restricted Boltzmann', 'HACA', 'FNR', 'RNAs', 'MDCNN', 'Recurrent Neural Networks', 'FSMs', 'VNF', 'Dialog', 'CDNSA', 'POS', 'Hierarchical', 'LEAST', 'Radial Basis Function Neural Network', 'DSTC2', 'Gaussian', 'SVNSs', 'DTW', 'Cross', 'CRP', 'Likelihood Function', 'Logit Boost', 'Mixture Model DPMM', 'Load', 'Low Frequency Fluctuations', 'Givens', 'WebKB4', 'QALD', 'AES', 'Advent', 'Laser Range', 'CNN', 'SVSS', 'Ackermann', 'ABC', 'XMLeXtensible Markup', 'CFFT', 'Naives Bayes', 'INDEX', 'CONSTITUTION', 'MATLAB', 'CPA', 'KeywordPrediction', 'REDFA', 'OOV', 'GLUE', 'Pirkola', 'Guided Regularized Random Forest', 'fast fourier', 'Pattern Matching', 'Gabor', 'PSVMParallel Support Vector Machinederived', 'hierarchical agglomerative', 'MLP', 'Laplacian', 'Traditional Sampling Algorithm', 'Flags Properties', 'Indonesia', 'Third', 'Menlo Park', 'ReDoS', 'ECG', 'LAN', 'Hierarchical Agglomerative Clustering', 'Moldability', 'Mahalanobis Taguchi System', 'ML', 'Autoencoded Variational Inference For Topic Model AVITM', 'ROI', 'Arabic', 'TWSVM', 'IR4QA', 'CFD', 'VIF', 'Hilbert', 'Sweden', 'RDQCA', 'Systems Analysis', 'SCALCE', 'Java Android', 'Prefix Indexing', 'JeromeDL', 'Content', 'I2P', 'French', 'Gaussian Cox', 'SVD', 'Gibbs', 'VBI', 'Simple Random', 'LBA', 'Monash University', 'GWO', 'FDCT', 'Steepest Ascent Descent', 'Reflective Random Indexing', 'Partial Least Square', 'SBQL', 'cryoDRGN', 'NAFSM', 'rPPG', 'NASA93', 'Total', 'Ristretto', 'CHiME3', 'IMAE', 'QM', 'Deep', 'FFTsFast Fourier Transformers', 'RSM', 'TDNN', 'ARM University Program', 'TreeLogit', 'Word', 'HMM', 'VESSL', 'Tabu Search Algorithm', 'Layerwise Interweaving Convolutional', 'TREs', 'OFDM', 'MCMC', 'Algorithms', 'HCM', 'Shingle', 'Immune', 'RPSODE', 'Deep Autoencoder', 'Hierarchical Adaptive Clustering HAC', 'Accuracy', 'ARM', 'Restricted Boltzmann Machine', 'MGRBM', 'Transfer', 'Novelty', 'TSA Taboo Search Algorithm', 'SSD', 'Random Binary', 'Interlingua English', 'MAlg', 'Image', 'RDF Data Cubes', 'BWE', 'Connectivity', 'Neural Networks', 'C2LSH', 'recursive function', 'Design Language AADL', 'spectral clustering', 'CAE', 'Random Forest', 'QoS', 'UGM', 'Extreme Learning Machines ELM', 'Random Forests', 'Title', 'Data', 'IFSs', 'LDA Latent Dirichlet Allocation', 'Mandarin', 'Wavelet', 'SVMs', 'WiBro', 'Long Short Term Network', 'HACA Han', 'Throughput', 'ADC', 'Receiver', 'Speech', 'JobType FJT', 'Lipschitz', 'DST', 'Kneser Ney', 'Pattern Characteristics', 'rMRF', 'SkyNet', 'UEP', 'Monte Carlo MCMC', 'Cost', 'Random', 'Turkish', 'RAM', 'GCNSI Graph Convolutional Networks', 'FCM', 'Cross Language Information Retrieval', 'Reuters', 'Particle Swarm', 'GRE', 'TILT', 'DBN', 'Boltzmann', 'LEACH Low', 'REM', 'Xinjiang', 'word embedding', 'Data Presentation', 'Hanja', 'Fourier', 'Resource Allocation', 'UMQL', 'Dynamic Movement Primitives', 'RDCQA', 'Permasalahan', 'Affinity', 'Teknik', 'Convolutional Neural Network CNN', 'Channel', 'DFAs', 'Hailstorm', 'DNA', 'mcmc', 'Feature Cluster Grow', 'Keywords', 'Belief Propagation', 'Cuckoo Search', 'RWCP Theoretical Foundation', 'SVM', 'LIBLINEAR', 'Exponentiated', 'Pairwise Conditional Random Forest', 'RFFT', 'DynaMAD', 'ID3', 'Source Identification', 'Stability', 'Clustered', 'Attribute Weighting Method', 'Locality Sensitive Hashing', 'Query Analyzer', 'Log4J', 'InfoMax Autoencoder IMAE', 'Chinese', 'Support Vector Machine Support Vector Machine', 'Modified Random Forest', 'Random Indexing RI', 'DBNs', 'Adam', 'Weighted Random Forest', 'Abstract', 'Relations Frequency', 'Germanic', 'NRCF', 'Pearson', 'Enhanced Genetic Algorithm EGA', 'CORDIC', 'mulRBM', 'Random Rotation Forest', 'Genetic Algorithm', 'OP3', 'MRF', 'Restricted Boltzmann Machine RBM', 'LCCA', 'Long Short Term', 'Korean', 'Hierarchical Pitman Yor Process Language Model', 'Perceptual Linear', 'SoC', 'NOPs', 'Radial Basis Function Network', 'DBpedia', 'PREREQ', 'Direct Relation Frequency', 'Berkeley Segmentation Dataset', 'SOM', 'SSL', 'FFT', 'DCNN', 'IVIFSs', 'Extreme Learning Machines Autoencoder', 'Daphnee Rentfrow', 'Machine', 'Afterward', 'Hal', 'Alamouti', 'SSIM', 'SMF', 'Turbo', 'CD', 'Latent Dirichlet Allocation', 'BBFNN', 'autoencoderVAE', 'Network', 'Delta State University', 'Biogeography', 'Rotation Forest', 'Arduino', 'SMC', 'GRBM', 'PSNR', 'Bayesian', 'CFPRF', 'MDGVRPTW', 'OpenBUGS', 'Boruta Feature', 'Convolutional Neural Network', 'POP', 'BPDF', 'IRT', 'CPU', 'Snort', 'Restricted Boltzmann Machines', 'Relevance', 'Kim', 'Abstract Experimental', 'Stanford University', 'Particle Swarm Optimization PSO', 'radial basis function network', 'Particle Swarm Optimization', 'Distributed Database Systems', 'Expert', 'likelihood function', 'RKHSs', 'Collaborative Filtering', 'Hidden', 'TreeNetreg', 'Regional Homogeneity ReHo', 'Extreme Learning Machine Autoencoder', 'NOP', 'Variational Bayesian', 'RSVM', 'CEAS', 'Deep Neural Network', 'Hopfield', 'Golay', 'Poisson', 'ARS', 'Cooley', 'Mahony', 'System Analysis', 'Computer Science', 'RG', 'RMSProp', 'Sound', 'Google', 'Siamese', 'CRBM', 'RBMs', 'Cox', 'Random Connectivity', 'LASSO', 'Original Research', 'SPHC', 'TREC', 'DMOS', 'LKB', 'Dirichlet', 'Linear', 'Hybrid Radial Basis Kernel', 'IFFT', 'Hybrid Kernel Support Vector Machine', 'Japanese', 'Logistic', 'Middlebury', 'TNDM', 'Context', 'Island Code Transformation', 'Virtex II', 'Maximum Entropy Discriminant', 'DBSCAN', 'Android', 'AUV', 'UAVs', 'Riemannian', 'VSGML', 'Second', 'Structured Sparsity', 'SVM LinSVM', 'SQUARE', 'Hierarchical Dirichlet Process', 'Naive Bayes', 'RPCA', 'AFM', 'Timed Regular Expression Mining', 'AP', 'stochastic gradient descent', 'Cuckoo Search AlgorithmCSA', 'Link Latent Dirichlet Allocation LinkLDA', 'Hierarchical Machine Translation Model', 'reinforcement learning', 'OP2', 'DBM', 'semantic relevance', 'DWT', 'SAS Perl Regular Expression', 'Hierarchical Clustering Agglomerative Hierarchical Clustering', 'GDNSA', 'Pearson Correlation Coefficient', 'Normal Recovery Collaborative', 'linear regression', 'Nehorai', 'Short', 'Support Vector Machine', 'Profit Sharing', 'SMIB', 'Radial', 'SVGD', 'ICI', 'SpatDensReg', 'MEM', 'MCMC Niepert', 'PICR', 'TDCM', 'random forest', 'HRKSVM', 'OWASP Core Rule Set', 'Recurrent Neural Networks RNNs', 'Dynamic', 'Various', 'UMH', 'Minimum', 'Box', 'Instability', 'Fast Fourier Transform', 'User', 'Lib', 'Stochastic Gradient', 'Performance Evaluation', 'Generational Feature', 'Variational Autoencoder', 'Feature Elimination', 'Cubature Kalman Filter', 'MAB', 'NVRAM', 'Bayes', 'Horn', 'IR', 'FPGA', 'DLF', 'LSI', 'Radial Basis Functions', 'Simhash', 'basedonField Programmable', 'Random Forest RF', 'IDS', 'HDN', 'Volterra', 'ANN', 'Petroleum Engineering Department', 'DFT', 'Linear Regression', 'LibSVM', 'TSA', 'Computer', 'HSEG', 'THE', 'AMF']\n",
    "print(len(array))\n",
    "for x in array:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "doc2vec_model = gensim.models.Doc2Vec.load('embedding_models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['combined', 'with', 'principal', 'component', 'analysis', 'the', 'total', 'training', 'for', 'ten', 'one', 'against', 'the', 'rest', 'classifiers', 'on', 'mnist', 'took', 'just', 'hours']\n",
      "[(38122746, 0.6723868250846863), (16402600, 0.6712477207183838), (39632846, 0.6673611402511597)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "tokens = simple_preprocess(\"Combined with principal component analysis, the total training for ten one against the-rest classifiers on MNIST took just 0.77 hours.\")\n",
    "print(tokens)\n",
    "new_vector = doc2vec_model.infer_vector(tokens)\n",
    "sims = doc2vec_model.docvecs.most_similar([new_vector], topn=3)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40139652"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2vec_model.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03852049, -0.0665906 , -0.04628057, -0.06231441, -0.03460921,\n",
       "       -0.03901168, -0.06382208,  0.08229337,  0.00271638,  0.01581776,\n",
       "        0.05019219,  0.1256509 , -0.03815013, -0.05365263, -0.05448654,\n",
       "       -0.01666151, -0.01677482, -0.05219691,  0.04517033,  0.1090557 ,\n",
       "        0.12321562,  0.08372267, -0.2060628 , -0.01822349, -0.07854325,\n",
       "        0.01112043,  0.07415384, -0.10915   ,  0.06760772,  0.04235146,\n",
       "       -0.04535627,  0.03000781, -0.01149387, -0.03809126, -0.06970099,\n",
       "        0.01360476,  0.03218273,  0.11050344,  0.00869558,  0.04056333,\n",
       "       -0.0830144 , -0.02412082, -0.03001807, -0.11603937,  0.01732241,\n",
       "        0.02701879, -0.01442215,  0.05145655, -0.06035341,  0.06090457,\n",
       "       -0.08889908,  0.00640001,  0.04755795,  0.06097076, -0.15226342,\n",
       "        0.04331081,  0.0846197 ,  0.12408043, -0.03757742, -0.05635827,\n",
       "        0.05589816, -0.02187142, -0.00418653, -0.04294994, -0.07213835,\n",
       "       -0.02162061,  0.08874757, -0.01664794,  0.04523556, -0.05043289,\n",
       "       -0.0434793 , -0.00770898, -0.03542288, -0.013023  , -0.04793676,\n",
       "        0.12843202, -0.05848786, -0.02473668, -0.04551063, -0.05332342,\n",
       "        0.05984464, -0.02035155, -0.06080852,  0.10877939, -0.0353205 ,\n",
       "       -0.10186801, -0.02051136,  0.0074191 ,  0.09098926, -0.0142522 ,\n",
       "       -0.0938247 ,  0.05977534, -0.05708978, -0.04112995, -0.04948505,\n",
       "       -0.07815571,  0.07158952, -0.07923175, -0.01435863,  0.04016923],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.docvecs[30431524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4584659 ,  1.242882  , -0.20461333, -0.13547745,  0.17811774,\n",
       "       -0.7919226 ,  0.16580945,  0.5773504 ,  0.27312937, -0.4790782 ,\n",
       "        0.66863424,  0.35208142,  0.19789323,  0.15972097, -0.07386668,\n",
       "       -0.46394625, -0.5921684 , -0.08119006, -0.11479389,  0.73383313,\n",
       "        0.1443812 ,  0.29893023, -0.5370209 ,  0.09367172, -0.4327738 ,\n",
       "        0.22623162,  0.71356255, -0.55759346, -0.840005  ,  0.28527948,\n",
       "       -0.4888846 , -0.7491185 , -0.53943795, -0.24224772, -0.15615904,\n",
       "       -0.5730955 , -0.14364181,  0.07767376, -0.47759956, -0.66972166,\n",
       "       -0.4413844 ,  0.8561549 ,  0.08969028,  0.267796  , -0.04675876,\n",
       "       -0.37878293, -0.9136244 , -0.83220655, -0.00457794,  0.38117346,\n",
       "       -0.25626647,  0.3553839 , -0.5126471 ,  0.38053483, -0.627039  ,\n",
       "       -0.5737841 ,  0.18739061, -0.02421715, -0.46818072,  0.10705938,\n",
       "       -0.34396693, -0.03810226,  0.35623017,  0.39787683,  0.386166  ,\n",
       "        0.27503106,  0.27898815, -0.10873193, -0.4889278 , -0.01276421,\n",
       "       -0.45975396, -0.29845467, -0.75887173, -0.37779406, -0.24750292,\n",
       "        0.04804601,  0.2549241 , -0.13043785, -0.2672265 , -0.59674543,\n",
       "        0.0709485 ,  0.03508482, -0.21783131,  0.04127517,  0.44023204,\n",
       "       -0.09012762,  0.57489383, -0.2717398 , -0.18494445,  0.35666066,\n",
       "       -0.6908504 ,  0.17519404, -0.49229273, -0.4572178 , -0.06658658,\n",
       "        0.18815139, -0.3083252 , -0.23405486,  0.40748432,  0.15646876],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

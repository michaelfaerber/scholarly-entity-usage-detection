{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in case you update modules while working\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "es = Elasticsearch(\n",
    "    [{'host': 'localhost', 'port': 9200}], timeout=30, max_retries=10, retry_on_timeout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "journals = []\n",
    "publication = 'arxiv'\n",
    "x = 0\n",
    "for hit in helpers.scan(es, index = \"smartpub\", query = {\"query\":{\"match\":{\"journal\":{\"query\" : publication}}}}, size = 5000):\n",
    "    try:\n",
    "        journals.append(hit['_source']['journal'])\n",
    "    except KeyError:\n",
    "        x = x + 1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39261 0\n",
      "{'arxiv'}\n"
     ]
    }
   ],
   "source": [
    "print(len(journals), x)\n",
    "print(set(journals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 327 Hits:\n",
      "arxiv_1611.05490 Semantic Regularisation for Recurrent Image Annotation 2016\n",
      "6673 words\n",
      "arxiv_1610.07353 Filter-based regularisation for impulse response modelling 2016\n",
      "6773 words\n",
      "arxiv_1502.01400 Fast unsupervised Bayesian image segmentation with adaptive spatial\n",
      "  regularisation 2016\n",
      "6335 words\n",
      "arxiv_1407.2334 The jump set under geometric regularisation. Part 2: Higher-order\n",
      "  approaches 2014\n",
      "13764 words\n",
      "arxiv_1508.07243 Bilevel parameter learning for higher-order total variation\n",
      "  regularisation models 2015\n",
      "10849 words\n",
      "arxiv_1310.0322 Optical Flow on Evolving Surfaces with Space and Time Regularisation 2014\n",
      "7948 words\n",
      "arxiv_1605.04238 Semantic Spaces 2016\n",
      "12035 words\n",
      "arxiv_1607.06283 Real-Time Intensity-Image Reconstruction for Event Cameras Using\n",
      "  Manifold Regularisation 2016\n",
      "4596 words\n",
      "arxiv_1407.1531 The jump set under geometric regularisation. Part 1: Basic technique and\n",
      "  first-order denoising 2015\n",
      "14972 words\n",
      "arxiv_1608.02565 Semantic Code Browsing 2016\n",
      "7972 words\n",
      "arxiv_1509.01329 Semantic Amodal Segmentation 2016\n",
      "5697 words\n",
      "arxiv_1609.01819 Semantic Video Trailers 2016\n",
      "5488 words\n",
      "arxiv_1505.04474 Visual Semantic Role Labeling 2015\n",
      "6390 words\n",
      "arxiv_1602.01366 Semantic Acyclicity Under Constraints 2016\n",
      "20789 words\n",
      "arxiv_1401.2517 The semantic similarity ensemble 2014\n",
      "6457 words\n",
      "arxiv_1608.01961 De-Conflated Semantic Representations 2016\n",
      "5944 words\n",
      "arxiv_1511.00098 Semantic Cross-View Matching 2015\n",
      "5283 words\n",
      "arxiv_1610.09964 Ontology Verbalization using Semantic-Refinement 2016\n",
      "10885 words\n",
      "arxiv_1405.4364 Thematically Reinforced Explicit Semantic Analysis 2014\n",
      "5056 words\n",
      "arxiv_1603.09742 Object Boundary Guided Semantic Segmentation 2016\n",
      "4923 words\n"
     ]
    }
   ],
   "source": [
    "res = es.search(index = \"smartpub\", body = {\"query\": {\"match\": {\"title\" : \"Semantic Regularisation\"}}}, size = 20)\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "\n",
    "for hit in res['hits']['hits']:\n",
    "    print(hit['_id'], hit['_source']['title'], hit['_source']['year'])\n",
    "    print(len(hit['_source']['content'].split(' ')), 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from config import ROOTPATH, STANFORD_NER_PATH\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'trained_ner_DATA'\n",
    "path_to_model = ROOTPATH + '/crf_trained_files/' + model_name + '.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(path_to_model, STANFORD_NER_PATH)\n",
    "tag = 'DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = hit['_source']['content']\n",
    "sentences = nltk.sent_tokenize(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Boundary Guided Semantic Segmentation Qin Huang  , Chunyang Xia  , Wenchao Zheng  , Yuhang Song  , Hao Xu  , C.-C. Jay Kuo qinhuang@usc.edu Abstract.\n",
      "Semantic segmentation is critical to image content under- standing and object localization.\n",
      "Recent development in fully-convolutional neural network FCN has enabled accurate pixel-level labeling.\n",
      "One is- sue in previous works is that the FCN based method does not exploit the object boundary information to delineate segmentation details since the object boundary label is ignored in the network training.\n",
      "To tackle this problem  , we introduce a double branch fully convolutional neural network  , which separates the learning of the desirable semantic class la- beling with mask-level object proposals guided by relabeled boundaries.\n",
      "This network  , called object boundary guided FCN OBG-FCN  , is able to integrate the distinct properties of object shape and class features elegantly in a fully convolutional way with a designed masking archi- tecture.\n",
      "We conduct experiments on the PASCAL VOC segmentation benchmark  , and show that the end-to-end trainable OBG-FCN system offers great improvement in optimizing the target semantic segmentation quality.\n",
      "1 Introduction The convolutional neural network CNN has brought a rapid progress in com- puter vision research and development in recent years .\n",
      "Due to the avail- ability of a large amount of image data   , the performance of various CNNs has been improved significantly.\n",
      "These deep learning based approaches have been applied to high-level vision challenges such as image recognition and object de- tection  and low-level vision problems such as semantic segmentation .\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = []\n",
    "\n",
    "for sentence in sentences[:10]:\n",
    "    sentence = re.sub(r'\\[[^\\(]*?\\]', r'', sentence)\n",
    "    sentence = sentence.replace(\"@ BULLET\", \"\")\n",
    "    sentence = sentence.replace(\"@BULLET\", \"\")\n",
    "    sentence = sentence.replace(\", \", \" , \")\n",
    "    sentence = sentence.replace('(', '')\n",
    "    sentence = sentence.replace(')', '')\n",
    "    sentence = sentence.replace('[', '')\n",
    "    sentence = sentence.replace(']', '')\n",
    "    sentence = sentence.replace(',', ' ,')\n",
    "    sentence = sentence.replace('?', ' ?')\n",
    "    sentence = sentence.replace('..', '.')\n",
    "    sentence = re.sub(r\"(\\.)([A-Z])\", r\"\\1 \\2\", sentence)\n",
    "    print(sentence)\n",
    "    \n",
    "    tagged = ner_tagger.tag(sentence.split())\n",
    "    tagged_sentences.append(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "\n",
    "for jj, (a, b) in enumerate(tagged_sentences[6]):\n",
    "#             tag = model_name.upper()\n",
    "    if b == tag:\n",
    "        a = a.translate(str.maketrans('', '', string.punctuation))\n",
    "        try:\n",
    "            if sentences[jj + 1][1] == tag:\n",
    "                temp = sentences[jj + 1][0].translate(str.maketrans('', '', string.punctuation))\n",
    "                bigram = a + ' ' + temp\n",
    "                result.append(bigram)\n",
    "        except:\n",
    "            result.append(a)\n",
    "            continue\n",
    "        result.append(a)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ImageNet',\n",
       " 'CNNbased',\n",
       " 'OBGMask',\n",
       " 'SPPnet',\n",
       " 'VOC2012',\n",
       " 'DeepLab',\n",
       " 'DTEdgeNet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ['PASCAL', 'ImageNet', 'CNNbased', 'OBGMask', 'boundary', 'public', 'SPPnet', 'VOC2012', 'DeepLab', 'object', 'background', 'DTEdgeNet', 'network']\n",
    "\n",
    "[word for word in results if not wordnet.synsets(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PASCAL', 'ImageNet', 'CNNbased', 'OBGMask', 'boundary', 'public', 'SPPnet', 'VOC2012', 'DeepLab', 'object', 'background', 'DTEdgeNet', 'network']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m2_labelling import ner_labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started extraction for the MET model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................................................Total of 41 filtered entities added\n",
      "Object Boundary Guided Semantic Segmentation Qin Huang, Chunyang Xia, Wenchao Zheng, Yuhang Song, Hao Xu, C.-C. Jay Kuo (qinhuang@usc.edu) Abstract.\n",
      "Semantic segmentation is critical to image content under- standing and object localization.\n",
      "Recent development in fully-convolutional neural network (FCN) has enabled accurate pixel-level labeling.\n",
      "One is- sue in previous works is that the FCN based method does not exploit the object boundary information to delineate segmentation details since the object boundary label is ignored in the network training.\n",
      "To tackle this problem, we introduce a double branch fully convolutional neural network, which separates the learning of the desirable semantic class la- beling with mask-level object proposals guided by relabeled boundaries.\n",
      "This network, called object boundary guided FCN (OBG-FCN), is able to integrate the distinct properties of object shape and class features elegantly in a fully convolutional way with a designed masking archi- tecture.\n",
      "We conduct experiments on the PASCAL VOC segmentation benchmark, and show that the end-to-end trainable OBG-FCN system offers great improvement in optimizing the target semantic segmentation quality.\n",
      "1 Introduction The convolutional neural network (CNN) has brought a rapid progress in com- puter vision research and development in recent years [1,2,3].\n",
      "Due to the avail- ability of a large amount of image data [4,5,6], the performance of various CNNs has been improved significantly.\n",
      "These deep learning based approaches have been applied to high-level vision challenges such as image recognition and object de- tection [1,7,8,9] and low-level vision problems such as semantic segmentation [10,11,12].\n",
      "The network learns to design tailored feature pools for a vision task by examining deep features of discriminative properties and shallow features of local visual patterns.\n",
      "Recent developments in the fully convolutional neural network (FCN) [10] have extended CNN’s capability from image-level recognition to pixel-level deci- sion.\n",
      "It allows the network to see the object location as well as the object class.\n",
      "By taking the advantage of low-level pooling features and probability distribu- tions of neighboring contents, recent studies [10,11,12] have further improved segmentation accuracy on the PASCAL VOC dataset [4].\n",
      "One way to refine segmentation results is to exploit the edge information [13,14].\n",
      "The ground truth labels provided by the PASCAL VOC dataset have ar X iv :1 60 3.\n",
      "09 74 2v 4   [c s.C V]   6  Ju l 2 01 6 2 Authors Suppressed Due to Excessive Length already offered the object contour information.\n",
      "However, object boundaries and hard cases are marked with the same label.\n",
      "To avoid confusion, both object boundaries and hard cases are ignored during the loss calculation in the training stage.\n",
      "In this work, we propose an end-to-end fully convolutional neural network, which takes advantage of object boundaries to guide the semantic segmentation.\n",
      "By relabeling the ground truth into three classes (object without class differ- ence, object boundary and background), we first independently train an object boundary prediction FCN (OBP-FCN), which gives us an accurate prior knowl- edge of object localizations and shape details.\n",
      "This mask-level object proposal, then goes through a designed masking architecture (OBG-Mask), and is later combined with another FCN branch which specifically learn to predict the object classes, formulating the object boundary guided FCN (OBG-FCN).\n",
      "The final- ized system is thus able to combine the strengths of two independent pre-trained FCNs and refine the output with the standard back-propagation, as illustrated in Fig.\n",
      "1.\n",
      "Fig.\n",
      "1.\n",
      "The proposed fully convolutional OBG-FCN consists of three subnets: the FCN-8s, the Object Boundary Prediction FCN (OBP-FCN) and the Object Boundary Guided Mask (OBG-Mask).\n",
      "In the first stage, the FCN-8s is trained using the ground truth labels and used to predict object classes.\n",
      "In the second stage, we convert the ground truth labels into 3 categories (object without class distinction, background and object boundary) and train the OBP-FCN accordingly.\n",
      "In the third stage, we use the OBG-Mask subnet to pass the boundary information to the result of the FCN-8s to yield the ultimate semantic segmentation result.\n",
      "We evaluate the performance of the proposed OBG-FCN method on the PASCAL VOC 2011 and 2012 semantic segmentation datasets.\n",
      "It offers great improvement compared with the baseline FCN model on both validation and testing sets.\n",
      "The experimental results demonstrate that object boundaries offer useful information in delineating object details for better semantic segmentation.\n",
      "The rest of this paper is organized as follows.\n",
      "The related work is reviewed in Section 2.\n",
      "The label conversion and the design of the OBP-FCN are discussed in Section 3.\n",
      "The full OBG-FCN system is proposed in Section 4.\n",
      "The experi- mental results are presented in Section 5.\n",
      "Finally, concluding remarks and future research directions are given in Section 6.\n",
      "Object Boundary Guided Semantic Segmentation 3 2 Related Work Being apart from the traditional segmentation task [15,16,17], semantic segmen- tation demands both pixel-wise accuracy and semantic outputs.\n",
      "Thus, low-level image features and high-level object knowledge have to be integrated to achieve this goal.\n",
      "Deep learning methods have been proposed and proven to be effective for semantic segmentation.\n",
      "In this section, we review several related work along this direction.\n",
      "Object detection is a topic that is highly related to semantic segmentation.\n",
      "It has been extensively studied using CNNs, e.g., [7,8,18,19,20].\n",
      "By predicting ob- ject bounding boxes and categories, RCNN [7], SPPnet [20] and Fast RCNN [8] can detect object regions using object proposals.\n",
      "Faster RCNN [9] exploits the shared convolutional features to extract object proposals, leading to a faster in- ference speed.\n",
      "Masking level proposals can also be extracted in a similar manner by sharing either convolutional features or layer outputs [21,22,23,24].\n",
      "The FCN [10] allows pixel-wise regression.\n",
      "By leveraging the skip architecture [25] to combine the information from pooling layers, the FCN can achieve coarse segmentation with rough object boundaries.\n",
      "MRF/CRF-driven CNN methods have been used to train classifiers and graphical models simultaneously [26,27,11] to further improve detection accuracy and segmentation details.\n",
      "An end-to-end framework has been proposed in [12] to combine the conditional random field with the recursive neural network (RNN) [28] for performance enhancement to refine segmentation details.\n",
      "Recent developments in instance segmentation demonstrate the advantages of multi-task learning and multi-network assembling.\n",
      "For example, the bound- ing box locations and object scores are predicted in a fully-convolutional form in [9].\n",
      "Furthermore, a multi-task network cascades (MNCs) structure is proposed in [29].\n",
      "This structure utilizes the result of a sub-task as a pixel-level mask to help other subtasks in the network.\n",
      "The network involves several subnetworks (or subnets) and considers their mutual interaction to offer a powerful solution.\n",
      "The network training can be simplified by adopting an independent pre-training procedure for each subnet which is then followed by a dependent learning pro- cedure.\n",
      "One way particular in multi-tasking learning is to incorporate edge/contour detection with semantic class labeling.\n",
      "Specifically, Bertasius et al.\n",
      "[14] and Chen [13] exploit features from intermediate layers of a deep network and conduct a edge detection sub-task in the similar way of [30].\n",
      "In [14], Bertasius et al.\n",
      "improves the boundary detection with semantic segmentation, while Chen [13] designs a domain transform structure to conduct an edge-preserving filtering for segmentation.\n",
      "In comparison with previous related work, we propose a multi-network system that addresses the object boundary detection and the semantic segmentation problem simultaneously.\n",
      "It is shown that an improved object boundary predictor can guide the object labeling task in semantic segmentation.\n",
      "4 Authors Suppressed Due to Excessive Length 3 Object Boundary Prediction with OBP-FCN The FCN in [10] is trained using the PASCAL VOC dataset for the recognition of 20 object classes, which offers good performance since it recognizes patterns of desired classes by examining both coarse-level and fine-level visual features.\n",
      "It generates a blob-wise result to describe the coarse shape of an object and predict its class label.\n",
      "Although the deconvolution layer can partially recover the lost resolution of the input in the pooling layer, its segmented result is still rough and the class label could be wrong as local features can be confusing.\n",
      "Edge detection is conducted in [13] with middle-level features, yet this method detects edges around and inside an object.\n",
      "To enhance the accuracy of segmented object boundaries, we propose a variant of the FCN, called the OBP-FCN, that offers pixel-wise object/boundary prediction in this section.\n",
      "3.1 Generation of New Labels We first process the existing labels in the PASCAL-VOC dataset and convert them into a set of new labels.\n",
      "Then, the new labels will be used to train the OBP-FCN for more accurate object mask prediction.\n",
      "The PASCAL VOC dataset provides labels for object classes and instances as the ground truth.\n",
      "For each image with indexes I, N object classes are labeled in N colors, denoted by  Lc = {l1, l2, · · · , lN}, where N = 20 for the PASCAL VOC dataset.\n",
      "The background area (Ib) is labeled in black, denoted by lb, and region of the object boundary area and hard cases (Iw) are labeled in white, denoted by lw, which are usually ignored in the penalty function calculation during the CNN training process.\n",
      "To recover the accurate location information of object boundaries, we con- vert the existing PASCAL VOC labels into our desired 4 categories: 1) objects without class distinction (lo), 2) object boundaries (lob), 3) background (lb), and 4) hard cases (lhc).\n",
      "To begin with, we first derive the object indexes with labels LI ∈  Lc as object regions (Io).\n",
      "We then derive the outline of each object region as the object boundary (Iob).\n",
      "For this purpose, we compare the label of each object pixel with those of its neighbors in a 3 × 3 window, and label the one without uniform-class neighbor as object boundary.\n",
      "As a result, we can find all pixels that separate different class labeling (object classes as well as background).\n",
      "Algorithm A Label Conversion for the PASCAL VOC dataset.\n",
      ".Initialize all image pixels with the background label, LI ← lb .Assign object boundary label to extended region, LIexb ← lob .Assign object label to original object region, LIo ← lo .Label the hard case regions LIhc ← lhc Then we thicken the boundary region (Iob) into the extended boundary region (Iexb) by dilating a pixel to four directions by w pixels.\n",
      "The remaining pixels Object Boundary Guided Semantic Segmentation 5 with original lw label and not within the thickened boundary region are noted as hard cases (Ihc).\n",
      "After deriving all desirable class regions, we assign the target labels as in Algorithm A.\n",
      "Note that the order of the assignment is very important so as to keep the completeness of object and the accuracy of the boundary.\n",
      "A sample image, its original and corresponding new labels are shown in Figs.\n",
      "2 (a), (b) and (c), respectively.\n",
      "The white color in Fig.\n",
      "2 (b) represents not only object boundaries, but also occluded objects in the background that are difficult to recognize.\n",
      "In contrast, the new label system marks both the person and the horse in red, the thickened object boundaries in green, the background in black, and the hard cases in olive (or yellow green) in Fig.\n",
      "2 (c).\n",
      "We keep the hard case region and its loss would be ignored during training as in conventional methods.\n",
      "(a) Image (b) Original Labels (c) New Labels Fig.\n",
      "2.\n",
      "Illustration of (a) a sample input image; (b) its original labels from the PASCAL VOC dataset; and (c) its new labels with maximum width w = 4.\n",
      "3.2 Object Boundary Prediction FCN (OBP-FCN) With the relabeled ground truth, we then train a network that can predict object (without class distinction), boundary and background regions while ignoring the hard case region.\n",
      "The network structure of the proposed network, called the OBP-FCN, is shown in Fig.\n",
      "3, where its first 5 layers have the same convolution, pooling and ReLU operations as in VGG while the fully connected layers ’fc-6’ and ’fc-7’ in VGG are replaced by two convolutional layers.\n",
      "The unique characteristics of the OBP-FCN is that it considers all features of ’pool4’, ’pool3’ and ’pool2’ so as to combine large-scale class knowledge and detail boundary information.\n",
      "To initialize the OBP-FCN, we begin with the VGG network [2] pre-trained on the ImageNet dataset [6].\n",
      "Then, we use the new 4-category labels to train the OBP-FCN for the desired goal.\n",
      "We would like to elaborate the importance of thickened object boundaries below.\n",
      "In the traditional FCN, the size of all kernels in VGG-16 convolution layers is 3 × 3.\n",
      "And with the help of the pooling layer, the gradually growing receptive field of each layer allows the network to see patterns on different scales.\n",
      "The labeled object boundary has an influence on learning local features, and it can force filters to consider its existence at deeper layers.\n",
      "Without the constraint 6 Authors Suppressed Due to Excessive Length Fig.\n",
      "3.\n",
      "The proposed OBP-FCN follows the basic structure of the FCN by combining the coarse high level information with the detailed low level information.\n",
      "Deconvolution is applied in upscaling.\n",
      "The response maps of each output are summed element-wise by following a skip scheme.\n",
      "All network models (i.e., OBP-FCN-4s, OBP-FCN-8s, OBP- FCN-16s and OBP-FCN-32s) and their results can be retrieved in each training step.\n",
      "The final detail level is OBP-FCN-4s.\n",
      "of labeled object boundary, the original FCN network from [10] stops at pool-3 as the performance does not improve furthermore.\n",
      "In contrast, we observe that the OBP-FCN continues to refine its object boundary detection, benefiting from features in layer pool-2.\n",
      "This is because the labeled maximum boundary width is two or four pixels, which can be seen on smaller scales.\n",
      "4 Semantic Segmentation with OBG-FCN In this section, we propose an enhanced semantic segmentation solution, called the object boundary guided FCN (OBG-FCN).\n",
      "The object shape and location information predicted by the OBP-FCN is used as a spatial mask to guide the semantic segmentation task.\n",
      "An overview of the OBG-FCN is given in Sec.\n",
      "4.1.\n",
      "One important subnet of the OBG-FCN, called the OBG-Mask, is introduced in Sec.\n",
      "4.2.\n",
      "Some implementation details are discussed in Sec.\n",
      "4.3.\n",
      "4.1 Overview of the OBG-FCN The OBG-FCN system consists of three subnets; namely, FCN-8s, OBP-FCN- 4s and OBG-Mask.\n",
      "The evolution of the filter response maps for an exemplary bird image is shown in Fig.\n",
      "4.\n",
      "Since the output of the OBP-FCN is a 3-category map, we design a masking architecture called the OBG-Mask that passes the trained object shape and localization information to the FCN-8s that segments Object Boundary Guided Semantic Segmentation 7 21 semantic classes (namely, 20 object classes plus the background).\n",
      "This com- bination of two branches yields the final 21-class segmentation results.\n",
      "We show the response score maps of three subnets as well as the final output of the OBG- FCN in Fig.\n",
      "4, which demonstrates the performance has improved significantly by integrating the three subnets.\n",
      "Fig.\n",
      "4.\n",
      "Evolution of the response maps in the OBG-FCN: 1) the FCN-8s provides a coarse class label for all objects (20 classes); 2) the OBP-FCN-4s indicates the object localization without class distinction; 3) the OBG-Mask network produces an object mask, maps it to the corresponding class label, and yield a more accurate filtered score map; 4) the final output of the OBG-FCN.\n",
      "Multi-task learning is popular in recent CNN-based segmentation methods, where extracted features are shared among multiple tasks.\n",
      "However, when we attempt to share features for the FCN-8s and the OBP-FCN, the training tends to be biased on the FCN-8s sub-branch, resulting in poor performance of the OBP-FCN.\n",
      "For this reason, we train the OBP-FCN separately and adopt the learned filter weights in the OBG-FCN system afterwards.\n",
      "4.2 OBG-Mask The main purpose of the OBG-Mask subnet structure is to pass the 3-category (object, boundary and background) labels obtained from the OBP-FCN to the output of the FCN-8s to yield the ultimate output of the whole OBG-FCN system.\n",
      "Specifically, the OBG-Mask subnet first converts the 3-category inference result into 21-class object masks.\n",
      "Then, the mask-level object proposals are combined with the output of FCN-8s via element-wise production.\n",
      "One example of the OBG-Mask is shown in Fig.\n",
      "5, which consists of three convolution and rectifier linear unit (ReLU) layers in pair plus the 4th convolu- tion layer.\n",
      "The last convolution layer, conv-m4, does not include the ReLU layer.\n",
      "Its output is integrated with the output from FCN-8s using either element-wise multiplication or summation, which will be further discussed in the experiment 8 Authors Suppressed Due to Excessive Length section.\n",
      "The parameters of each conv layer are given in the bracket, indicating the number of filters, the number of input channels, the kernel height and width, respectively.\n",
      "Fig.\n",
      "5.\n",
      "An example of the OBG-Mask that accepts the output from the OBP-FCN with w = 2.\n",
      "The first two cascaded convolution layers have a compound receptive field of 5×5, which is large enough to see two adjacent boundaries.\n",
      "The masking architecture, then accepts the output from the FCN-8s and conducts element-wise multiplication to produce the final 21-class response score map.\n",
      "The maximum boundary width w in the OBP-FCN determines the kernel size of the convolutional layers.\n",
      "For example, the OBG-Mask shown in Fig.\n",
      "5, is designed for the case of w = 2.\n",
      "The first two cascaded convolution layers have a compound receptive field of 5× 5, where two adjacent boundaries can be covered at the same time.\n",
      "In this way, the detected boundary can help improve object or background labeling based on the local region information and back- propagated class information.\n",
      "This is especially beneficial to small objects and complex regions.\n",
      "Although we can benefit from a larger receptive field by in- creasing the kernal size, this increases training complexity as well.\n",
      "The proposed simple structure is already sufficient to meet our needs.\n",
      "4.3 Implementation Details The full OBG-FCN system is designed to integrate the strengths of the FCN-8s and the OBP-FCN.\n",
      "We first discuss its training procedure.\n",
      "Both filter weights of FCN-8s and OBP-FCN subnets are pre-trained and their pre-trained values are used for initialization.\n",
      "For filter weights of the OBG-Mask subnet, we adopt a random initialization scheme.\n",
      "The performance of the OBG-FCN is sensitive to its learning rate.\n",
      "We adopt three training rates: 1) 10−15 for the FCN-8s; 2)10−17 for the OBP-FCN; and 3) 10−10 for the OBG-Mark.\n",
      "The training rate of 10−15 is commonly used for the FCN training.\n",
      "We adopt a lower training rate for the OBP-FCN to ensure the provided object boundary information is consistent and stable.\n",
      "We adopt a higher training rate for the OBG-Mask so that its weights can be adjusted more aggressively for faster converging.\n",
      "Object Boundary Guided Semantic Segmentation 9 We trained multiple OBP-FCNs step-by-step: OBP-FCN-32s first, OBP- FCN-16s next, OBP-FCN-8s afterwards, and OBP-FCN-4s last.\n",
      "The network learning rates are fixed at 10−10, 10−13, 10−14 and 10−15, respectively.\n",
      "The mo- mentum is set to 0.99 as we use the full image for training with a batch size of 1.\n",
      "For the training of OBP-FCN-32s, the weights of the first 5 convolutional layers are copied from VGG and network surgery is performed to transform the parameters of the original fully-connected form into the fully convolutional form.\n",
      "The weight decay is set to 0.005 for OBP-FCN and 0.016 for OBG-FCN following the set-up of FCN and CRF-RNN, respectively.\n",
      "We use the standard softmax loss function, which is referred to as the log-likelihood error function in [31], and the ReLU throughout the system for non-linearity.\n",
      "We implement the OBG-FCN system using the Caffe [32] library.\n",
      "The com- plete source code and trained models will be available to the public.\n",
      "Each train- ing stage was conducted on a Titan-X graphic card, and the training time varies depending on the number of training images.\n",
      "5 Experiments In this section, we evaluate the performance of the proposed OBG-FCN on the PASCAL VOC dataset.\n",
      "We first evaluate the contributions of edge labeling on object inference with the PASCAL VOC 11 dataset.\n",
      "We then follow [12] to train the proposed OBG-FCN framework with both PASCAL VOC 2012 training image and an augmented PASCAL labeling [18].\n",
      "The performance is evaluated on a non-overlapping subset of the PASCAL VOC 2012 validation image set.\n",
      "Finally, we compare the performance on both the PASCAL 2011 and 2012 test sets by submitting the proposed solution to the evaluation server.\n",
      "5.1 Performance of OBP-FCN We first evaluate the impact of object boundary labeling on the accuracy of object region prediction.\n",
      "As mentioned in Sec.\n",
      "3.1, we can choose any desired maximum width in object boundary relabeling.\n",
      "We conduct experiments with three different maximum boundary widths and compare the performance on the PASCAL VOC 2011.\n",
      "1112 images are used for the training of OBP-FCN, and 1111 images are used for validation.\n",
      "In order to compare with the original 20 object-class labels, we retrain the original FCN model with 20 object classes using the training data of the PASCAL VOC 2011.\n",
      "Then, we convert the seg- mentation result into object labels without class distinction for fair comparison.\n",
      "We evaluate the performance on object region prediction by calculating the accuracy between the predicted object area and the relabeled ground truth.\n",
      "By following [10], four evaluation metrics are used, including pixel accuracy, mean accuracy, mean IU (Intersection over Union) and frequency weighted IU.\n",
      "The relabeled object boundaries are proven to be more effective in predicting objects’ detail shapes than the 20-class labeling.\n",
      "10 Authors Suppressed Due to Excessive Length Table 1.\n",
      "Performance on object area prediction with different labeling methods.\n",
      "pixel acc.\n",
      "mean acc.\n",
      "mean IU f.w.\n",
      "IU 32s 16s 8s 4s 32s 16s 8s 4s 32s 16s 8s 4s 32s 16s 8s 4s FCN 90.4 90.7 90.9 90.3 83.6 83.9 84.0 82.6 76.8 77.4 77.7 76.2 82.4 82.8 83.1 82.0 0-pixel OBP-FCN 90.7 91.2 91.3 91.0 84.8 86.1 86.4 85.5 77.8 79.1 79.3 78.5 82.9 83.9 84.0 83.5 2-pixel OBP-FCN 91.4 91.9 92.0 92.0 89.3 89.0 88.8 89.0 80.5 81.1 81.2 81.3 84.6 85.2 85.3 85.4 4-pixel OBP-FCN 91.1 91.3 91.5 91.4 86.9 87.1 87.1 87.2 79.2 79.6 79.9 79.8 83.7 84.1 84.4 84.3 Input Image FCN-8s OBP-FCN- 4s (w=0) OBP-FCN- 4s (w=2) OBP-FCN- 4s (w=4) Ground Truth Fig.\n",
      "6.\n",
      "Performance comparison of FCN-8s and OBP-FCN-4s with three maximum width values.\n",
      "We compare object shape prediction results of FCN-8s and OBP-FCN-4s with three maximum width values (w = 0, 2 and 4.)\n",
      "on a test image in Fig.\n",
      "6, where the original input and the ground truth label are also provided.\n",
      "We see that object boundary relabeling and training does help improve object localization in providing more object details and avoiding false alarms.\n",
      "The case w = 2 gives the best performance.\n",
      "It is worthwhile to point out that we do not evaluate the performance of the object boundaries prediction since we only use boundaries in the training to help object/background or object/object segmentation.\n",
      "Thus, even if the predicted object boundary (in green color) may not be closed, it still contributes to the completeness of object prediction when all results are integrated in a later stage.\n",
      "In addition, the results indicate that with w = 2, the accuracy of object inference can be further improved even on OBP-FCN-4s.\n",
      "In Fig.\n",
      "7, we show that by gradually combining the low-level features, the object contours begin to emerge and OBP-FCN-4s gives the best result with natural and sufficient details.\n",
      "5.2 Performance of OBG-FCN As described in Sec.\n",
      "3, we train the proposed OBP-FCN-4s with 11,685 relabeled ground truth images, including 1,464 labeled images in the PASCAL VOC 2012 trainging set and the augmented labeled images from [18].\n",
      "Then, we adopt the Object Boundary Guided Semantic Segmentation 11 OBP-FCN- 32s OBP-FCN- 16s OBP-FCN-8s OBP-FCN-4s New Label Fig.\n",
      "7.\n",
      "Performance comparison of four OBP networks with maximum width w = 2: OBP-FCN-32s, OBP-FCN-16s, OBP-FCN-8s and OBP-FCN-4s.\n",
      "pre-trained FCN-8s model from [10] and conduct the end-to-end training to get the final OBG-FCN.\n",
      "Since the labeling of augmented data is not very accurate, we use the full set of 11,685 images to train the OBP-FCN-32s only.\n",
      "This is done because a large amount of image data can provide rich yet coarse information of object features.\n",
      "Then, we train the remaining two subnets of the OBG-FCN with the 1,464 labeled images in the PASCAL VOC 2012 datset.\n",
      "The accurately labeled object boundaries help construct more accurate object shapes, leading to better segmentation results.\n",
      "Performance on Validation Set Since there is an overlap between the aug- mented labeled image set and the PASCAL VOC 12 validation set, we select a list of 346 non-overlapping images from the PASCAL VOC 12 validation set, and evaluate the performance of the proposed OBG-FCN on this subset.\n",
      "The results of the baseline FCN-8s, along with the proposed OBG-FCN with rela- beled boundaries of 2-pixel and 4-pixel maximum widths are presented in Table 2.\n",
      "We see that the OBG-FCN with w = 2 offers the best results.\n",
      "Table 2.\n",
      "Performance on selective PASCAL VOC 2012 validation set of OBG-FCN with different labeling pixel widths (w).\n",
      "pixel acc.\n",
      "mean acc.\n",
      "mean IU f.w.\n",
      "IU FCN-8s 90.1 74.1 61.1 82.7 OBG-FCN (w = 2) 91.6 76.4 64.9 85.3 OBG-FCN (w = 4) 91.5 75.5 64.5 84.9 Exemplary segmentation results of single-class images are shown in Fig.\n",
      "8.\n",
      "Compared with FCN-8s, more accurate silhouettes are obtained by the OBP- FCN-4s since the relabeled boundary offers extra information to constraint the object.\n",
      "As a result, the OBG-FCN can improve FCN’s results by either providing some lost object information (e.g.\n",
      "bird’s wing) or correcting some false decisions (e.g.\n",
      "train’s tail).\n",
      "The boundary regions are smoother and more natural.\n",
      "12 Authors Suppressed Due to Excessive Length Input Image FCN-8s OBP-FCN-4s OBG-FCN Ground Truth Fig.\n",
      "8.\n",
      "Visualization of exemplary single-class segmentation results in VOC2012 val- idation set (from left to right and best viewed in color): input images, intermediate results of FCN-8s, intermediate results of OBP-FCN-4s, and final results of OBG-FCN, and the ground truth labels.\n",
      "Furthermore, we compare the segmentation results of FCN-8s and OBG-FCN for several exemplary multi-class and multi-object images in Fig.\n",
      "9.\n",
      "We see that the proposed OBP-FCN-4s can localize some boundaries between concatenated objects (e.g.\n",
      "boundaries between humans in the last row).\n",
      "Generally speaking, the segmentation results of OBG-FCN are better than those of FCN-8s.\n",
      "Comparison of Different Structure As mentioned in Sec.\n",
      "4.2, the combina- tion of two fully convolutional branches can be either element-wise multiplica- tion or summation.\n",
      "Therefore, we evaluate the performance of these two different settings to show that they bring with similar results.\n",
      "Furthermore, we present another set of results using only OBP-FCN-8s in stage 2 to construct the OBG- FCN system.\n",
      "Results in Table.\n",
      "3 prove that OBP-FCN-4s does provide a better benchmark result with refined object details.\n",
      "Performance on PASCAL VOC Test Set We then use the same training data set and evaluate the performance on the PASCAL VOC 2011 and 2012 test Object Boundary Guided Semantic Segmentation 13 Input Image FCN-8s OBP-FCN OBG-FCN Ground Truth Fig.\n",
      "9.\n",
      "Examples of multi-class and multi-object segmentation results in VOC2012 validation set.\n",
      "Best viewed in color.\n",
      "Table 3.\n",
      "Comparison of mean IU performance using multiplication or summation for subnet fusion and using the OBP-FCN-4s or the OBP-FCN-8s.\n",
      "OBG-Mask method Product Summation OBP-FCN-8s OBP-FCN-4s OBP-FCN-8s OBP-FCN-4s Mean IU 64.5 64.8 64.5 64.9 sets by submitting the results to the evaluation server.\n",
      "The results are given in Table 4.\n",
      "As shown in this table, the OBG-FCN with w = 2 reaches 69.5% mean IU in VOC 2011 test and 69.1% in VOC 2012 test, outperforming the baseline FCN by about 7%.\n",
      "It has also surpassed the previous state-of-the-art methods without relying on conditional random fields.\n",
      "Comparison of Inference Speed We finally evaluate the inference speed of the proposed framework.\n",
      "As in Table.\n",
      "5, the OBG-FCN takes about 0.187 s/image in average on a Titan X GPU, which offers possibilities for real-time segmentation.\n",
      "The inference time on the Intel Core i7-5930 CPU takes about 5.6 s/image in average.\n",
      "Compared with FCN-8s, the proposed OBG-FCN takes about twice the inference time as we rely on two distinct FCN sub-branches.\n",
      "We further test on a publicly available model of CRF-RNN with 10 mean-filed iterations.\n",
      "And although the CRF-RNN provides the best 72.0 % mean IU in VOC12 test 14 Authors Suppressed Due to Excessive Length Table 4.\n",
      "Performance comparison of nean IU accuracy on the PASCAL VOC 2011 and 2012 test datasets between FCN-8s, DeepLab, DT-SE, DT-EdgeNet and OBG- FCN with w = 2 and w = 4.\n",
      "FCN-8s [10] DeepLab [11] DT-SE [13] OBG-FCN (w=4) DT-EdgeNet [13] OBG-FCN (w=2) VOC2011 test 62.7 / / 68.9 / 69.5 VOC2012 test 62.2 65.1 67.8 68.6 69.0 69.1 set, its average inference time on GPU is ten times slower than the proposed OBG-FCN.\n",
      "Table 5.\n",
      "Average inference time (s/image).\n",
      "FCN-8s OBG-FCN CRF-RNN CPU time 2.56 5.34 5.21 GPU time 0.09 0.186 1.92 6 Conclusion and Future Work In this work, we propose a fully-convolutional network with two distinct branches in earlier stage to specifically learn the class information and the mask-level object proposal.\n",
      "The strengths of two sub-networks are then fused together with a proposed OBG-Mask architecture to provide better semantic segmentation results.\n",
      "The method was proven to be effective in generating accurate object localizations and refined object details.\n",
      "Although the proposed OBP-FCN subnet can provide a better object shape constraint and yield better semantic segmentation results, the final performance is still limited by the accuracy of the 20-class FCN subnet.\n",
      "It is important to find a better baseline in building the full OBG-FCN system.\n",
      "Also, the performance can be further improved if better labeling can be provided for more training images.\n",
      "Generally speaking, semantic segmentation remains to be a challenging problem for further research.\n",
      "\n",
      "Entities labelled ['public', 'Due', 'Deep', 'background', 'FCNs', 'outputs', 'RCNN', 'Suppressed', 'Excessive', 'network', 'Length', 'Caffe', 'library', 'Semantic', 'ReLU', 'learning', 'object', 'PASCAL', 'Segmentation', 'Boundary', 'Guided', 'although', 'OBGMark', 'Although', 'deep', 'MNCs', 'boundary', 'Fig', 'CNNs', 'Authors', 'OBPFCN', 'map', 'layer', 'Object', 'ImageNet', 'OBPFCN8s', 'Mask', 'SPPnet', 'OBGMask', 'OBGFCN', 'segmentation']\n"
     ]
    }
   ],
   "source": [
    "result = ner_labelling.long_tail_labelling('MET', hit['_source']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FCNs',\n",
       " 'RCNN',\n",
       " 'Caffe',\n",
       " 'ReLU',\n",
       " 'although',\n",
       " 'OBGMark',\n",
       " 'Although',\n",
       " 'MNCs',\n",
       " 'CNNs',\n",
       " 'OBPFCN',\n",
       " 'ImageNet',\n",
       " 'OBPFCN8s',\n",
       " 'SPPnet',\n",
       " 'OBGMask',\n",
       " 'OBGFCN']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in result if not wordnet.synsets(word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
